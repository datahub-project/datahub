{
  "eventTime": "2023-09-27T13:38:39.278Z",
  "producer": "https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark",
  "schemaURL": "https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent",
  "eventType": "FAIL",
  "run": {
    "runId": "9f06a36e-97c5-4d4a-9e92-0aef9ed2be4c",
    "facets": {
      "processing_engine": {
        "_producer": "https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark",
        "_schemaURL": "https://openlineage.io/spec/facets/1-1-0/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet",
        "version": "3.0.3",
        "name": "spark"
      },
      "environment-properties": {
        "_producer": "https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark",
        "_schemaURL": "https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet",
        "environment-properties": {}
      },
      "spark_version": {
        "_producer": "https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark",
        "_schemaURL": "https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet",
        "spark-version": "3.0.3"
      },
      "spark.exception": {
        "_producer": "https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark",
        "_schemaURL": "https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet",
        "message": "Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 246, tamass-mbp-2.chello.hu, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.nio.file.AccessDeniedException: delete on s3a://acryl-datahub-offline/tmp/daily_stats2/_temporary/0/_temporary/attempt_202309271538167729286925835797846_0002_m_000000_246: com.amazonaws.services.s3.model.MultiObjectDeleteException: One or more objects could not be deleted (Service: null; Status Code: 200; Error Code: null; Request ID: 8NSVQ0ZA5AE9C3J6; S3 Extended Request ID: EC7gYrBqATpz1PTW5yN1sy9VyXW8yIVIrfbuwKUmwXkF4skjjxp6eMjpkHytY4cXII/NQOguMBc=), S3 Extended Request ID: EC7gYrBqATpz1PTW5yN1sy9VyXW8yIVIrfbuwKUmwXkF4skjjxp6eMjpkHytY4cXII/NQOguMBc=:null: AccessDenied: tmp/daily_stats2/_temporary/0/_temporary/attempt_202309271538167729286925835797846_0002_m_000000_246/part-00000-ac0d9627-17c4-47ee-9559-4c52ce9a9975-c000.csv: Access Denied\n\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateMultiObjectDeleteException(S3AUtils.java:460)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:269)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.delete(S3AFileSystem.java:1727)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.abortTask(FileOutputCommitter.java:637)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.abortTask(FileOutputCommitter.java:626)\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:55)\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:254)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:79)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:280)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\n\t... 9 more\nCaused by: com.amazonaws.services.s3.model.MultiObjectDeleteException: One or more objects could not be deleted (Service: null; Status Code: 200; Error Code: null; Request ID: 8NSVQ0ZA5AE9C3J6; S3 Extended Request ID: EC7gYrBqATpz1PTW5yN1sy9VyXW8yIVIrfbuwKUmwXkF4skjjxp6eMjpkHytY4cXII/NQOguMBc=), S3 Extended Request ID: EC7gYrBqATpz1PTW5yN1sy9VyXW8yIVIrfbuwKUmwXkF4skjjxp6eMjpkHytY4cXII/NQOguMBc=\n\tat com.amazonaws.services.s3.AmazonS3Client.deleteObjects(AmazonS3Client.java:2146)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$deleteObjects$8(S3AFileSystem.java:1420)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.deleteObjects(S3AFileSystem.java:1416)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.removeKeys(S3AFileSystem.java:1676)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerDelete(S3AFileSystem.java:1796)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.delete(S3AFileSystem.java:1711)\n\t... 18 more\n\nDriver stacktrace:",
        "stackTrace": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 246, tamass-mbp-2.chello.hu, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.nio.file.AccessDeniedException: delete on s3a://acryl-datahub-offline/tmp/daily_stats2/_temporary/0/_temporary/attempt_202309271538167729286925835797846_0002_m_000000_246: com.amazonaws.services.s3.model.MultiObjectDeleteException: One or more objects could not be deleted (Service: null; Status Code: 200; Error Code: null; Request ID: 8NSVQ0ZA5AE9C3J6; S3 Extended Request ID: EC7gYrBqATpz1PTW5yN1sy9VyXW8yIVIrfbuwKUmwXkF4skjjxp6eMjpkHytY4cXII/NQOguMBc=), S3 Extended Request ID: EC7gYrBqATpz1PTW5yN1sy9VyXW8yIVIrfbuwKUmwXkF4skjjxp6eMjpkHytY4cXII/NQOguMBc=:null: AccessDenied: tmp/daily_stats2/_temporary/0/_temporary/attempt_202309271538167729286925835797846_0002_m_000000_246/part-00000-ac0d9627-17c4-47ee-9559-4c52ce9a9975-c000.csv: Access Denied\n\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateMultiObjectDeleteException(S3AUtils.java:460)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:269)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.delete(S3AFileSystem.java:1727)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.abortTask(FileOutputCommitter.java:637)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.abortTask(FileOutputCommitter.java:626)\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:55)\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:254)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:79)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:280)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\n\t... 9 more\nCaused by: com.amazonaws.services.s3.model.MultiObjectDeleteException: One or more objects could not be deleted (Service: null; Status Code: 200; Error Code: null; Request ID: 8NSVQ0ZA5AE9C3J6; S3 Extended Request ID: EC7gYrBqATpz1PTW5yN1sy9VyXW8yIVIrfbuwKUmwXkF4skjjxp6eMjpkHytY4cXII/NQOguMBc=), S3 Extended Request ID: EC7gYrBqATpz1PTW5yN1sy9VyXW8yIVIrfbuwKUmwXkF4skjjxp6eMjpkHytY4cXII/NQOguMBc=\n\tat com.amazonaws.services.s3.AmazonS3Client.deleteObjects(AmazonS3Client.java:2146)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$deleteObjects$8(S3AFileSystem.java:1420)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.deleteObjects(S3AFileSystem.java:1416)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.removeKeys(S3AFileSystem.java:1676)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerDelete(S3AFileSystem.java:1796)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.delete(S3AFileSystem.java:1711)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:952)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.nio.file.AccessDeniedException: delete on s3a://acryl-datahub-offline/tmp/daily_stats2/_temporary/0/_temporary/attempt_202309271538167729286925835797846_0002_m_000000_246: com.amazonaws.services.s3.model.MultiObjectDeleteException: One or more objects could not be deleted (Service: null; Status Code: 200; Error Code: null; Request ID: 8NSVQ0ZA5AE9C3J6; S3 Extended Request ID: EC7gYrBqATpz1PTW5yN1sy9VyXW8yIVIrfbuwKUmwXkF4skjjxp6eMjpkHytY4cXII/NQOguMBc=), S3 Extended Request ID: EC7gYrBqATpz1PTW5yN1sy9VyXW8yIVIrfbuwKUmwXkF4skjjxp6eMjpkHytY4cXII/NQOguMBc=:null: AccessDenied: tmp/daily_stats2/_temporary/0/_temporary/attempt_202309271538167729286925835797846_0002_m_000000_246/part-00000-ac0d9627-17c4-47ee-9559-4c52ce9a9975-c000.csv: Access Denied\n\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateMultiObjectDeleteException(S3AUtils.java:460)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:269)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.delete(S3AFileSystem.java:1727)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.abortTask(FileOutputCommitter.java:637)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.abortTask(FileOutputCommitter.java:626)\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:55)\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:254)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:79)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:280)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\n\t... 9 more\nCaused by: com.amazonaws.services.s3.model.MultiObjectDeleteException: One or more objects could not be deleted (Service: null; Status Code: 200; Error Code: null; Request ID: 8NSVQ0ZA5AE9C3J6; S3 Extended Request ID: EC7gYrBqATpz1PTW5yN1sy9VyXW8yIVIrfbuwKUmwXkF4skjjxp6eMjpkHytY4cXII/NQOguMBc=), S3 Extended Request ID: EC7gYrBqATpz1PTW5yN1sy9VyXW8yIVIrfbuwKUmwXkF4skjjxp6eMjpkHytY4cXII/NQOguMBc=\n\tat com.amazonaws.services.s3.AmazonS3Client.deleteObjects(AmazonS3Client.java:2146)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$deleteObjects$8(S3AFileSystem.java:1420)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.deleteObjects(S3AFileSystem.java:1416)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.removeKeys(S3AFileSystem.java:1676)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerDelete(S3AFileSystem.java:1796)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.delete(S3AFileSystem.java:1711)\n\t... 18 more\n"
      }
    }
  },
  "job": {
    "namespace": "default",
    "name": "cloud_trail_log_statistics.execute_insert_into_hadoop_fs_relation_command.tmp_daily_stats2",
    "facets": {}
  },
  "inputs": [
    {
      "namespace": "s3a://aws-cloudtrail-logs-795586375822-837d93fd",
      "name": "/AWSLogs/795586375822/CloudTrail/eu-west-1",
      "facets": {
        "dataSource": {
          "_producer": "https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark",
          "_schemaURL": "https://openlineage.io/spec/facets/1-0-0/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet",
          "name": "s3a://aws-cloudtrail-logs-795586375822-837d93fd",
          "uri": "s3a://aws-cloudtrail-logs-795586375822-837d93fd"
        },
        "schema": {
          "_producer": "https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark",
          "_schemaURL": "https://openlineage.io/spec/facets/1-0-0/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet",
          "fields": [
            {
              "name": "Records",
              "type": "array"
            }
          ]
        }
      },
      "inputFacets": {}
    }
  ],
  "outputs": [
    {
      "namespace": "s3a://acryl-datahub-offline",
      "name": "/tmp/daily_stats2",
      "facets": {
        "dataSource": {
          "_producer": "https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark",
          "_schemaURL": "https://openlineage.io/spec/facets/1-0-0/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet",
          "name": "s3a://acryl-datahub-offline",
          "uri": "s3a://acryl-datahub-offline"
        },
        "schema": {
          "_producer": "https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark",
          "_schemaURL": "https://openlineage.io/spec/facets/1-0-0/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet",
          "fields": [
            {
              "name": "date",
              "type": "string"
            },
            {
              "name": "count",
              "type": "long"
            }
          ]
        },
        "lifecycleStateChange": {
          "_producer": "https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark",
          "_schemaURL": "https://openlineage.io/spec/facets/1-0-0/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet",
          "lifecycleStateChange": "OVERWRITE"
        }
      },
      "outputFacets": {
        "outputStatistics": {
          "_producer": "https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark",
          "_schemaURL": "https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet",
          "rowCount": 0,
          "size": 0
        }
      }
    }
  ]
}