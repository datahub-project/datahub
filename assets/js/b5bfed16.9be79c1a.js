"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[40406],{44800:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>c,contentTitle:()=>d,default:()=>u,frontMatter:()=>p,metadata:()=>m,toc:()=>g});t(96540);var n=t(15680),l=t(53720),i=t(5400);function s(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){return a=null!=a?a:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):function(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))})),e}function o(e,a){if(null==e)return{};var t,n,l=function(e,a){if(null==e)return{};var t,n,l={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(l[t]=e[t]);return l}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(l[t]=e[t])}return l}const p={sidebar_position:68,title:"S3 / Local Files",slug:"/generated/ingestion/sources/s3",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/generated/ingestion/sources/s3.md"},d="S3 / Local Files",m={unversionedId:"docs/generated/ingestion/sources/s3",id:"docs/generated/ingestion/sources/s3",title:"S3 / Local Files",description:"This connector ingests AWS S3 datasets into DataHub. It allows mapping an individual file or a folder of files to a dataset in DataHub.",source:"@site/genDocs/docs/generated/ingestion/sources/s3.md",sourceDirName:"docs/generated/ingestion/sources",slug:"/generated/ingestion/sources/s3",permalink:"/docs/generated/ingestion/sources/s3",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/generated/ingestion/sources/s3.md",tags:[],version:"current",sidebarPosition:68,frontMatter:{sidebar_position:68,title:"S3 / Local Files",slug:"/generated/ingestion/sources/s3",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/generated/ingestion/sources/s3.md"},sidebar:"overviewSidebar",previous:{title:"Redshift",permalink:"/docs/generated/ingestion/sources/redshift"},next:{title:"SageMaker",permalink:"/docs/generated/ingestion/sources/sagemaker"}},c={},g=[{value:"Supported file types",id:"supported-file-types",level:3},{value:"Concept Mapping",id:"concept-mapping",level:3},{value:"Profiling",id:"profiling",level:3},{value:"Important Capabilities",id:"important-capabilities",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"CLI based Ingestion",id:"cli-based-ingestion",level:3},{value:"Starter Recipe",id:"starter-recipe",level:3},{value:"Config Details",id:"config-details",level:3},{value:"Path Specs",id:"path-specs",level:3},{value:"Partitioned Dataset support",id:"partitioned-dataset-support",level:4},{value:"Path Specs - Examples",id:"path-specs---examples",level:3},{value:"Example 1 - Individual file as Dataset",id:"example-1---individual-file-as-dataset",level:4},{value:"Example 2 - Folder of files as Dataset (without Partitions)",id:"example-2---folder-of-files-as-dataset-without-partitions",level:4},{value:"Example 3 - Folder of files as Dataset (with Partitions)",id:"example-3---folder-of-files-as-dataset-with-partitions",level:4},{value:"Example 4 - Folder of files as Dataset (with Partitions), and Exclude Filter",id:"example-4---folder-of-files-as-dataset-with-partitions-and-exclude-filter",level:4},{value:"Example 5 - Advanced - Either Individual file OR Folder of files as Dataset",id:"example-5---advanced---either-individual-file-or-folder-of-files-as-dataset",level:4},{value:"Compatibility",id:"compatibility",level:3},{value:"Code Coordinates",id:"code-coordinates",level:3}],y={toc:g},f="wrapper";function u(e){var{components:a}=e,t=o(e,["components"]);return(0,n.yg)(f,r(function(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{},n=Object.keys(t);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(t).filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable})))),n.forEach((function(a){s(e,a,t[a])}))}return e}({},y,t),{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"s3--local-files"},"S3 / Local Files"),(0,n.yg)("p",null,"This connector ingests AWS S3 datasets into DataHub. It allows mapping an individual file or a folder of files to a dataset in DataHub.\nRefer to the section ",(0,n.yg)("a",{parentName:"p",href:"/docs/generated/ingestion/sources/s3/#path-specs"},"Path Specs")," for more details."),(0,n.yg)("admonition",{type:"tip"},(0,n.yg)("p",{parentName:"admonition"},"This connector can also be used to ingest local files.\nJust replace ",(0,n.yg)("inlineCode",{parentName:"p"},"s3://")," in your path_specs with an absolute path to files on the machine running ingestion.")),(0,n.yg)("h3",{id:"supported-file-types"},"Supported file types"),(0,n.yg)("p",null,"Supported file types are as follows:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"CSV (","*",".csv)"),(0,n.yg)("li",{parentName:"ul"},"TSV (","*",".tsv)"),(0,n.yg)("li",{parentName:"ul"},"JSONL (","*",".jsonl)"),(0,n.yg)("li",{parentName:"ul"},"JSON (","*",".json)"),(0,n.yg)("li",{parentName:"ul"},"Parquet (","*",".parquet)"),(0,n.yg)("li",{parentName:"ul"},"Apache Avro (","*",".avro)")),(0,n.yg)("p",null,"Schemas for Parquet and Avro files are extracted as provided."),(0,n.yg)("p",null,"Schemas for schemaless formats (CSV, TSV, JSONL, JSON) are inferred. For CSV, TSV and JSONL files, we consider the first 100 rows by default, which can be controlled via the ",(0,n.yg)("inlineCode",{parentName:"p"},"max_rows")," recipe parameter (see ",(0,n.yg)("a",{parentName:"p",href:"#config-details"},"below"),")\nJSON file schemas are inferred on the basis of the entire file (given the difficulty in extracting only the first few objects of the file), which may impact performance.\nWe are working on using iterator-based JSON parsers to avoid reading in the entire JSON object."),(0,n.yg)("h3",{id:"concept-mapping"},"Concept Mapping"),(0,n.yg)("p",null,"This ingestion source maps the following Source System Concepts to DataHub Concepts:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Source Concept"),(0,n.yg)("th",{parentName:"tr",align:null},"DataHub Concept"),(0,n.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},'"s3"')),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/metamodel/entities/dataplatform/"},"Data Platform")),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"s3 object / Folder containing s3 objects"),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/metamodel/entities/dataset/"},"Dataset")),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"s3 bucket"),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/metamodel/entities/container/"},"Container")),(0,n.yg)("td",{parentName:"tr",align:null},"Subtype ",(0,n.yg)("inlineCode",{parentName:"td"},"S3 bucket"))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"s3 folder"),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/metamodel/entities/container/"},"Container")),(0,n.yg)("td",{parentName:"tr",align:null},"Subtype ",(0,n.yg)("inlineCode",{parentName:"td"},"Folder"))))),(0,n.yg)("h3",{id:"profiling"},"Profiling"),(0,n.yg)("p",null,"This plugin extracts:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Row and column counts for each dataset"),(0,n.yg)("li",{parentName:"ul"},"For each column, if profiling is enabled:",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"null counts and proportions"),(0,n.yg)("li",{parentName:"ul"},"distinct counts and proportions"),(0,n.yg)("li",{parentName:"ul"},"minimum, maximum, mean, median, standard deviation, some quantile values"),(0,n.yg)("li",{parentName:"ul"},"histograms or frequencies of unique values")))),(0,n.yg)("p",null,"Note that because the profiling is run with PySpark, we require Spark 3.0.3 with Hadoop 3.2 to be installed (see ",(0,n.yg)("a",{parentName:"p",href:"#compatibility"},"compatibility")," for more details). If profiling, make sure that permissions for ",(0,n.yg)("strong",{parentName:"p"},"s3a://")," access are set because Spark and Hadoop use the s3a:// protocol to interface with AWS (schema inference outside of profiling requires s3:// access).\nEnabling profiling will slow down ingestion runs.\n",(0,n.yg)("img",{parentName:"p",src:"https://img.shields.io/badge/support%20status-certified-brightgreen",alt:"Certified"})),(0,n.yg)("h3",{id:"important-capabilities"},"Important Capabilities"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Capability"),(0,n.yg)("th",{parentName:"tr",align:null},"Status"),(0,n.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Asset Containers"),(0,n.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,n.yg)("td",{parentName:"tr",align:null},"Enabled by default. Supported for types - Folder, S3 bucket.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/metadata-ingestion/docs/dev_guides/sql_profiles"},"Data Profiling")),(0,n.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,n.yg)("td",{parentName:"tr",align:null},"Optionally enabled via configuration.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/metadata-ingestion/docs/dev_guides/stateful#stale-entity-removal"},"Detect Deleted Entities")),(0,n.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,n.yg)("td",{parentName:"tr",align:null},"Enabled by default via stateful ingestion.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Extract Tags"),(0,n.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,n.yg)("td",{parentName:"tr",align:null},"Can extract S3 object/bucket tags if enabled.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Schema Metadata"),(0,n.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,n.yg)("td",{parentName:"tr",align:null},"Can infer schema from supported file types.")))),(0,n.yg)("h3",{id:"prerequisites"},"Prerequisites"),(0,n.yg)("p",null,"To allow the S3 ingestion source ingest metadata from an S3 bucket, you need to grant the necessary permissions to an IAM user or role. Follow these steps:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Create an IAM Policy"),": Create a policy that grants read access to the S3 bucket.")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "Version": "2012-10-17",\n  "Statement": [\n    {\n      "Sid": "VisualEditor0",\n      "Effect": "Allow",\n      "Action": ["s3:ListBucket", "s3:GetBucketLocation", "s3:GetObject"],\n      "Resource": [\n        "arn:aws:s3:::your-bucket-name",\n        "arn:aws:s3:::your-bucket-name/*"\n      ]\n    }\n  ]\n}\n')),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Permissions Explanation"),":"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"s3:ListBucket"),": Allows listing the objects in the bucket. This permission is necessary for the S3 ingestion source to know which objects are available to read."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"s3:GetBucketLocation"),": Allows retrieving the location of the bucket."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"s3:GetObject"),": Allows reading the actual content of the objects in the bucket. This is needed to infer schema from sample files.")),(0,n.yg)("ol",{start:2},(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Attach the Policy to an IAM User or Role"),": Attach the created policy to the IAM user or role that the S3 ingestion source will use.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Configure the S3 Ingestion Source"),": Configure the user in s3 ingestion who you attached the role above."))),(0,n.yg)("h3",{id:"cli-based-ingestion"},"CLI based Ingestion"),(0,n.yg)("h3",{id:"starter-recipe"},"Starter Recipe"),(0,n.yg)("p",null,"Check out the following recipe to get started with ingestion! See ",(0,n.yg)("a",{parentName:"p",href:"#config-details"},"below")," for full configuration options."),(0,n.yg)("p",null,"For general pointers on writing and running a recipe, see our ",(0,n.yg)("a",{parentName:"p",href:"/docs/metadata-ingestion#recipes"},"main recipe guide"),"."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'# Ingest data from S3\nsource:\n  type: s3\n  config:\n    path_specs:\n      - include: "s3://covid19-lake/covid_knowledge_graph/csv/nodes/*.*"\n\n    aws_config:\n      aws_access_key_id: *****\n      aws_secret_access_key: *****\n      aws_region: us-east-2\n    env: "PROD"\n    profiling:\n      enabled: false\n\n# Ingest data from local filesystem\nsource:\n  type: s3\n  config:\n    path_specs:\n      - include: "/absolute/path/*.csv"\n\n')),(0,n.yg)("h3",{id:"config-details"},"Config Details"),(0,n.yg)(l.A,{mdxType:"Tabs"},(0,n.yg)(i.A,{value:"options",label:"Options",default:!0,mdxType:"TabItem"},(0,n.yg)("p",null,"Note that a ",(0,n.yg)("inlineCode",{parentName:"p"},".")," is used to denote nested fields in the YAML recipe."),(0,n.yg)("div",{className:"config-table"},(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:"left"},"Field"),(0,n.yg)("th",{parentName:"tr",align:"left"},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"path_specs"),"\xa0",(0,n.yg)("abbr",{title:"Required"},"\u2705"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"array"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"List of PathSpec. See ",(0,n.yg)("a",{parentName:"td",href:"#path-spec"},"below")," the details about PathSpec")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs."),(0,n.yg)("span",{className:"path-main"},"PathSpec"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"PathSpec"))),(0,n.yg)("td",{parentName:"tr",align:"left"})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"include"),"\xa0",(0,n.yg)("abbr",{title:"Required if PathSpec is set"},"\u2753"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Path to table. Name variable ",(0,n.yg)("inlineCode",{parentName:"td"},"{table}")," is used to mark the folder with dataset. In absence of ",(0,n.yg)("inlineCode",{parentName:"td"},"{table}"),", file level dataset will be created. Check below examples for more details.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"allow_double_stars"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Allow double stars in the include path. This can affect performance significantly if enabled ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"autodetect_partitions"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Autodetect partition(s) from the path. If set to true, it will autodetect partition key/value if the folder format is {partition_key}={partition_value} for example ",(0,n.yg)("inlineCode",{parentName:"td"},"year=2024")," ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"default_extension"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of string, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"For files without extension it will assume the specified file type. If it is not set the files without extensions will be skipped. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"enable_compression"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Enable or disable processing compressed files. Currently .gz and .bz files are supported. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"include_hidden_folders"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Include hidden folders in the traversal (folders starting with . or _ ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"sample_files"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Not listing all the files but only taking a handful amount of sample file to infer the schema. File count and file size calculation will be disabled. This can affect performance significantly if enabled ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"table_name"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of string, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Display name of the dataset.Combination of named variables from include path and strings ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"traversal_method"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"Enum"))),(0,n.yg)("td",{parentName:"tr",align:"left"},'One of: "ALL", "MIN_MAX", "MAX"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"exclude"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of array, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"list of paths in glob pattern which will be excluded while scanning for the datasets ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"[","]")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec.exclude."),(0,n.yg)("span",{className:"path-main"},"string"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"file_types"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"array"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Files with extenstions specified here (subset of default value) only will be scanned to create dataset. Other files will be omitted. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"[","'","csv","'",", ","'","tsv","'",", ","'","json","'",", ","'","parquet","'",", ","'","avro","'","]")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec.file_types."),(0,n.yg)("span",{className:"path-main"},"string"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"tables_filter_pattern"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"AllowDenyPattern"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"A class to store allow deny regexes")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec.tables_filter_pattern."),(0,n.yg)("span",{className:"path-main"},"ignoreCase"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of boolean, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to ignore case sensitivity during pattern matching. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"add_partition_columns_to_schema"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to add partition fields to the schema. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"convert_urns_to_lowercase"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to convert dataset urns to lowercase. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"generate_partition_aspects"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to generate partition aspects for partitioned tables. On older servers for backward compatibility, this should be set to False. This flag will be removed in future versions. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"max_rows"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"integer"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Maximum number of rows to use when inferring schemas for TSV and CSV files. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"100")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"number_of_files_to_sample"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"integer"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Number of files to list to sample for schema inference. This will be ignored if sample_files is set to False in the pathspec. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"100")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"platform"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"The platform that this source connects to (either 's3' or 'file'). If not specified, the platform will be inferred from the path_specs. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"})))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"platform_instance"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of string, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"The instance of the platform that all assets produced by this recipe belong to. This should be unique within the platform. See ",(0,n.yg)("a",{parentName:"td",href:"https://docs.datahub.com/docs/platform-instances/"},"https://docs.datahub.com/docs/platform-instances/")," for more details. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"sort_schema_fields"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to sort schema fields by fieldPath when inferring schemas. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"spark_config"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"object"))),(0,n.yg)("td",{parentName:"tr",align:"left"},'Spark configuration properties to set on the SparkSession. Put config property names into quotes. For example: \'"spark.executor.memory": "2g"\' ',(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"{","}")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"spark_driver_memory"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Max amount of memory to grant Spark. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"4g")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"use_s3_bucket_tags"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of boolean, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether or not to create tags in datahub from the s3 bucket ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"use_s3_content_type"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"If enabled, use S3 Object metadata to determine content type over file extension, if set. Warning: this requires a separate query to S3 for each object, which can be slow for large datasets. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"use_s3_object_tags"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of boolean, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether or not to create tags in datahub from the s3 object ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"verify_ssl"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of boolean, string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Either a boolean, in which case it controls whether we verify the server's TLS certificate, or a string, in which case it must be a path to a CA bundle to use. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"env"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"The environment that all assets produced by this connector belong to ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"PROD")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"aws_config"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of AwsConnectionConfig, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"AWS configuration ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"aws_config."),(0,n.yg)("span",{className:"path-main"},"aws_access_key_id"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of string, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"AWS access key ID. Can be auto-detected, see ",(0,n.yg)("a",{parentName:"td",href:"https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html"},"the AWS boto3 docs")," for details. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"aws_config."),(0,n.yg)("span",{className:"path-main"},"aws_advanced_config"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"object"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Advanced AWS configuration options. These are passed directly to ",(0,n.yg)("a",{parentName:"td",href:"https://botocore.amazonaws.com/v1/documentation/api/latest/reference/config.html"},"botocore.config.Config"),".")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"aws_config."),(0,n.yg)("span",{className:"path-main"},"aws_endpoint_url"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of string, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"The AWS service endpoint. This is normally ",(0,n.yg)("a",{parentName:"td",href:"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html"},"constructed automatically"),", but can be overridden here. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"aws_config."),(0,n.yg)("span",{className:"path-main"},"aws_profile"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of string, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"The ",(0,n.yg)("a",{parentName:"td",href:"https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html"},"named profile")," to use from AWS credentials. Falls back to default profile if not specified and no access keys provided. Profiles are configured in ~/.aws/credentials or ~/.aws/config. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"aws_config."),(0,n.yg)("span",{className:"path-main"},"aws_proxy"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of string, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"A set of proxy configs to use with AWS. See the ",(0,n.yg)("a",{parentName:"td",href:"https://botocore.amazonaws.com/v1/documentation/api/latest/reference/config.html"},"botocore.config")," docs for details. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"aws_config."),(0,n.yg)("span",{className:"path-main"},"aws_region"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of string, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"AWS region code. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"aws_config."),(0,n.yg)("span",{className:"path-main"},"aws_retry_mode"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"Enum"))),(0,n.yg)("td",{parentName:"tr",align:"left"},'One of: "legacy", "standard", "adaptive" ',(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"standard")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"aws_config."),(0,n.yg)("span",{className:"path-main"},"aws_retry_num"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"integer"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Number of times to retry failed AWS requests. See the ",(0,n.yg)("a",{parentName:"td",href:"https://boto3.amazonaws.com/v1/documentation/api/latest/guide/retries.html"},"botocore.retry")," docs for details. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"5")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"aws_config."),(0,n.yg)("span",{className:"path-main"},"aws_secret_access_key"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of string(password), null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"AWS secret access key. Can be auto-detected, see ",(0,n.yg)("a",{parentName:"td",href:"https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html"},"the AWS boto3 docs")," for details. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"aws_config."),(0,n.yg)("span",{className:"path-main"},"aws_session_token"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of string(password), null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"AWS session token. Can be auto-detected, see ",(0,n.yg)("a",{parentName:"td",href:"https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html"},"the AWS boto3 docs")," for details. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"aws_config."),(0,n.yg)("span",{className:"path-main"},"read_timeout"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"number"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"The timeout for reading from the connection (in seconds). ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"60")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"aws_config."),(0,n.yg)("span",{className:"path-main"},"aws_role"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of string, array, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"AWS roles to assume. If using the string format, the role ARN can be specified directly. If using the object format, the role can be specified in the RoleArn field and additional available arguments are the same as ",(0,n.yg)("a",{parentName:"td",href:"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sts.html?highlight=assume_role#STS.Client.assume_role"},"boto3's STS.Client.assume_role"),". ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"aws_config.aws_role."),(0,n.yg)("span",{className:"path-main"},"union"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of string, AwsAssumeRoleConfig"))),(0,n.yg)("td",{parentName:"tr",align:"left"})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"aws_config.aws_role.union."),(0,n.yg)("span",{className:"path-main"},"RoleArn"),"\xa0",(0,n.yg)("abbr",{title:"Required if union is set"},"\u2753"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"ARN of the role to assume.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"aws_config.aws_role.union."),(0,n.yg)("span",{className:"path-main"},"ExternalId"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of string, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"External ID to use when assuming the role. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"profile_patterns"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"AllowDenyPattern"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"A class to store allow deny regexes")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profile_patterns."),(0,n.yg)("span",{className:"path-main"},"ignoreCase"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of boolean, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to ignore case sensitivity during pattern matching. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profile_patterns."),(0,n.yg)("span",{className:"path-main"},"allow"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"array"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"List of regex patterns to include in ingestion ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"[","'",".","*","'","]")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profile_patterns.allow."),(0,n.yg)("span",{className:"path-main"},"string"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profile_patterns."),(0,n.yg)("span",{className:"path-main"},"deny"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"array"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"List of regex patterns to exclude from ingestion. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"[","]")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profile_patterns.deny."),(0,n.yg)("span",{className:"path-main"},"string"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"profiling"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"DataLakeProfilerConfig"))),(0,n.yg)("td",{parentName:"tr",align:"left"})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"enabled"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether profiling should be done. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_distinct_value_frequencies"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for distinct value frequencies. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_histogram"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the histogram for numeric fields. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_max_value"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the max value of numeric columns. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_mean_value"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the mean value of numeric columns. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_median_value"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the median value of numeric columns. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_min_value"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the min value of numeric columns. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_null_count"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the number of nulls for each column. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_quantiles"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the quantiles of numeric columns. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_sample_values"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the sample values for all columns. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_stddev_value"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the standard deviation of numeric columns. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"max_number_of_fields_to_profile"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of integer, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"A positive integer that specifies the maximum number of columns to profile for any table. ",(0,n.yg)("inlineCode",{parentName:"td"},"None")," implies all columns. The cost of profiling goes up significantly as the number of columns to profile goes up. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"profile_table_level_only"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to perform profiling at table-level only or include column-level profiling as well. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"operation_config"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"OperationConfig"))),(0,n.yg)("td",{parentName:"tr",align:"left"})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling.operation_config."),(0,n.yg)("span",{className:"path-main"},"lower_freq_profile_enabled"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to do profiling at lower freq or not. This does not do any scheduling just adds additional checks to when not to run profiling. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling.operation_config."),(0,n.yg)("span",{className:"path-main"},"profile_date_of_month"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of integer, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Number between 1 to 31 for date of month (both inclusive). If not specified, defaults to Nothing and this field does not take affect. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling.operation_config."),(0,n.yg)("span",{className:"path-main"},"profile_day_of_week"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of integer, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Number between 0 to 6 for day of week (both inclusive). 0 is Monday and 6 is Sunday. If not specified, defaults to Nothing and this field does not take affect. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"stateful_ingestion"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of StatefulStaleMetadataRemovalConfig, null"))),(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"default-line "},"Default: ",(0,n.yg)("span",{className:"default-value"},"None")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"stateful_ingestion."),(0,n.yg)("span",{className:"path-main"},"enabled"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether or not to enable stateful ingest. Default: True if a pipeline_name is set and either a datahub-rest sink or ",(0,n.yg)("inlineCode",{parentName:"td"},"datahub_api")," is specified, otherwise False ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"stateful_ingestion."),(0,n.yg)("span",{className:"path-main"},"fail_safe_threshold"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"number"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Prevents large amount of soft deletes & the state from committing from accidental changes to the source configuration if the relative change percent in entities compared to the previous state is above the 'fail_safe_threshold'. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"75.0")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"stateful_ingestion."),(0,n.yg)("span",{className:"path-main"},"remove_stale_metadata"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Soft-deletes the entities present in the last successful run but missing in the current run with stateful_ingestion enabled. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))))))),(0,n.yg)(i.A,{value:"schema",label:"Schema",mdxType:"TabItem"},(0,n.yg)("p",null,"The ",(0,n.yg)("a",{parentName:"p",href:"https://json-schema.org/"},"JSONSchema")," for this configuration is inlined below."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-javascript"},'{\n  "$defs": {\n    "AllowDenyPattern": {\n      "additionalProperties": false,\n      "description": "A class to store allow deny regexes",\n      "properties": {\n        "allow": {\n          "default": [\n            ".*"\n          ],\n          "description": "List of regex patterns to include in ingestion",\n          "items": {\n            "type": "string"\n          },\n          "title": "Allow",\n          "type": "array"\n        },\n        "deny": {\n          "default": [],\n          "description": "List of regex patterns to exclude from ingestion.",\n          "items": {\n            "type": "string"\n          },\n          "title": "Deny",\n          "type": "array"\n        },\n        "ignoreCase": {\n          "anyOf": [\n            {\n              "type": "boolean"\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": true,\n          "description": "Whether to ignore case sensitivity during pattern matching.",\n          "title": "Ignorecase"\n        }\n      },\n      "title": "AllowDenyPattern",\n      "type": "object"\n    },\n    "AwsAssumeRoleConfig": {\n      "additionalProperties": true,\n      "properties": {\n        "RoleArn": {\n          "description": "ARN of the role to assume.",\n          "title": "Rolearn",\n          "type": "string"\n        },\n        "ExternalId": {\n          "anyOf": [\n            {\n              "type": "string"\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": null,\n          "description": "External ID to use when assuming the role.",\n          "title": "Externalid"\n        }\n      },\n      "required": [\n        "RoleArn"\n      ],\n      "title": "AwsAssumeRoleConfig",\n      "type": "object"\n    },\n    "AwsConnectionConfig": {\n      "additionalProperties": false,\n      "description": "Common AWS credentials config.\\n\\nCurrently used by:\\n    - Glue source\\n    - SageMaker source\\n    - dbt source",\n      "properties": {\n        "aws_access_key_id": {\n          "anyOf": [\n            {\n              "type": "string"\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": null,\n          "description": "AWS access key ID. Can be auto-detected, see [the AWS boto3 docs](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) for details.",\n          "title": "Aws Access Key Id"\n        },\n        "aws_secret_access_key": {\n          "anyOf": [\n            {\n              "format": "password",\n              "type": "string",\n              "writeOnly": true\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": null,\n          "description": "AWS secret access key. Can be auto-detected, see [the AWS boto3 docs](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) for details.",\n          "title": "Aws Secret Access Key"\n        },\n        "aws_session_token": {\n          "anyOf": [\n            {\n              "format": "password",\n              "type": "string",\n              "writeOnly": true\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": null,\n          "description": "AWS session token. Can be auto-detected, see [the AWS boto3 docs](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) for details.",\n          "title": "Aws Session Token"\n        },\n        "aws_role": {\n          "anyOf": [\n            {\n              "type": "string"\n            },\n            {\n              "items": {\n                "anyOf": [\n                  {\n                    "type": "string"\n                  },\n                  {\n                    "$ref": "#/$defs/AwsAssumeRoleConfig"\n                  }\n                ]\n              },\n              "type": "array"\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": null,\n          "description": "AWS roles to assume. If using the string format, the role ARN can be specified directly. If using the object format, the role can be specified in the RoleArn field and additional available arguments are the same as [boto3\'s STS.Client.assume_role](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sts.html?highlight=assume_role#STS.Client.assume_role).",\n          "title": "Aws Role"\n        },\n        "aws_profile": {\n          "anyOf": [\n            {\n              "type": "string"\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": null,\n          "description": "The [named profile](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html) to use from AWS credentials. Falls back to default profile if not specified and no access keys provided. Profiles are configured in ~/.aws/credentials or ~/.aws/config.",\n          "title": "Aws Profile"\n        },\n        "aws_region": {\n          "anyOf": [\n            {\n              "type": "string"\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": null,\n          "description": "AWS region code.",\n          "title": "Aws Region"\n        },\n        "aws_endpoint_url": {\n          "anyOf": [\n            {\n              "type": "string"\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": null,\n          "description": "The AWS service endpoint. This is normally [constructed automatically](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html), but can be overridden here.",\n          "title": "Aws Endpoint Url"\n        },\n        "aws_proxy": {\n          "anyOf": [\n            {\n              "additionalProperties": {\n                "type": "string"\n              },\n              "type": "object"\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": null,\n          "description": "A set of proxy configs to use with AWS. See the [botocore.config](https://botocore.amazonaws.com/v1/documentation/api/latest/reference/config.html) docs for details.",\n          "title": "Aws Proxy"\n        },\n        "aws_retry_num": {\n          "default": 5,\n          "description": "Number of times to retry failed AWS requests. See the [botocore.retry](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/retries.html) docs for details.",\n          "title": "Aws Retry Num",\n          "type": "integer"\n        },\n        "aws_retry_mode": {\n          "default": "standard",\n          "description": "Retry mode to use for failed AWS requests. See the [botocore.retry](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/retries.html) docs for details.",\n          "enum": [\n            "legacy",\n            "standard",\n            "adaptive"\n          ],\n          "title": "Aws Retry Mode",\n          "type": "string"\n        },\n        "read_timeout": {\n          "default": 60,\n          "description": "The timeout for reading from the connection (in seconds).",\n          "title": "Read Timeout",\n          "type": "number"\n        },\n        "aws_advanced_config": {\n          "additionalProperties": true,\n          "description": "Advanced AWS configuration options. These are passed directly to [botocore.config.Config](https://botocore.amazonaws.com/v1/documentation/api/latest/reference/config.html).",\n          "title": "Aws Advanced Config",\n          "type": "object"\n        }\n      },\n      "title": "AwsConnectionConfig",\n      "type": "object"\n    },\n    "DataLakeProfilerConfig": {\n      "additionalProperties": false,\n      "properties": {\n        "enabled": {\n          "default": false,\n          "description": "Whether profiling should be done.",\n          "title": "Enabled",\n          "type": "boolean"\n        },\n        "operation_config": {\n          "$ref": "#/$defs/OperationConfig",\n          "description": "Experimental feature. To specify operation configs."\n        },\n        "profile_table_level_only": {\n          "default": false,\n          "description": "Whether to perform profiling at table-level only or include column-level profiling as well.",\n          "title": "Profile Table Level Only",\n          "type": "boolean"\n        },\n        "max_number_of_fields_to_profile": {\n          "anyOf": [\n            {\n              "exclusiveMinimum": 0,\n              "type": "integer"\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": null,\n          "description": "A positive integer that specifies the maximum number of columns to profile for any table. `None` implies all columns. The cost of profiling goes up significantly as the number of columns to profile goes up.",\n          "title": "Max Number Of Fields To Profile"\n        },\n        "include_field_null_count": {\n          "default": true,\n          "description": "Whether to profile for the number of nulls for each column.",\n          "title": "Include Field Null Count",\n          "type": "boolean"\n        },\n        "include_field_min_value": {\n          "default": true,\n          "description": "Whether to profile for the min value of numeric columns.",\n          "title": "Include Field Min Value",\n          "type": "boolean"\n        },\n        "include_field_max_value": {\n          "default": true,\n          "description": "Whether to profile for the max value of numeric columns.",\n          "title": "Include Field Max Value",\n          "type": "boolean"\n        },\n        "include_field_mean_value": {\n          "default": true,\n          "description": "Whether to profile for the mean value of numeric columns.",\n          "title": "Include Field Mean Value",\n          "type": "boolean"\n        },\n        "include_field_median_value": {\n          "default": true,\n          "description": "Whether to profile for the median value of numeric columns.",\n          "title": "Include Field Median Value",\n          "type": "boolean"\n        },\n        "include_field_stddev_value": {\n          "default": true,\n          "description": "Whether to profile for the standard deviation of numeric columns.",\n          "title": "Include Field Stddev Value",\n          "type": "boolean"\n        },\n        "include_field_quantiles": {\n          "default": true,\n          "description": "Whether to profile for the quantiles of numeric columns.",\n          "title": "Include Field Quantiles",\n          "type": "boolean"\n        },\n        "include_field_distinct_value_frequencies": {\n          "default": true,\n          "description": "Whether to profile for distinct value frequencies.",\n          "title": "Include Field Distinct Value Frequencies",\n          "type": "boolean"\n        },\n        "include_field_histogram": {\n          "default": true,\n          "description": "Whether to profile for the histogram for numeric fields.",\n          "title": "Include Field Histogram",\n          "type": "boolean"\n        },\n        "include_field_sample_values": {\n          "default": true,\n          "description": "Whether to profile for the sample values for all columns.",\n          "title": "Include Field Sample Values",\n          "type": "boolean"\n        }\n      },\n      "title": "DataLakeProfilerConfig",\n      "type": "object"\n    },\n    "FolderTraversalMethod": {\n      "enum": [\n        "ALL",\n        "MIN_MAX",\n        "MAX"\n      ],\n      "title": "FolderTraversalMethod",\n      "type": "string"\n    },\n    "OperationConfig": {\n      "additionalProperties": false,\n      "properties": {\n        "lower_freq_profile_enabled": {\n          "default": false,\n          "description": "Whether to do profiling at lower freq or not. This does not do any scheduling just adds additional checks to when not to run profiling.",\n          "title": "Lower Freq Profile Enabled",\n          "type": "boolean"\n        },\n        "profile_day_of_week": {\n          "anyOf": [\n            {\n              "type": "integer"\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": null,\n          "description": "Number between 0 to 6 for day of week (both inclusive). 0 is Monday and 6 is Sunday. If not specified, defaults to Nothing and this field does not take affect.",\n          "title": "Profile Day Of Week"\n        },\n        "profile_date_of_month": {\n          "anyOf": [\n            {\n              "type": "integer"\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": null,\n          "description": "Number between 1 to 31 for date of month (both inclusive). If not specified, defaults to Nothing and this field does not take affect.",\n          "title": "Profile Date Of Month"\n        }\n      },\n      "title": "OperationConfig",\n      "type": "object"\n    },\n    "PathSpec": {\n      "additionalProperties": false,\n      "properties": {\n        "include": {\n          "description": "Path to table. Name variable `{table}` is used to mark the folder with dataset. In absence of `{table}`, file level dataset will be created. Check below examples for more details.",\n          "title": "Include",\n          "type": "string"\n        },\n        "exclude": {\n          "anyOf": [\n            {\n              "items": {\n                "type": "string"\n              },\n              "type": "array"\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": [],\n          "description": "list of paths in glob pattern which will be excluded while scanning for the datasets",\n          "title": "Exclude"\n        },\n        "file_types": {\n          "default": [\n            "csv",\n            "tsv",\n            "json",\n            "parquet",\n            "avro"\n          ],\n          "description": "Files with extenstions specified here (subset of default value) only will be scanned to create dataset. Other files will be omitted.",\n          "items": {\n            "type": "string"\n          },\n          "title": "File Types",\n          "type": "array"\n        },\n        "default_extension": {\n          "anyOf": [\n            {\n              "type": "string"\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": null,\n          "description": "For files without extension it will assume the specified file type. If it is not set the files without extensions will be skipped.",\n          "title": "Default Extension"\n        },\n        "table_name": {\n          "anyOf": [\n            {\n              "type": "string"\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": null,\n          "description": "Display name of the dataset.Combination of named variables from include path and strings",\n          "title": "Table Name"\n        },\n        "enable_compression": {\n          "default": true,\n          "description": "Enable or disable processing compressed files. Currently .gz and .bz files are supported.",\n          "title": "Enable Compression",\n          "type": "boolean"\n        },\n        "sample_files": {\n          "default": true,\n          "description": "Not listing all the files but only taking a handful amount of sample file to infer the schema. File count and file size calculation will be disabled. This can affect performance significantly if enabled",\n          "title": "Sample Files",\n          "type": "boolean"\n        },\n        "allow_double_stars": {\n          "default": false,\n          "description": "Allow double stars in the include path. This can affect performance significantly if enabled",\n          "title": "Allow Double Stars",\n          "type": "boolean"\n        },\n        "autodetect_partitions": {\n          "default": true,\n          "description": "Autodetect partition(s) from the path. If set to true, it will autodetect partition key/value if the folder format is {partition_key}={partition_value} for example `year=2024`",\n          "title": "Autodetect Partitions",\n          "type": "boolean"\n        },\n        "traversal_method": {\n          "$ref": "#/$defs/FolderTraversalMethod",\n          "default": "MAX",\n          "description": "Method to traverse the folder. ALL: Traverse all the folders, MIN_MAX: Traverse the folders by finding min and max value, MAX: Traverse the folder with max value"\n        },\n        "include_hidden_folders": {\n          "default": false,\n          "description": "Include hidden folders in the traversal (folders starting with . or _",\n          "title": "Include Hidden Folders",\n          "type": "boolean"\n        },\n        "tables_filter_pattern": {\n          "$ref": "#/$defs/AllowDenyPattern",\n          "default": {\n            "allow": [\n              ".*"\n            ],\n            "deny": [],\n            "ignoreCase": true\n          },\n          "description": "The tables_filter_pattern configuration field uses regular expressions to filter the tables part of the Pathspec for ingestion, allowing fine-grained control over which tables are included or excluded based on specified patterns. The default setting allows all tables."\n        }\n      },\n      "required": [\n        "include"\n      ],\n      "title": "PathSpec",\n      "type": "object"\n    },\n    "StatefulStaleMetadataRemovalConfig": {\n      "additionalProperties": false,\n      "description": "Base specialized config for Stateful Ingestion with stale metadata removal capability.",\n      "properties": {\n        "enabled": {\n          "default": false,\n          "description": "Whether or not to enable stateful ingest. Default: True if a pipeline_name is set and either a datahub-rest sink or `datahub_api` is specified, otherwise False",\n          "title": "Enabled",\n          "type": "boolean"\n        },\n        "remove_stale_metadata": {\n          "default": true,\n          "description": "Soft-deletes the entities present in the last successful run but missing in the current run with stateful_ingestion enabled.",\n          "title": "Remove Stale Metadata",\n          "type": "boolean"\n        },\n        "fail_safe_threshold": {\n          "default": 75.0,\n          "description": "Prevents large amount of soft deletes & the state from committing from accidental changes to the source configuration if the relative change percent in entities compared to the previous state is above the \'fail_safe_threshold\'.",\n          "maximum": 100.0,\n          "minimum": 0.0,\n          "title": "Fail Safe Threshold",\n          "type": "number"\n        }\n      },\n      "title": "StatefulStaleMetadataRemovalConfig",\n      "type": "object"\n    }\n  },\n  "additionalProperties": false,\n  "properties": {\n    "convert_urns_to_lowercase": {\n      "default": false,\n      "description": "Whether to convert dataset urns to lowercase.",\n      "title": "Convert Urns To Lowercase",\n      "type": "boolean"\n    },\n    "path_specs": {\n      "description": "List of PathSpec. See [below](#path-spec) the details about PathSpec",\n      "items": {\n        "$ref": "#/$defs/PathSpec"\n      },\n      "title": "Path Specs",\n      "type": "array"\n    },\n    "env": {\n      "default": "PROD",\n      "description": "The environment that all assets produced by this connector belong to",\n      "title": "Env",\n      "type": "string"\n    },\n    "platform_instance": {\n      "anyOf": [\n        {\n          "type": "string"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "The instance of the platform that all assets produced by this recipe belong to. This should be unique within the platform. See https://docs.datahub.com/docs/platform-instances/ for more details.",\n      "title": "Platform Instance"\n    },\n    "stateful_ingestion": {\n      "anyOf": [\n        {\n          "$ref": "#/$defs/StatefulStaleMetadataRemovalConfig"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null\n    },\n    "platform": {\n      "default": "",\n      "description": "The platform that this source connects to (either \'s3\' or \'file\'). If not specified, the platform will be inferred from the path_specs.",\n      "title": "Platform",\n      "type": "string"\n    },\n    "aws_config": {\n      "anyOf": [\n        {\n          "$ref": "#/$defs/AwsConnectionConfig"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "AWS configuration"\n    },\n    "use_s3_bucket_tags": {\n      "anyOf": [\n        {\n          "type": "boolean"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "Whether or not to create tags in datahub from the s3 bucket",\n      "title": "Use S3 Bucket Tags"\n    },\n    "use_s3_object_tags": {\n      "anyOf": [\n        {\n          "type": "boolean"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "Whether or not to create tags in datahub from the s3 object",\n      "title": "Use S3 Object Tags"\n    },\n    "use_s3_content_type": {\n      "default": false,\n      "description": "If enabled, use S3 Object metadata to determine content type over file extension, if set. Warning: this requires a separate query to S3 for each object, which can be slow for large datasets.",\n      "title": "Use S3 Content Type",\n      "type": "boolean"\n    },\n    "profile_patterns": {\n      "$ref": "#/$defs/AllowDenyPattern",\n      "default": {\n        "allow": [\n          ".*"\n        ],\n        "deny": [],\n        "ignoreCase": true\n      },\n      "description": "regex patterns for tables to profile "\n    },\n    "profiling": {\n      "$ref": "#/$defs/DataLakeProfilerConfig",\n      "default": {\n        "enabled": false,\n        "operation_config": {\n          "lower_freq_profile_enabled": false,\n          "profile_date_of_month": null,\n          "profile_day_of_week": null\n        },\n        "profile_table_level_only": false,\n        "max_number_of_fields_to_profile": null,\n        "include_field_null_count": true,\n        "include_field_min_value": true,\n        "include_field_max_value": true,\n        "include_field_mean_value": true,\n        "include_field_median_value": true,\n        "include_field_stddev_value": true,\n        "include_field_quantiles": true,\n        "include_field_distinct_value_frequencies": true,\n        "include_field_histogram": true,\n        "include_field_sample_values": true\n      },\n      "description": "Data profiling configuration"\n    },\n    "spark_driver_memory": {\n      "default": "4g",\n      "description": "Max amount of memory to grant Spark.",\n      "title": "Spark Driver Memory",\n      "type": "string"\n    },\n    "spark_config": {\n      "additionalProperties": true,\n      "default": {},\n      "description": "Spark configuration properties to set on the SparkSession. Put config property names into quotes. For example: \'\\"spark.executor.memory\\": \\"2g\\"\'",\n      "title": "Spark Config",\n      "type": "object"\n    },\n    "max_rows": {\n      "default": 100,\n      "description": "Maximum number of rows to use when inferring schemas for TSV and CSV files.",\n      "title": "Max Rows",\n      "type": "integer"\n    },\n    "add_partition_columns_to_schema": {\n      "default": false,\n      "description": "Whether to add partition fields to the schema.",\n      "title": "Add Partition Columns To Schema",\n      "type": "boolean"\n    },\n    "verify_ssl": {\n      "anyOf": [\n        {\n          "type": "boolean"\n        },\n        {\n          "type": "string"\n        }\n      ],\n      "default": true,\n      "description": "Either a boolean, in which case it controls whether we verify the server\'s TLS certificate, or a string, in which case it must be a path to a CA bundle to use.",\n      "title": "Verify Ssl"\n    },\n    "number_of_files_to_sample": {\n      "default": 100,\n      "description": "Number of files to list to sample for schema inference. This will be ignored if sample_files is set to False in the pathspec.",\n      "title": "Number Of Files To Sample",\n      "type": "integer"\n    },\n    "sort_schema_fields": {\n      "default": false,\n      "description": "Whether to sort schema fields by fieldPath when inferring schemas.",\n      "title": "Sort Schema Fields",\n      "type": "boolean"\n    },\n    "generate_partition_aspects": {\n      "default": true,\n      "description": "Whether to generate partition aspects for partitioned tables. On older servers for backward compatibility, this should be set to False. This flag will be removed in future versions.",\n      "title": "Generate Partition Aspects",\n      "type": "boolean"\n    }\n  },\n  "required": [\n    "path_specs"\n  ],\n  "title": "DataLakeSourceConfig",\n  "type": "object"\n}\n')))),(0,n.yg)("h3",{id:"path-specs"},"Path Specs"),(0,n.yg)("p",null,"Path Specs (",(0,n.yg)("inlineCode",{parentName:"p"},"path_specs"),") is a list of Path Spec (",(0,n.yg)("inlineCode",{parentName:"p"},"path_spec"),") objects where each individual ",(0,n.yg)("inlineCode",{parentName:"p"},"path_spec")," represents one or more datasets. Include path (",(0,n.yg)("inlineCode",{parentName:"p"},"path_spec.include"),") represents formatted path to the dataset. This path must end with ",(0,n.yg)("inlineCode",{parentName:"p"},"*.*")," or ",(0,n.yg)("inlineCode",{parentName:"p"},"*.[ext]")," to represent leaf level. If ",(0,n.yg)("inlineCode",{parentName:"p"},"*.[ext]"),' is provided then files with only specified extension type will be scanned. "',(0,n.yg)("inlineCode",{parentName:"p"},".[ext]"),'" can be any of ',(0,n.yg)("a",{parentName:"p",href:"#supported-file-types"},"supported file types"),". Refer ",(0,n.yg)("a",{parentName:"p",href:"#example-1---individual-file-as-dataset"},"example 1")," below for more details."),(0,n.yg)("p",null,"All folder levels need to be specified in include path. You can use ",(0,n.yg)("inlineCode",{parentName:"p"},"/*/")," to represent a folder level and avoid specifying exact folder name. To map folder as a dataset, use ",(0,n.yg)("inlineCode",{parentName:"p"},"{table}")," placeholder to represent folder level for which dataset is to be created. For a partitioned dataset, you can use placeholder ",(0,n.yg)("inlineCode",{parentName:"p"},"{partition_key[i]}")," to represent name of ",(0,n.yg)("inlineCode",{parentName:"p"},"i"),"th partition and ",(0,n.yg)("inlineCode",{parentName:"p"},"{partition_value[i]}")," to represent value of ",(0,n.yg)("inlineCode",{parentName:"p"},"i"),"th partition. During ingestion, ",(0,n.yg)("inlineCode",{parentName:"p"},"i")," will be used to match partition_key to partition. Refer ",(0,n.yg)("a",{parentName:"p",href:"#example-2---folder-of-files-as-dataset-without-partitions"},"example 2 and 3")," below for more details."),(0,n.yg)("p",null,"Exclude paths (",(0,n.yg)("inlineCode",{parentName:"p"},"path_spec.exclude"),") can be used to ignore paths that are not relevant to current ",(0,n.yg)("inlineCode",{parentName:"p"},"path_spec"),". This path cannot have named variables ( ",(0,n.yg)("inlineCode",{parentName:"p"},"{}")," ). Exclude path can have ",(0,n.yg)("inlineCode",{parentName:"p"},"**")," to represent multiple folder levels. Refer ",(0,n.yg)("a",{parentName:"p",href:"#example-4---folder-of-files-as-dataset-with-partitions-and-exclude-filter"},"example 4")," below for more details."),(0,n.yg)("p",null,"Refer ",(0,n.yg)("a",{parentName:"p",href:"#example-5---advanced---either-individual-file-or-folder-of-files-as-dataset"},"example 5")," if your bucket has more complex dataset representation."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Additional points to note")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Folder names should not contain {, }, ","*",", / in their names."),(0,n.yg)("li",{parentName:"ul"},"Named variable {folder} is reserved for internal working. please do not use in named variables.")),(0,n.yg)("h4",{id:"partitioned-dataset-support"},"Partitioned Dataset support"),(0,n.yg)("p",null,"If your dataset is partitioned by the ",(0,n.yg)("inlineCode",{parentName:"p"},"partition_key"),"=",(0,n.yg)("inlineCode",{parentName:"p"},"partition_value")," format, then the partition values are auto-detected."),(0,n.yg)("p",null,"Otherwise, you can specify partitions in the following way in the path_spec:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Specify partition_key and partition_value in the path like => ",(0,n.yg)("inlineCode",{parentName:"li"},"{partition_key[0]}={partition_value[0]}/{partition_key[1]}={partition_value[1]}/{partition_key[2]}={partition_value[2]}")),(0,n.yg)("li",{parentName:"ol"},"Partition key can be specify using named variables in the path_spec like => ",(0,n.yg)("inlineCode",{parentName:"li"},"year={year}/month={month}/day={day}"),"\n3 if the path is in the form of /value1/value2/value3 the source infer partition value from the path and assign partition_0, partition_1, partition_2 etc")),(0,n.yg)("p",null,"Dataset creation time is determined by the creation time of earliest created file in the lowest partition while last updated time is determined by the last updated time of the latest updated file in the highest partition."),(0,n.yg)("p",null,"How the source determines the highest/lowest partition it is based on the traversal method set in the path_spec."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"If the traversal method is set to ",(0,n.yg)("inlineCode",{parentName:"li"},"MAX")," then the source will try to find the latest partition by ordering the partitions each level and find the latest partiton. This traversal method won't look for earilest partition/creation time but this is the fastest."),(0,n.yg)("li",{parentName:"ul"},"If the traversal method is set to ",(0,n.yg)("inlineCode",{parentName:"li"},"MIN_MAX")," then the source will try to find the latest and earliest partition by ordering the partitions each level and find the latest/earliest partiton. This traversal sort folders purely by name therefor it is fast but it doesn't guarantee the latest partition will have the latest created file."),(0,n.yg)("li",{parentName:"ul"},"If the traversal method is set to ",(0,n.yg)("inlineCode",{parentName:"li"},"ALL")," then the source will try to find the latest and earliest partition by listing all the files in all the partitions and find the creation/last modification time based on the file creations. This is the slowest but for non time partitioned datasets this is the only way to find the latest/earliest partition.")),(0,n.yg)("h3",{id:"path-specs---examples"},"Path Specs - Examples"),(0,n.yg)("h4",{id:"example-1---individual-file-as-dataset"},"Example 1 - Individual file as Dataset"),(0,n.yg)("p",null,"Bucket structure:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"test-bucket\n\u251c\u2500\u2500 employees.csv\n\u251c\u2500\u2500 departments.json\n\u2514\u2500\u2500 food_items.csv\n")),(0,n.yg)("p",null,"Path specs config to ingest ",(0,n.yg)("inlineCode",{parentName:"p"},"employees.csv")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"food_items.csv")," as datasets:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"path_specs:\n    - include: s3://test-bucket/*.csv\n\n")),(0,n.yg)("p",null,"This will automatically ignore ",(0,n.yg)("inlineCode",{parentName:"p"},"departments.json")," file. To include it, use ",(0,n.yg)("inlineCode",{parentName:"p"},"*.*")," instead of ",(0,n.yg)("inlineCode",{parentName:"p"},"*.csv"),"."),(0,n.yg)("h4",{id:"example-2---folder-of-files-as-dataset-without-partitions"},"Example 2 - Folder of files as Dataset (without Partitions)"),(0,n.yg)("p",null,"Bucket structure:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"test-bucket\n\u2514\u2500\u2500  offers\n  \xa0\xa0 \u251c\u2500\u2500 1.avro\n     \u2514\u2500\u2500 2.avro\n\n")),(0,n.yg)("p",null,"Path specs config to ingest folder ",(0,n.yg)("inlineCode",{parentName:"p"},"offers")," as dataset:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"path_specs:\n    - include: s3://test-bucket/{table}/*.avro\n")),(0,n.yg)("p",null,(0,n.yg)("inlineCode",{parentName:"p"},"{table}")," represents folder for which dataset will be created."),(0,n.yg)("h4",{id:"example-3---folder-of-files-as-dataset-with-partitions"},"Example 3 - Folder of files as Dataset (with Partitions)"),(0,n.yg)("p",null,"Bucket structure:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"test-bucket\n\u251c\u2500\u2500 orders\n\u2502\xa0\xa0 \u2514\u2500\u2500 year=2022\n\u2502\xa0\xa0     \u2514\u2500\u2500 month=2\n\u2502\xa0\xa0         \u251c\u2500\u2500 1.parquet\n\u2502\xa0\xa0         \u2514\u2500\u2500 2.parquet\n\u2514\u2500\u2500 returns\n    \u2514\u2500\u2500 year=2021\n        \u2514\u2500\u2500 month=2\n            \u2514\u2500\u2500 1.parquet\n\n")),(0,n.yg)("p",null,"Path specs config to ingest folders ",(0,n.yg)("inlineCode",{parentName:"p"},"orders")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"returns")," as datasets:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"path_specs:\n    - include: s3://test-bucket/{table}/{partition_key[0]}={partition_value[0]}/{partition_key[1]}={partition_value[1]}/*.parquet\n")),(0,n.yg)("p",null,"or with partition auto-detection:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"path_specs:\n    - include: s3://test-bucket/{table}/\n")),(0,n.yg)("p",null,"One can also use ",(0,n.yg)("inlineCode",{parentName:"p"},"include: s3://test-bucket/{table}/*/*/*.parquet")," here however above format is preferred as it allows declaring partitions explicitly."),(0,n.yg)("h4",{id:"example-4---folder-of-files-as-dataset-with-partitions-and-exclude-filter"},"Example 4 - Folder of files as Dataset (with Partitions), and Exclude Filter"),(0,n.yg)("p",null,"Bucket structure:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"test-bucket\n\u251c\u2500\u2500 orders\n\u2502\xa0\xa0 \u2514\u2500\u2500 year=2022\n\u2502\xa0\xa0     \u2514\u2500\u2500 month=2\n\u2502\xa0\xa0         \u251c\u2500\u2500 1.parquet\n\u2502\xa0\xa0         \u2514\u2500\u2500 2.parquet\n\u2514\u2500\u2500 tmp_orders\n    \u2514\u2500\u2500 year=2021\n        \u2514\u2500\u2500 month=2\n            \u2514\u2500\u2500 1.parquet\n\n\n")),(0,n.yg)("p",null,"Path specs config to ingest folder ",(0,n.yg)("inlineCode",{parentName:"p"},"orders")," as dataset but not folder ",(0,n.yg)("inlineCode",{parentName:"p"},"tmp_orders"),":"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"path_specs:\n    - include: s3://test-bucket/{table}/{partition_key[0]}={partition_value[0]}/{partition_key[1]}={partition_value[1]}/*.parquet\n      exclude:\n        - **/tmp_orders/**\n")),(0,n.yg)("p",null,"or with partition auto-detection:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"path_specs:\n    - include: s3://test-bucket/{table}/\n")),(0,n.yg)("h4",{id:"example-5---advanced---either-individual-file-or-folder-of-files-as-dataset"},"Example 5 - Advanced - Either Individual file OR Folder of files as Dataset"),(0,n.yg)("p",null,"Bucket structure:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"test-bucket\n\u251c\u2500\u2500 customers\n\u2502\xa0\xa0 \u251c\u2500\u2500 part1.json\n\u2502\xa0\xa0 \u251c\u2500\u2500 part2.json\n\u2502\xa0\xa0 \u251c\u2500\u2500 part3.json\n\u2502\xa0\xa0 \u2514\u2500\u2500 part4.json\n\u251c\u2500\u2500 employees.csv\n\u251c\u2500\u2500 food_items.csv\n\u251c\u2500\u2500 tmp_10101000.csv\n\u2514\u2500\u2500  orders\n \xa0\xa0 \u2514\u2500\u2500 year=2022\n\xa0 \xa0     \u2514\u2500\u2500 month=2\n\xa0\xa0          \u251c\u2500\u2500 1.parquet\n\xa0\xa0          \u251c\u2500\u2500 2.parquet\n\xa0\xa0          \u2514\u2500\u2500 3.parquet\n\n")),(0,n.yg)("p",null,"Path specs config:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"path_specs:\n    - include: s3://test-bucket/*.csv\n      exclude:\n        - **/tmp_10101000.csv\n    - include: s3://test-bucket/{table}/*.json\n    - include: s3://test-bucket/{table}/{partition_key[0]}={partition[0]}/{partition_key[1]}={partition[1]}/*.parquet\n")),(0,n.yg)("p",null,"Above config has 3 path_specs and will ingest following datasets"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"employees.csv")," - Single File as Dataset"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"food_items.csv")," - Single File as Dataset"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"customers")," - Folder as Dataset"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"orders")," - Folder as Dataset\nand will ignore file ",(0,n.yg)("inlineCode",{parentName:"li"},"tmp_10101000.csv"))),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Valid path_specs.include")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},"s3://my-bucket/foo/tests/bar.avro # single file table\ns3://my-bucket/foo/tests/*.* # mulitple file level tables\ns3://my-bucket/foo/tests/{table}/*.avro #table without partition\ns3://my-bucket/foo/tests/{table}/ #table with partition autodetection. Partition only can be detected if it is in the format of key=value\ns3://my-bucket/foo/tests/{table}/*/*.avro #table where partitions are not specified\ns3://my-bucket/foo/tests/{table}/*.* # table where no partitions as well as data type specified\ns3://my-bucket/{dept}/tests/{table}/*.avro # specifying keywords to be used in display name\ns3://my-bucket/{dept}/tests/{table}/{partition_key[0]}={partition[0]}/{partition_key[1]}={partition[1]}/*.avro # specify partition key and value format\ns3://my-bucket/{dept}/tests/{table}/{partition[0]}/{partition[1]}/{partition[2]}/*.avro # specify partition value only format\ns3://my-bucket/{dept}/tests/{table}/{partition[0]}/{partition[1]}/{partition[2]}/*.* # for all extensions\ns3://my-bucket/*/{table}/{partition[0]}/{partition[1]}/{partition[2]}/*.* # table is present at 2 levels down in bucket\ns3://my-bucket/*/*/{table}/{partition[0]}/{partition[1]}/{partition[2]}/*.* # table is present at 3 levels down in bucket\n")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Valid path_specs.exclude")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"*","*","/tests/","*","*"),(0,n.yg)("li",{parentName:"ul"},"s3://my-bucket/hr/","*","*"),(0,n.yg)("li",{parentName:"ul"},"*",(0,n.yg)("em",{parentName:"li"},"/tests/"),".csv"),(0,n.yg)("li",{parentName:"ul"},"s3://my-bucket/foo/","*","/my_table/","*","*")),(0,n.yg)("p",null,"If you would like to write a more complicated function for resolving file names, then a {transformer} would be a good fit."),(0,n.yg)("admonition",{type:"caution"},(0,n.yg)("p",{parentName:"admonition"},"Specify as long fixed prefix ( with out /","*","/ ) as possible in ",(0,n.yg)("inlineCode",{parentName:"p"},"path_specs.include"),". This will reduce the scanning time and cost, specifically on AWS S3")),(0,n.yg)("admonition",{type:"caution"},(0,n.yg)("p",{parentName:"admonition"},"Running profiling against many tables or over many rows can run up significant costs.\nWhile we've done our best to limit the expensiveness of the queries the profiler runs, you\nshould be prudent about the set of tables profiling is enabled on or the frequency\nof the profiling runs.")),(0,n.yg)("admonition",{type:"caution"},(0,n.yg)("p",{parentName:"admonition"},"If you are ingesting datasets from AWS S3, we recommend running the ingestion on a server in the same region to avoid high egress costs.")),(0,n.yg)("h3",{id:"compatibility"},"Compatibility"),(0,n.yg)("p",null,"Profiles are computed with PyDeequ, which relies on PySpark. Therefore, for computing profiles, we currently require Spark 3.0.3 with Hadoop 3.2 to be installed and the ",(0,n.yg)("inlineCode",{parentName:"p"},"SPARK_HOME")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"SPARK_VERSION")," environment variables to be set. The Spark+Hadoop binary can be downloaded ",(0,n.yg)("a",{parentName:"p",href:"https://www.apache.org/dyn/closer.lua/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz"},"here"),"."),(0,n.yg)("p",null,"For an example guide on setting up PyDeequ on AWS, see ",(0,n.yg)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/big-data/testing-data-quality-at-scale-with-pydeequ/"},"this guide"),"."),(0,n.yg)("admonition",{type:"caution"},(0,n.yg)("p",{parentName:"admonition"},"From Spark 3.2.0+, Avro reader fails on column names that don't start with a letter and contains other character than letters, number, and underscore. ","[https://github.com/apache/spark/blob/72c62b6596d21e975c5597f8fff84b1a9d070a02/connector/avro/src/main/scala/org/apache/spark/sql/avro/AvroFileFormat.scala#L158]","\nAvro files that contain such columns won't be profiled.")),(0,n.yg)("h3",{id:"code-coordinates"},"Code Coordinates"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Class Name: ",(0,n.yg)("inlineCode",{parentName:"li"},"datahub.ingestion.source.s3.source.S3Source")),(0,n.yg)("li",{parentName:"ul"},"Browse on ",(0,n.yg)("a",{parentName:"li",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/ingestion/source/s3/source.py"},"GitHub"))),(0,n.yg)("h2",null,"Questions"),(0,n.yg)("p",null,"If you've got any questions on configuring ingestion for S3 / Local Files, feel free to ping us on ",(0,n.yg)("a",{parentName:"p",href:"https://datahub.com/slack"},"our Slack"),"."))}u.isMDXComponent=!0}}]);