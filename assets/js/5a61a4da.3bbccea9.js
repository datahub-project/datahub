"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[5728],{42388:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>m,contentTitle:()=>d,default:()=>u,frontMatter:()=>p,metadata:()=>c,toc:()=>g});t(96540);var n=t(15680),i=t(53720),l=t(5400);function s(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){return a=null!=a?a:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):function(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))})),e}function o(e,a){if(null==e)return{};var t,n,i=function(e,a){if(null==e)return{};var t,n,i={},l=Object.keys(e);for(n=0;n<l.length;n++)t=l[n],a.indexOf(t)>=0||(i[t]=e[t]);return i}(e,a);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)t=l[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}const p={sidebar_position:1,title:"ABS Data Lake",slug:"/generated/ingestion/sources/abs",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/generated/ingestion/sources/abs.md"},d="ABS Data Lake",c={unversionedId:"docs/generated/ingestion/sources/abs",id:"version-1.1.0/docs/generated/ingestion/sources/abs",title:"ABS Data Lake",description:"This connector ingests Azure Blob Storage (abbreviated to abs) datasets into DataHub. It allows mapping an individual",source:"@site/versioned_docs/version-1.1.0/docs/generated/ingestion/sources/abs.md",sourceDirName:"docs/generated/ingestion/sources",slug:"/generated/ingestion/sources/abs",permalink:"/docs/1.1.0/generated/ingestion/sources/abs",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/generated/ingestion/sources/abs.md",tags:[],version:"1.1.0",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"ABS Data Lake",slug:"/generated/ingestion/sources/abs",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/generated/ingestion/sources/abs.md"},sidebar:"overviewSidebar",previous:{title:"Protobuf Schemas",permalink:"/docs/1.1.0/metadata-integration/java/datahub-protobuf"},next:{title:"Athena",permalink:"/docs/1.1.0/generated/ingestion/sources/athena"}},m={},g=[{value:"Concept Mapping",id:"concept-mapping",level:3},{value:"Supported file types",id:"supported-file-types",level:3},{value:"Profiling",id:"profiling",level:3},{value:"Important Capabilities",id:"important-capabilities",level:3},{value:"CLI based Ingestion",id:"cli-based-ingestion",level:3},{value:"Starter Recipe",id:"starter-recipe",level:3},{value:"Config Details",id:"config-details",level:3},{value:"Path Specs",id:"path-specs",level:3},{value:"Path Specs - Examples",id:"path-specs---examples",level:3},{value:"Example 1 - Individual file as Dataset",id:"example-1---individual-file-as-dataset",level:4},{value:"Example 2 - Folder of files as Dataset (without Partitions)",id:"example-2---folder-of-files-as-dataset-without-partitions",level:4},{value:"Example 3 - Folder of files as Dataset (with Partitions)",id:"example-3---folder-of-files-as-dataset-with-partitions",level:4},{value:"Example 4 - Folder of files as Dataset (with Partitions), and Exclude Filter",id:"example-4---folder-of-files-as-dataset-with-partitions-and-exclude-filter",level:4},{value:"Example 5 - Advanced - Either Individual file OR Folder of files as Dataset",id:"example-5---advanced---either-individual-file-or-folder-of-files-as-dataset",level:4},{value:"Compatibility",id:"compatibility",level:3},{value:"Code Coordinates",id:"code-coordinates",level:3}],y={toc:g},f="wrapper";function u(e){var{components:a}=e,t=o(e,["components"]);return(0,n.yg)(f,r(function(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{},n=Object.keys(t);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(t).filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable})))),n.forEach((function(a){s(e,a,t[a])}))}return e}({},y,t),{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"abs-data-lake"},"ABS Data Lake"),(0,n.yg)("p",null,"This connector ingests Azure Blob Storage (abbreviated to abs) datasets into DataHub. It allows mapping an individual\nfile or a folder of files to a dataset in DataHub.\nTo specify the group of files that form a dataset, use ",(0,n.yg)("inlineCode",{parentName:"p"},"path_specs")," configuration in ingestion recipe. Refer\nsection ",(0,n.yg)("a",{parentName:"p",href:"/docs/generated/ingestion/sources/s3/#path-specs"},"Path Specs")," for more details."),(0,n.yg)("h3",{id:"concept-mapping"},"Concept Mapping"),(0,n.yg)("p",null,"This ingestion source maps the following Source System Concepts to DataHub Concepts:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Source Concept"),(0,n.yg)("th",{parentName:"tr",align:null},"DataHub Concept"),(0,n.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},'"abs"')),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/metamodel/entities/dataplatform/"},"Data Platform")),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"abs blob / Folder containing abs blobs"),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/metamodel/entities/dataset/"},"Dataset")),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"abs container"),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/metamodel/entities/container/"},"Container")),(0,n.yg)("td",{parentName:"tr",align:null},"Subtype ",(0,n.yg)("inlineCode",{parentName:"td"},"Folder"))))),(0,n.yg)("p",null,"This connector supports both local files and those stored on Azure Blob Storage (which must be identified using the\nprefix ",(0,n.yg)("inlineCode",{parentName:"p"},"http(s)://<account>.blob.core.windows.net/")," or ",(0,n.yg)("inlineCode",{parentName:"p"},"azure://"),")."),(0,n.yg)("h3",{id:"supported-file-types"},"Supported file types"),(0,n.yg)("p",null,"Supported file types are as follows:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"CSV (","*",".csv)"),(0,n.yg)("li",{parentName:"ul"},"TSV (","*",".tsv)"),(0,n.yg)("li",{parentName:"ul"},"JSONL (","*",".jsonl)"),(0,n.yg)("li",{parentName:"ul"},"JSON (","*",".json)"),(0,n.yg)("li",{parentName:"ul"},"Parquet (","*",".parquet)"),(0,n.yg)("li",{parentName:"ul"},"Apache Avro (","*",".avro)")),(0,n.yg)("p",null,"Schemas for Parquet and Avro files are extracted as provided."),(0,n.yg)("p",null,"Schemas for schemaless formats (CSV, TSV, JSONL, JSON) are inferred. For CSV, TSV and JSONL files, we consider the first\n100 rows by default, which can be controlled via the ",(0,n.yg)("inlineCode",{parentName:"p"},"max_rows")," recipe parameter (see ",(0,n.yg)("a",{parentName:"p",href:"#config-details"},"below"),")\nJSON file schemas are inferred on the basis of the entire file (given the difficulty in extracting only the first few\nobjects of the file), which may impact performance.\nWe are working on using iterator-based JSON parsers to avoid reading in the entire JSON object."),(0,n.yg)("h3",{id:"profiling"},"Profiling"),(0,n.yg)("p",null,"Profiling is not available in the current release.\n",(0,n.yg)("img",{parentName:"p",src:"https://img.shields.io/badge/support%20status-incubating-blue",alt:"Incubating"})),(0,n.yg)("h3",{id:"important-capabilities"},"Important Capabilities"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Capability"),(0,n.yg)("th",{parentName:"tr",align:null},"Status"),(0,n.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/1.1.0/metadata-ingestion/docs/dev_guides/sql_profiles"},"Data Profiling")),(0,n.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,n.yg)("td",{parentName:"tr",align:null},"Optionally enabled via configuration")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/1.1.0/metadata-ingestion/docs/dev_guides/stateful#stale-entity-removal"},"Detect Deleted Entities")),(0,n.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,n.yg)("td",{parentName:"tr",align:null},"Optionally enabled via ",(0,n.yg)("inlineCode",{parentName:"td"},"stateful_ingestion.remove_stale_metadata"))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Extract Tags"),(0,n.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,n.yg)("td",{parentName:"tr",align:null},"Can extract ABS object/container tags if enabled")))),(0,n.yg)("h3",{id:"cli-based-ingestion"},"CLI based Ingestion"),(0,n.yg)("h3",{id:"starter-recipe"},"Starter Recipe"),(0,n.yg)("p",null,"Check out the following recipe to get started with ingestion! See ",(0,n.yg)("a",{parentName:"p",href:"#config-details"},"below")," for full configuration options."),(0,n.yg)("p",null,"For general pointers on writing and running a recipe, see our ",(0,n.yg)("a",{parentName:"p",href:"/docs/1.1.0/metadata-ingestion#recipes"},"main recipe guide"),"."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'source:\n  type: abs\n  config:\n    path_specs:\n      - include: "https://storageaccountname.blob.core.windows.net/covid19-lake/covid_knowledge_graph/csv/nodes/*.*"\n\n    azure_config:\n      account_name: "*****"\n      sas_token: "*****"\n      container_name: "covid_knowledge_graph"\n    env: "PROD"\n\n# sink configs\n\n')),(0,n.yg)("h3",{id:"config-details"},"Config Details"),(0,n.yg)(i.A,{mdxType:"Tabs"},(0,n.yg)(l.A,{value:"options",label:"Options",default:!0,mdxType:"TabItem"},(0,n.yg)("p",null,"Note that a ",(0,n.yg)("inlineCode",{parentName:"p"},".")," is used to denote nested fields in the YAML recipe."),(0,n.yg)("div",{className:"config-table"},(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:"left"},"Field"),(0,n.yg)("th",{parentName:"tr",align:"left"},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"path_specs"),"\xa0",(0,n.yg)("abbr",{title:"Required"},"\u2705"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"array"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"List of PathSpec. See ",(0,n.yg)("a",{parentName:"td",href:"#path-spec"},"below")," the details about PathSpec")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs."),(0,n.yg)("span",{className:"path-main"},"PathSpec"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"PathSpec"))),(0,n.yg)("td",{parentName:"tr",align:"left"})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"include"),"\xa0",(0,n.yg)("abbr",{title:"Required if PathSpec is set"},"\u2753"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Path to table. Name variable ",(0,n.yg)("inlineCode",{parentName:"td"},"{table}")," is used to mark the folder with dataset. In absence of ",(0,n.yg)("inlineCode",{parentName:"td"},"{table}"),", file level dataset will be created. Check below examples for more details.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"allow_double_stars"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Allow double stars in the include path. This can affect performance significantly if enabled ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"autodetect_partitions"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Autodetect partition(s) from the path. If set to true, it will autodetect partition key/value if the folder format is {partition_key}={partition_value} for example ",(0,n.yg)("inlineCode",{parentName:"td"},"year=2024")," ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"default_extension"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"For files without extension it will assume the specified file type. If it is not set the files without extensions will be skipped.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"enable_compression"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Enable or disable processing compressed files. Currently .gz and .bz files are supported. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"include_hidden_folders"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Include hidden folders in the traversal (folders starting with . or _ ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"sample_files"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Not listing all the files but only taking a handful amount of sample file to infer the schema. File count and file size calculation will be disabled. This can affect performance significantly if enabled ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"table_name"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Display name of the dataset.Combination of named variables from include path and strings")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"traversal_method"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"Enum"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Method to traverse the folder. ALL: Traverse all the folders, MIN_MAX: Traverse the folders by finding min and max value, MAX: Traverse the folder with max value ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"MAX")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"exclude"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"array"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"list of paths in glob pattern which will be excluded while scanning for the datasets ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"[","]")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec.exclude."),(0,n.yg)("span",{className:"path-main"},"string"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"file_types"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"array"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Files with extenstions specified here (subset of default value) only will be scanned to create dataset. Other files will be omitted. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"[","'","csv","'",", ","'","tsv","'",", ","'","json","'",", ","'","parquet","'",", ","'","avro","'","]")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec.file_types."),(0,n.yg)("span",{className:"path-main"},"string"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec."),(0,n.yg)("span",{className:"path-main"},"tables_filter_pattern"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"AllowDenyPattern"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"The tables_filter_pattern configuration field uses regular expressions to filter the tables part of the Pathspec for ingestion, allowing fine-grained control over which tables are included or excluded based on specified patterns. The default setting allows all tables. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"{","'","allow","'",": ","[","'",".","*","'","]",", ","'","deny","'",": ","[","]",", ","'","ignoreCase","'",": True","}")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec.tables_filter_pattern."),(0,n.yg)("span",{className:"path-main"},"ignoreCase"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to ignore case sensitivity during pattern matching. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec.tables_filter_pattern."),(0,n.yg)("span",{className:"path-main"},"allow"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"array"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"List of regex patterns to include in ingestion ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"[","'",".","*","'","]")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec.tables_filter_pattern.allow."),(0,n.yg)("span",{className:"path-main"},"string"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec.tables_filter_pattern."),(0,n.yg)("span",{className:"path-main"},"deny"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"array"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"List of regex patterns to exclude from ingestion. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"[","]")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"path_specs.PathSpec.tables_filter_pattern.deny."),(0,n.yg)("span",{className:"path-main"},"string"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"add_partition_columns_to_schema"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to add partition fields to the schema. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"max_rows"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"integer"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Maximum number of rows to use when inferring schemas for TSV and CSV files. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"100")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"number_of_files_to_sample"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"integer"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Number of files to list to sample for schema inference. This will be ignored if sample_files is set to False in the pathspec. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"100")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"platform"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"The platform that this source connects to (either 'abs' or 'file'). If not specified, the platform will be inferred from the path_specs. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"})))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"platform_instance"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"The instance of the platform that all assets produced by this recipe belong to. This should be unique within the platform. See ",(0,n.yg)("a",{parentName:"td",href:"https://docs.datahub.com/docs/platform-instances/"},"https://docs.datahub.com/docs/platform-instances/")," for more details.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"spark_config"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"object"))),(0,n.yg)("td",{parentName:"tr",align:"left"},'Spark configuration properties to set on the SparkSession. Put config property names into quotes. For example: \'"spark.executor.memory": "2g"\' ',(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"{","}")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"spark_driver_memory"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Max amount of memory to grant Spark. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"4g")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"use_abs_blob_properties"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to create tags in datahub from the abs blob properties")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"use_abs_blob_tags"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to create tags in datahub from the abs blob tags")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"use_abs_container_properties"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to create tags in datahub from the abs container properties")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"verify_ssl"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"One of boolean, string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Either a boolean, in which case it controls whether we verify the server's TLS certificate, or a string, in which case it must be a path to a CA bundle to use. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"env"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"The environment that all assets produced by this connector belong to ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"PROD")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"azure_config"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"AzureConnectionConfig"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Azure configuration")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"azure_config."),(0,n.yg)("span",{className:"path-main"},"account_name"),"\xa0",(0,n.yg)("abbr",{title:"Required if azure_config is set"},"\u2753"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Name of the Azure storage account.  See ",(0,n.yg)("a",{parentName:"td",href:"https://docs.microsoft.com/en-us/azure/storage/blobs/create-data-lake-storage-account"},"Microsoft official documentation on how to create a storage account."))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"azure_config."),(0,n.yg)("span",{className:"path-main"},"container_name"),"\xa0",(0,n.yg)("abbr",{title:"Required if azure_config is set"},"\u2753"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Azure storage account container name.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"azure_config."),(0,n.yg)("span",{className:"path-main"},"account_key"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Azure storage account access key that can be used as a credential. ",(0,n.yg)("strong",{parentName:"td"},"An account key, a SAS token or a client secret is required for authentication."))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"azure_config."),(0,n.yg)("span",{className:"path-main"},"base_path"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Base folder in hierarchical namespaces to start from. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"/")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"azure_config."),(0,n.yg)("span",{className:"path-main"},"client_id"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Azure client (Application) ID required when a ",(0,n.yg)("inlineCode",{parentName:"td"},"client_secret")," is used as a credential.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"azure_config."),(0,n.yg)("span",{className:"path-main"},"client_secret"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Azure client secret that can be used as a credential. ",(0,n.yg)("strong",{parentName:"td"},"An account key, a SAS token or a client secret is required for authentication."))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"azure_config."),(0,n.yg)("span",{className:"path-main"},"sas_token"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Azure storage account Shared Access Signature (SAS) token that can be used as a credential. ",(0,n.yg)("strong",{parentName:"td"},"An account key, a SAS token or a client secret is required for authentication."))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"azure_config."),(0,n.yg)("span",{className:"path-main"},"tenant_id"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Azure tenant (Directory) ID required when a ",(0,n.yg)("inlineCode",{parentName:"td"},"client_secret")," is used as a credential.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"profile_patterns"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"AllowDenyPattern"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"regex patterns for tables to profile  ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"{","'","allow","'",": ","[","'",".","*","'","]",", ","'","deny","'",": ","[","]",", ","'","ignoreCase","'",": True","}")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profile_patterns."),(0,n.yg)("span",{className:"path-main"},"ignoreCase"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to ignore case sensitivity during pattern matching. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profile_patterns."),(0,n.yg)("span",{className:"path-main"},"allow"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"array"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"List of regex patterns to include in ingestion ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"[","'",".","*","'","]")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profile_patterns.allow."),(0,n.yg)("span",{className:"path-main"},"string"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profile_patterns."),(0,n.yg)("span",{className:"path-main"},"deny"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"array"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"List of regex patterns to exclude from ingestion. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"[","]")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profile_patterns.deny."),(0,n.yg)("span",{className:"path-main"},"string"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"string"))),(0,n.yg)("td",{parentName:"tr",align:"left"})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"profiling"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"DataLakeProfilerConfig"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Data profiling configuration ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"{","'","enabled","'",": False, ","'","operation","_","config","'",": ","{","'","lower","_","fre...")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"enabled"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether profiling should be done. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_distinct_value_frequencies"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for distinct value frequencies. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_histogram"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the histogram for numeric fields. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_max_value"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the max value of numeric columns. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_mean_value"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the mean value of numeric columns. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_median_value"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the median value of numeric columns. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_min_value"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the min value of numeric columns. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_null_count"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the number of nulls for each column. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_quantiles"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the quantiles of numeric columns. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_sample_values"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the sample values for all columns. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"include_field_stddev_value"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to profile for the standard deviation of numeric columns. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"max_number_of_fields_to_profile"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"integer"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"A positive integer that specifies the maximum number of columns to profile for any table. ",(0,n.yg)("inlineCode",{parentName:"td"},"None")," implies all columns. The cost of profiling goes up significantly as the number of columns to profile goes up.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"profile_table_level_only"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to perform profiling at table-level only or include column-level profiling as well. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling."),(0,n.yg)("span",{className:"path-main"},"operation_config"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"OperationConfig"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Experimental feature. To specify operation configs.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling.operation_config."),(0,n.yg)("span",{className:"path-main"},"lower_freq_profile_enabled"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether to do profiling at lower freq or not. This does not do any scheduling just adds additional checks to when not to run profiling. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling.operation_config."),(0,n.yg)("span",{className:"path-main"},"profile_date_of_month"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"integer"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Number between 1 to 31 for date of month (both inclusive). If not specified, defaults to Nothing and this field does not take affect.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"profiling.operation_config."),(0,n.yg)("span",{className:"path-main"},"profile_day_of_week"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"integer"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Number between 0 to 6 for day of week (both inclusive). 0 is Monday and 6 is Sunday. If not specified, defaults to Nothing and this field does not take affect.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-main"},"stateful_ingestion"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"StatefulStaleMetadataRemovalConfig"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Base specialized config for Stateful Ingestion with stale metadata removal capability.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"stateful_ingestion."),(0,n.yg)("span",{className:"path-main"},"enabled"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Whether or not to enable stateful ingest. Default: True if a pipeline_name is set and either a datahub-rest sink or ",(0,n.yg)("inlineCode",{parentName:"td"},"datahub_api")," is specified, otherwise False ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"False")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"stateful_ingestion."),(0,n.yg)("span",{className:"path-main"},"fail_safe_threshold"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"number"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Prevents large amount of soft deletes & the state from committing from accidental changes to the source configuration if the relative change percent in entities compared to the previous state is above the 'fail_safe_threshold'. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"75.0")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:"left"},(0,n.yg)("div",{className:"path-line"},(0,n.yg)("span",{className:"path-prefix"},"stateful_ingestion."),(0,n.yg)("span",{className:"path-main"},"remove_stale_metadata"))," ",(0,n.yg)("div",{className:"type-name-line"},(0,n.yg)("span",{className:"type-name"},"boolean"))),(0,n.yg)("td",{parentName:"tr",align:"left"},"Soft-deletes the entities present in the last successful run but missing in the current run with stateful_ingestion enabled. ",(0,n.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,n.yg)("span",{className:"default-value"},"True")))))))),(0,n.yg)(l.A,{value:"schema",label:"Schema",mdxType:"TabItem"},(0,n.yg)("p",null,"The ",(0,n.yg)("a",{parentName:"p",href:"https://json-schema.org/"},"JSONSchema")," for this configuration is inlined below."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-javascript"},'{\n  "title": "DataLakeSourceConfig",\n  "description": "Base configuration class for stateful ingestion for source configs to inherit from.",\n  "type": "object",\n  "properties": {\n    "path_specs": {\n      "title": "Path Specs",\n      "description": "List of PathSpec. See [below](#path-spec) the details about PathSpec",\n      "type": "array",\n      "items": {\n        "$ref": "#/definitions/PathSpec"\n      }\n    },\n    "env": {\n      "title": "Env",\n      "description": "The environment that all assets produced by this connector belong to",\n      "default": "PROD",\n      "type": "string"\n    },\n    "platform_instance": {\n      "title": "Platform Instance",\n      "description": "The instance of the platform that all assets produced by this recipe belong to. This should be unique within the platform. See https://docs.datahub.com/docs/platform-instances/ for more details.",\n      "type": "string"\n    },\n    "stateful_ingestion": {\n      "$ref": "#/definitions/StatefulStaleMetadataRemovalConfig"\n    },\n    "platform": {\n      "title": "Platform",\n      "description": "The platform that this source connects to (either \'abs\' or \'file\'). If not specified, the platform will be inferred from the path_specs.",\n      "default": "",\n      "type": "string"\n    },\n    "azure_config": {\n      "title": "Azure Config",\n      "description": "Azure configuration",\n      "allOf": [\n        {\n          "$ref": "#/definitions/AzureConnectionConfig"\n        }\n      ]\n    },\n    "use_abs_container_properties": {\n      "title": "Use Abs Container Properties",\n      "description": "Whether to create tags in datahub from the abs container properties",\n      "type": "boolean"\n    },\n    "use_abs_blob_tags": {\n      "title": "Use Abs Blob Tags",\n      "description": "Whether to create tags in datahub from the abs blob tags",\n      "type": "boolean"\n    },\n    "use_abs_blob_properties": {\n      "title": "Use Abs Blob Properties",\n      "description": "Whether to create tags in datahub from the abs blob properties",\n      "type": "boolean"\n    },\n    "profile_patterns": {\n      "title": "Profile Patterns",\n      "description": "regex patterns for tables to profile ",\n      "default": {\n        "allow": [\n          ".*"\n        ],\n        "deny": [],\n        "ignoreCase": true\n      },\n      "allOf": [\n        {\n          "$ref": "#/definitions/AllowDenyPattern"\n        }\n      ]\n    },\n    "profiling": {\n      "title": "Profiling",\n      "description": "Data profiling configuration",\n      "default": {\n        "enabled": false,\n        "operation_config": {\n          "lower_freq_profile_enabled": false,\n          "profile_day_of_week": null,\n          "profile_date_of_month": null\n        },\n        "profile_table_level_only": false,\n        "max_number_of_fields_to_profile": null,\n        "include_field_null_count": true,\n        "include_field_min_value": true,\n        "include_field_max_value": true,\n        "include_field_mean_value": true,\n        "include_field_median_value": true,\n        "include_field_stddev_value": true,\n        "include_field_quantiles": true,\n        "include_field_distinct_value_frequencies": true,\n        "include_field_histogram": true,\n        "include_field_sample_values": true\n      },\n      "allOf": [\n        {\n          "$ref": "#/definitions/DataLakeProfilerConfig"\n        }\n      ]\n    },\n    "spark_driver_memory": {\n      "title": "Spark Driver Memory",\n      "description": "Max amount of memory to grant Spark.",\n      "default": "4g",\n      "type": "string"\n    },\n    "spark_config": {\n      "title": "Spark Config",\n      "description": "Spark configuration properties to set on the SparkSession. Put config property names into quotes. For example: \'\\"spark.executor.memory\\": \\"2g\\"\'",\n      "default": {},\n      "type": "object"\n    },\n    "max_rows": {\n      "title": "Max Rows",\n      "description": "Maximum number of rows to use when inferring schemas for TSV and CSV files.",\n      "default": 100,\n      "type": "integer"\n    },\n    "add_partition_columns_to_schema": {\n      "title": "Add Partition Columns To Schema",\n      "description": "Whether to add partition fields to the schema.",\n      "default": false,\n      "type": "boolean"\n    },\n    "verify_ssl": {\n      "title": "Verify Ssl",\n      "description": "Either a boolean, in which case it controls whether we verify the server\'s TLS certificate, or a string, in which case it must be a path to a CA bundle to use.",\n      "default": true,\n      "anyOf": [\n        {\n          "type": "boolean"\n        },\n        {\n          "type": "string"\n        }\n      ]\n    },\n    "number_of_files_to_sample": {\n      "title": "Number Of Files To Sample",\n      "description": "Number of files to list to sample for schema inference. This will be ignored if sample_files is set to False in the pathspec.",\n      "default": 100,\n      "type": "integer"\n    }\n  },\n  "required": [\n    "path_specs"\n  ],\n  "additionalProperties": false,\n  "definitions": {\n    "SortKeyType": {\n      "title": "SortKeyType",\n      "description": "An enumeration.",\n      "enum": [\n        "STRING",\n        "INTEGER",\n        "FLOAT",\n        "DATETIME",\n        "DATE"\n      ]\n    },\n    "SortKey": {\n      "title": "SortKey",\n      "type": "object",\n      "properties": {\n        "key": {\n          "title": "Key",\n          "description": "The key to sort on. This can be a compound key based on the path_spec variables.",\n          "type": "string"\n        },\n        "type": {\n          "description": "The date format to use when sorting. This is used to parse the date from the key. The format should follow the java [SimpleDateFormat](https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html) format.",\n          "default": "STRING",\n          "allOf": [\n            {\n              "$ref": "#/definitions/SortKeyType"\n            }\n          ]\n        },\n        "date_format": {\n          "title": "Date Format",\n          "description": "The date format to use when sorting. This is used to parse the date from the key. The format should follow the java [SimpleDateFormat](https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html) format.",\n          "type": "string"\n        }\n      },\n      "required": [\n        "key"\n      ],\n      "additionalProperties": false\n    },\n    "FolderTraversalMethod": {\n      "title": "FolderTraversalMethod",\n      "description": "An enumeration.",\n      "enum": [\n        "ALL",\n        "MIN_MAX",\n        "MAX"\n      ]\n    },\n    "AllowDenyPattern": {\n      "title": "AllowDenyPattern",\n      "description": "A class to store allow deny regexes",\n      "type": "object",\n      "properties": {\n        "allow": {\n          "title": "Allow",\n          "description": "List of regex patterns to include in ingestion",\n          "default": [\n            ".*"\n          ],\n          "type": "array",\n          "items": {\n            "type": "string"\n          }\n        },\n        "deny": {\n          "title": "Deny",\n          "description": "List of regex patterns to exclude from ingestion.",\n          "default": [],\n          "type": "array",\n          "items": {\n            "type": "string"\n          }\n        },\n        "ignoreCase": {\n          "title": "Ignorecase",\n          "description": "Whether to ignore case sensitivity during pattern matching.",\n          "default": true,\n          "type": "boolean"\n        }\n      },\n      "additionalProperties": false\n    },\n    "PathSpec": {\n      "title": "PathSpec",\n      "type": "object",\n      "properties": {\n        "include": {\n          "title": "Include",\n          "description": "Path to table. Name variable `{table}` is used to mark the folder with dataset. In absence of `{table}`, file level dataset will be created. Check below examples for more details.",\n          "type": "string"\n        },\n        "exclude": {\n          "title": "Exclude",\n          "description": "list of paths in glob pattern which will be excluded while scanning for the datasets",\n          "default": [],\n          "type": "array",\n          "items": {\n            "type": "string"\n          }\n        },\n        "file_types": {\n          "title": "File Types",\n          "description": "Files with extenstions specified here (subset of default value) only will be scanned to create dataset. Other files will be omitted.",\n          "default": [\n            "csv",\n            "tsv",\n            "json",\n            "parquet",\n            "avro"\n          ],\n          "type": "array",\n          "items": {\n            "type": "string"\n          }\n        },\n        "default_extension": {\n          "title": "Default Extension",\n          "description": "For files without extension it will assume the specified file type. If it is not set the files without extensions will be skipped.",\n          "type": "string"\n        },\n        "table_name": {\n          "title": "Table Name",\n          "description": "Display name of the dataset.Combination of named variables from include path and strings",\n          "type": "string"\n        },\n        "enable_compression": {\n          "title": "Enable Compression",\n          "description": "Enable or disable processing compressed files. Currently .gz and .bz files are supported.",\n          "default": true,\n          "type": "boolean"\n        },\n        "sample_files": {\n          "title": "Sample Files",\n          "description": "Not listing all the files but only taking a handful amount of sample file to infer the schema. File count and file size calculation will be disabled. This can affect performance significantly if enabled",\n          "default": true,\n          "type": "boolean"\n        },\n        "allow_double_stars": {\n          "title": "Allow Double Stars",\n          "description": "Allow double stars in the include path. This can affect performance significantly if enabled",\n          "default": false,\n          "type": "boolean"\n        },\n        "autodetect_partitions": {\n          "title": "Autodetect Partitions",\n          "description": "Autodetect partition(s) from the path. If set to true, it will autodetect partition key/value if the folder format is {partition_key}={partition_value} for example `year=2024`",\n          "default": true,\n          "type": "boolean"\n        },\n        "traversal_method": {\n          "description": "Method to traverse the folder. ALL: Traverse all the folders, MIN_MAX: Traverse the folders by finding min and max value, MAX: Traverse the folder with max value",\n          "default": "MAX",\n          "allOf": [\n            {\n              "$ref": "#/definitions/FolderTraversalMethod"\n            }\n          ]\n        },\n        "include_hidden_folders": {\n          "title": "Include Hidden Folders",\n          "description": "Include hidden folders in the traversal (folders starting with . or _",\n          "default": false,\n          "type": "boolean"\n        },\n        "tables_filter_pattern": {\n          "title": "Tables Filter Pattern",\n          "description": "The tables_filter_pattern configuration field uses regular expressions to filter the tables part of the Pathspec for ingestion, allowing fine-grained control over which tables are included or excluded based on specified patterns. The default setting allows all tables.",\n          "default": {\n            "allow": [\n              ".*"\n            ],\n            "deny": [],\n            "ignoreCase": true\n          },\n          "allOf": [\n            {\n              "$ref": "#/definitions/AllowDenyPattern"\n            }\n          ]\n        }\n      },\n      "required": [\n        "include"\n      ],\n      "additionalProperties": false\n    },\n    "DynamicTypedStateProviderConfig": {\n      "title": "DynamicTypedStateProviderConfig",\n      "type": "object",\n      "properties": {\n        "type": {\n          "title": "Type",\n          "description": "The type of the state provider to use. For DataHub use `datahub`",\n          "type": "string"\n        },\n        "config": {\n          "title": "Config",\n          "description": "The configuration required for initializing the state provider. Default: The datahub_api config if set at pipeline level. Otherwise, the default DatahubClientConfig. See the defaults (https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/ingestion/graph/client.py#L19).",\n          "default": {},\n          "type": "object"\n        }\n      },\n      "required": [\n        "type"\n      ],\n      "additionalProperties": false\n    },\n    "StatefulStaleMetadataRemovalConfig": {\n      "title": "StatefulStaleMetadataRemovalConfig",\n      "description": "Base specialized config for Stateful Ingestion with stale metadata removal capability.",\n      "type": "object",\n      "properties": {\n        "enabled": {\n          "title": "Enabled",\n          "description": "Whether or not to enable stateful ingest. Default: True if a pipeline_name is set and either a datahub-rest sink or `datahub_api` is specified, otherwise False",\n          "default": false,\n          "type": "boolean"\n        },\n        "remove_stale_metadata": {\n          "title": "Remove Stale Metadata",\n          "description": "Soft-deletes the entities present in the last successful run but missing in the current run with stateful_ingestion enabled.",\n          "default": true,\n          "type": "boolean"\n        },\n        "fail_safe_threshold": {\n          "title": "Fail Safe Threshold",\n          "description": "Prevents large amount of soft deletes & the state from committing from accidental changes to the source configuration if the relative change percent in entities compared to the previous state is above the \'fail_safe_threshold\'.",\n          "default": 75.0,\n          "minimum": 0.0,\n          "maximum": 100.0,\n          "type": "number"\n        }\n      },\n      "additionalProperties": false\n    },\n    "AzureConnectionConfig": {\n      "title": "AzureConnectionConfig",\n      "description": "Common Azure credentials config.\\n\\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-directory-file-acl-python",\n      "type": "object",\n      "properties": {\n        "base_path": {\n          "title": "Base Path",\n          "description": "Base folder in hierarchical namespaces to start from.",\n          "default": "/",\n          "type": "string"\n        },\n        "container_name": {\n          "title": "Container Name",\n          "description": "Azure storage account container name.",\n          "type": "string"\n        },\n        "account_name": {\n          "title": "Account Name",\n          "description": "Name of the Azure storage account.  See [Microsoft official documentation on how to create a storage account.](https://docs.microsoft.com/en-us/azure/storage/blobs/create-data-lake-storage-account)",\n          "type": "string"\n        },\n        "account_key": {\n          "title": "Account Key",\n          "description": "Azure storage account access key that can be used as a credential. **An account key, a SAS token or a client secret is required for authentication.**",\n          "type": "string"\n        },\n        "sas_token": {\n          "title": "Sas Token",\n          "description": "Azure storage account Shared Access Signature (SAS) token that can be used as a credential. **An account key, a SAS token or a client secret is required for authentication.**",\n          "type": "string"\n        },\n        "client_secret": {\n          "title": "Client Secret",\n          "description": "Azure client secret that can be used as a credential. **An account key, a SAS token or a client secret is required for authentication.**",\n          "type": "string"\n        },\n        "client_id": {\n          "title": "Client Id",\n          "description": "Azure client (Application) ID required when a `client_secret` is used as a credential.",\n          "type": "string"\n        },\n        "tenant_id": {\n          "title": "Tenant Id",\n          "description": "Azure tenant (Directory) ID required when a `client_secret` is used as a credential.",\n          "type": "string"\n        }\n      },\n      "required": [\n        "container_name",\n        "account_name"\n      ],\n      "additionalProperties": false\n    },\n    "OperationConfig": {\n      "title": "OperationConfig",\n      "type": "object",\n      "properties": {\n        "lower_freq_profile_enabled": {\n          "title": "Lower Freq Profile Enabled",\n          "description": "Whether to do profiling at lower freq or not. This does not do any scheduling just adds additional checks to when not to run profiling.",\n          "default": false,\n          "type": "boolean"\n        },\n        "profile_day_of_week": {\n          "title": "Profile Day Of Week",\n          "description": "Number between 0 to 6 for day of week (both inclusive). 0 is Monday and 6 is Sunday. If not specified, defaults to Nothing and this field does not take affect.",\n          "type": "integer"\n        },\n        "profile_date_of_month": {\n          "title": "Profile Date Of Month",\n          "description": "Number between 1 to 31 for date of month (both inclusive). If not specified, defaults to Nothing and this field does not take affect.",\n          "type": "integer"\n        }\n      },\n      "additionalProperties": false\n    },\n    "DataLakeProfilerConfig": {\n      "title": "DataLakeProfilerConfig",\n      "type": "object",\n      "properties": {\n        "enabled": {\n          "title": "Enabled",\n          "description": "Whether profiling should be done.",\n          "default": false,\n          "type": "boolean"\n        },\n        "operation_config": {\n          "title": "Operation Config",\n          "description": "Experimental feature. To specify operation configs.",\n          "allOf": [\n            {\n              "$ref": "#/definitions/OperationConfig"\n            }\n          ]\n        },\n        "profile_table_level_only": {\n          "title": "Profile Table Level Only",\n          "description": "Whether to perform profiling at table-level only or include column-level profiling as well.",\n          "default": false,\n          "type": "boolean"\n        },\n        "max_number_of_fields_to_profile": {\n          "title": "Max Number Of Fields To Profile",\n          "description": "A positive integer that specifies the maximum number of columns to profile for any table. `None` implies all columns. The cost of profiling goes up significantly as the number of columns to profile goes up.",\n          "exclusiveMinimum": 0,\n          "type": "integer"\n        },\n        "include_field_null_count": {\n          "title": "Include Field Null Count",\n          "description": "Whether to profile for the number of nulls for each column.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_min_value": {\n          "title": "Include Field Min Value",\n          "description": "Whether to profile for the min value of numeric columns.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_max_value": {\n          "title": "Include Field Max Value",\n          "description": "Whether to profile for the max value of numeric columns.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_mean_value": {\n          "title": "Include Field Mean Value",\n          "description": "Whether to profile for the mean value of numeric columns.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_median_value": {\n          "title": "Include Field Median Value",\n          "description": "Whether to profile for the median value of numeric columns.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_stddev_value": {\n          "title": "Include Field Stddev Value",\n          "description": "Whether to profile for the standard deviation of numeric columns.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_quantiles": {\n          "title": "Include Field Quantiles",\n          "description": "Whether to profile for the quantiles of numeric columns.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_distinct_value_frequencies": {\n          "title": "Include Field Distinct Value Frequencies",\n          "description": "Whether to profile for distinct value frequencies.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_histogram": {\n          "title": "Include Field Histogram",\n          "description": "Whether to profile for the histogram for numeric fields.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_sample_values": {\n          "title": "Include Field Sample Values",\n          "description": "Whether to profile for the sample values for all columns.",\n          "default": true,\n          "type": "boolean"\n        }\n      },\n      "additionalProperties": false\n    }\n  }\n}\n')))),(0,n.yg)("h3",{id:"path-specs"},"Path Specs"),(0,n.yg)("p",null,"Path Specs (",(0,n.yg)("inlineCode",{parentName:"p"},"path_specs"),") is a list of Path Spec (",(0,n.yg)("inlineCode",{parentName:"p"},"path_spec"),") objects, where each individual ",(0,n.yg)("inlineCode",{parentName:"p"},"path_spec")," represents one or more datasets. The include path (",(0,n.yg)("inlineCode",{parentName:"p"},"path_spec.include"),") represents a formatted path to the dataset. This path must end with ",(0,n.yg)("inlineCode",{parentName:"p"},"*.*")," or ",(0,n.yg)("inlineCode",{parentName:"p"},"*.[ext]")," to represent the leaf level. If ",(0,n.yg)("inlineCode",{parentName:"p"},"*.[ext]"),' is provided, then only files with the specified extension type will be scanned. "',(0,n.yg)("inlineCode",{parentName:"p"},".[ext]"),'" can be any of the ',(0,n.yg)("a",{parentName:"p",href:"#supported-file-types"},"supported file types"),". Refer to ",(0,n.yg)("a",{parentName:"p",href:"#example-1---individual-file-as-dataset"},"example 1")," below for more details."),(0,n.yg)("p",null,"All folder levels need to be specified in the include path. You can use ",(0,n.yg)("inlineCode",{parentName:"p"},"/*/")," to represent a folder level and avoid specifying the exact folder name. To map a folder as a dataset, use the ",(0,n.yg)("inlineCode",{parentName:"p"},"{table}")," placeholder to represent the folder level for which the dataset is to be created. For a partitioned dataset, you can use the placeholder ",(0,n.yg)("inlineCode",{parentName:"p"},"{partition_key[i]}")," to represent the name of the ",(0,n.yg)("inlineCode",{parentName:"p"},"i"),"th partition and ",(0,n.yg)("inlineCode",{parentName:"p"},"{partition[i]}")," to represent the value of the ",(0,n.yg)("inlineCode",{parentName:"p"},"i"),"th partition. During ingestion, ",(0,n.yg)("inlineCode",{parentName:"p"},"i")," will be used to match the partition_key to the partition. Refer to ",(0,n.yg)("a",{parentName:"p",href:"#example-2---folder-of-files-as-dataset-without-partitions"},"examples 2 and 3")," below for more details."),(0,n.yg)("p",null,"Exclude paths (",(0,n.yg)("inlineCode",{parentName:"p"},"path_spec.exclude"),") can be used to ignore paths that are not relevant to the current ",(0,n.yg)("inlineCode",{parentName:"p"},"path_spec"),". This path cannot have named variables (",(0,n.yg)("inlineCode",{parentName:"p"},"{}"),"). The exclude path can have ",(0,n.yg)("inlineCode",{parentName:"p"},"**")," to represent multiple folder levels. Refer to ",(0,n.yg)("a",{parentName:"p",href:"#example-4---folder-of-files-as-dataset-with-partitions-and-exclude-filter"},"example 4")," below for more details."),(0,n.yg)("p",null,"Refer to ",(0,n.yg)("a",{parentName:"p",href:"#example-5---advanced---either-individual-file-or-folder-of-files-as-dataset"},"example 5")," if your container has a more complex dataset representation."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Additional points to note")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Folder names should not contain {, }, ","*",", / in their names."),(0,n.yg)("li",{parentName:"ul"},"Named variable {folder} is reserved for internal working. please do not use in named variables.")),(0,n.yg)("h3",{id:"path-specs---examples"},"Path Specs - Examples"),(0,n.yg)("h4",{id:"example-1---individual-file-as-dataset"},"Example 1 - Individual file as Dataset"),(0,n.yg)("p",null,"Container structure:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"test-container\n\u251c\u2500\u2500 employees.csv\n\u251c\u2500\u2500 departments.json\n\u2514\u2500\u2500 food_items.csv\n")),(0,n.yg)("p",null,"Path specs config to ingest ",(0,n.yg)("inlineCode",{parentName:"p"},"employees.csv")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"food_items.csv")," as datasets:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"path_specs:\n    - include: https://storageaccountname.blob.core.windows.net/test-container/*.csv\n\n")),(0,n.yg)("p",null,"This will automatically ignore ",(0,n.yg)("inlineCode",{parentName:"p"},"departments.json")," file. To include it, use ",(0,n.yg)("inlineCode",{parentName:"p"},"*.*")," instead of ",(0,n.yg)("inlineCode",{parentName:"p"},"*.csv"),"."),(0,n.yg)("h4",{id:"example-2---folder-of-files-as-dataset-without-partitions"},"Example 2 - Folder of files as Dataset (without Partitions)"),(0,n.yg)("p",null,"Container structure:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"test-container\n\u2514\u2500\u2500  offers\n     \u251c\u2500\u2500 1.avro\n     \u2514\u2500\u2500 2.avro\n\n")),(0,n.yg)("p",null,"Path specs config to ingest folder ",(0,n.yg)("inlineCode",{parentName:"p"},"offers")," as dataset:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"path_specs:\n    - include: https://storageaccountname.blob.core.windows.net/test-container/{table}/*.avro\n")),(0,n.yg)("p",null,(0,n.yg)("inlineCode",{parentName:"p"},"{table}")," represents folder for which dataset will be created."),(0,n.yg)("h4",{id:"example-3---folder-of-files-as-dataset-with-partitions"},"Example 3 - Folder of files as Dataset (with Partitions)"),(0,n.yg)("p",null,"Container structure:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"test-container\n\u251c\u2500\u2500 orders\n\u2502   \u2514\u2500\u2500 year=2022\n\u2502       \u2514\u2500\u2500 month=2\n\u2502           \u251c\u2500\u2500 1.parquet\n\u2502           \u2514\u2500\u2500 2.parquet\n\u2514\u2500\u2500 returns\n    \u2514\u2500\u2500 year=2021\n        \u2514\u2500\u2500 month=2\n            \u2514\u2500\u2500 1.parquet\n\n")),(0,n.yg)("p",null,"Path specs config to ingest folders ",(0,n.yg)("inlineCode",{parentName:"p"},"orders")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"returns")," as datasets:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"path_specs:\n    - include: https://storageaccountname.blob.core.windows.net/test-container/{table}/{partition_key[0]}={partition[0]}/{partition_key[1]}={partition[1]}/*.parquet\n")),(0,n.yg)("p",null,"One can also use ",(0,n.yg)("inlineCode",{parentName:"p"},"include: https://storageaccountname.blob.core.windows.net/test-container/{table}/*/*/*.parquet")," here however above format is preferred as it allows declaring partitions explicitly."),(0,n.yg)("h4",{id:"example-4---folder-of-files-as-dataset-with-partitions-and-exclude-filter"},"Example 4 - Folder of files as Dataset (with Partitions), and Exclude Filter"),(0,n.yg)("p",null,"Container structure:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"test-container\n\u251c\u2500\u2500 orders\n\u2502   \u2514\u2500\u2500 year=2022\n\u2502       \u2514\u2500\u2500 month=2\n\u2502           \u251c\u2500\u2500 1.parquet\n\u2502           \u2514\u2500\u2500 2.parquet\n\u2514\u2500\u2500 tmp_orders\n    \u2514\u2500\u2500 year=2021\n        \u2514\u2500\u2500 month=2\n            \u2514\u2500\u2500 1.parquet\n\n\n")),(0,n.yg)("p",null,"Path specs config to ingest folder ",(0,n.yg)("inlineCode",{parentName:"p"},"orders")," as dataset but not folder ",(0,n.yg)("inlineCode",{parentName:"p"},"tmp_orders"),":"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"path_specs:\n    - include: https://storageaccountname.blob.core.windows.net/test-container/{table}/{partition_key[0]}={partition[0]}/{partition_key[1]}={partition[1]}/*.parquet\n      exclude:\n        - **/tmp_orders/**\n")),(0,n.yg)("h4",{id:"example-5---advanced---either-individual-file-or-folder-of-files-as-dataset"},"Example 5 - Advanced - Either Individual file OR Folder of files as Dataset"),(0,n.yg)("p",null,"Container structure:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"test-container\n\u251c\u2500\u2500 customers\n\u2502   \u251c\u2500\u2500 part1.json\n\u2502   \u251c\u2500\u2500 part2.json\n\u2502   \u251c\u2500\u2500 part3.json\n\u2502   \u2514\u2500\u2500 part4.json\n\u251c\u2500\u2500 employees.csv\n\u251c\u2500\u2500 food_items.csv\n\u251c\u2500\u2500 tmp_10101000.csv\n\u2514\u2500\u2500  orders\n     \u2514\u2500\u2500 year=2022\n         \u2514\u2500\u2500 month=2\n             \u251c\u2500\u2500 1.parquet\n             \u251c\u2500\u2500 2.parquet\n             \u2514\u2500\u2500 3.parquet\n\n")),(0,n.yg)("p",null,"Path specs config:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"path_specs:\n    - include: https://storageaccountname.blob.core.windows.net/test-container/*.csv\n      exclude:\n        - **/tmp_10101000.csv\n    - include: https://storageaccountname.blob.core.windows.net/test-container/{table}/*.json\n    - include: https://storageaccountname.blob.core.windows.net/test-container/{table}/{partition_key[0]}={partition[0]}/{partition_key[1]}={partition[1]}/*.parquet\n")),(0,n.yg)("p",null,"Above config has 3 path_specs and will ingest following datasets"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"employees.csv")," - Single File as Dataset"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"food_items.csv")," - Single File as Dataset"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"customers")," - Folder as Dataset"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"orders")," - Folder as Dataset\nand will ignore file ",(0,n.yg)("inlineCode",{parentName:"li"},"tmp_10101000.csv"))),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Valid path_specs.include")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},"https://storageaccountname.blob.core.windows.net/my-container/foo/tests/bar.avro # single file table\nhttps://storageaccountname.blob.core.windows.net/my-container/foo/tests/*.* # mulitple file level tables\nhttps://storageaccountname.blob.core.windows.net/my-container/foo/tests/{table}/*.avro #table without partition\nhttps://storageaccountname.blob.core.windows.net/my-container/foo/tests/{table}/*/*.avro #table where partitions are not specified\nhttps://storageaccountname.blob.core.windows.net/my-container/foo/tests/{table}/*.* # table where no partitions as well as data type specified\nhttps://storageaccountname.blob.core.windows.net/my-container/{dept}/tests/{table}/*.avro # specifying keywords to be used in display name\nhttps://storageaccountname.blob.core.windows.net/my-container/{dept}/tests/{table}/{partition_key[0]}={partition[0]}/{partition_key[1]}={partition[1]}/*.avro # specify partition key and value format\nhttps://storageaccountname.blob.core.windows.net/my-container/{dept}/tests/{table}/{partition[0]}/{partition[1]}/{partition[2]}/*.avro # specify partition value only format\nhttps://storageaccountname.blob.core.windows.net/my-container/{dept}/tests/{table}/{partition[0]}/{partition[1]}/{partition[2]}/*.* # for all extensions\nhttps://storageaccountname.blob.core.windows.net/my-container/*/{table}/{partition[0]}/{partition[1]}/{partition[2]}/*.* # table is present at 2 levels down in container\nhttps://storageaccountname.blob.core.windows.net/my-container/*/*/{table}/{partition[0]}/{partition[1]}/{partition[2]}/*.* # table is present at 3 levels down in container\n")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Valid path_specs.exclude")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"*","*","/tests/","*","*"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://storageaccountname.blob.core.windows.net/my-container/hr/"},"https://storageaccountname.blob.core.windows.net/my-container/hr/"),"**"),(0,n.yg)("li",{parentName:"ul"},"*",(0,n.yg)("em",{parentName:"li"},"/tests/"),".csv"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://storageaccountname.blob.core.windows.net/my-container/foo/*/my_table/"},"https://storageaccountname.blob.core.windows.net/my-container/foo/*/my_table/"),"**")),(0,n.yg)("p",null,"If you would like to write a more complicated function for resolving file names, then a {transformer} would be a good fit."),(0,n.yg)("admonition",{type:"caution"},(0,n.yg)("p",{parentName:"admonition"},"Specify as long fixed prefix ( with out /","*","/ ) as possible in ",(0,n.yg)("inlineCode",{parentName:"p"},"path_specs.include"),". This will reduce the scanning time and cost, specifically on AWS S3")),(0,n.yg)("admonition",{type:"caution"},(0,n.yg)("p",{parentName:"admonition"},"Running profiling against many tables or over many rows can run up significant costs.\nWhile we've done our best to limit the expensiveness of the queries the profiler runs, you\nshould be prudent about the set of tables profiling is enabled on or the frequency\nof the profiling runs.")),(0,n.yg)("admonition",{type:"caution"},(0,n.yg)("p",{parentName:"admonition"},"If you are ingesting datasets from AWS S3, we recommend running the ingestion on a server in the same region to avoid high egress costs.")),(0,n.yg)("h3",{id:"compatibility"},"Compatibility"),(0,n.yg)("p",null,"Profiles are computed with PyDeequ, which relies on PySpark. Therefore, for computing profiles, we currently require Spark 3.0.3 with Hadoop 3.2 to be installed and the ",(0,n.yg)("inlineCode",{parentName:"p"},"SPARK_HOME")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"SPARK_VERSION")," environment variables to be set. The Spark+Hadoop binary can be downloaded ",(0,n.yg)("a",{parentName:"p",href:"https://www.apache.org/dyn/closer.lua/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz"},"here"),"."),(0,n.yg)("p",null,"For an example guide on setting up PyDeequ on AWS, see ",(0,n.yg)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/big-data/testing-data-quality-at-scale-with-pydeequ/"},"this guide"),"."),(0,n.yg)("admonition",{type:"caution"},(0,n.yg)("p",{parentName:"admonition"},"From Spark 3.2.0+, Avro reader fails on column names that don't start with a letter and contains other character than letters, number, and underscore. ","[https://github.com/apache/spark/blob/72c62b6596d21e975c5597f8fff84b1a9d070a02/connector/avro/src/main/scala/org/apache/spark/sql/avro/AvroFileFormat.scala#L158]","\nAvro files that contain such columns won't be profiled.")),(0,n.yg)("h3",{id:"code-coordinates"},"Code Coordinates"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Class Name: ",(0,n.yg)("inlineCode",{parentName:"li"},"datahub.ingestion.source.abs.source.ABSSource")),(0,n.yg)("li",{parentName:"ul"},"Browse on ",(0,n.yg)("a",{parentName:"li",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/ingestion/source/abs/source.py"},"GitHub"))),(0,n.yg)("h2",null,"Questions"),(0,n.yg)("p",null,"If you've got any questions on configuring ingestion for ABS Data Lake, feel free to ping us on ",(0,n.yg)("a",{parentName:"p",href:"https://datahub.com/slack"},"our Slack"),"."))}u.isMDXComponent=!0}}]);