"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[6955],{15680:(e,a,t)=>{t.d(a,{xA:()=>u,yg:()=>c});var n=t(96540);function l(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function r(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){l(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function o(e,a){if(null==e)return{};var t,n,l=function(e,a){if(null==e)return{};var t,n,l={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(l[t]=e[t]);return l}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(l[t]=e[t])}return l}var s=n.createContext({}),p=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):r(r({},a),e)),t},u=function(e){var a=p(e.components);return n.createElement(s.Provider,{value:a},e.children)},g="mdxType",d={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},m=n.forwardRef((function(e,a){var t=e.components,l=e.mdxType,i=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),g=p(t),m=l,c=g["".concat(s,".").concat(m)]||g[m]||d[m]||i;return t?n.createElement(c,r(r({ref:a},u),{},{components:t})):n.createElement(c,r({ref:a},u))}));function c(e,a){var t=arguments,l=a&&a.mdxType;if("string"==typeof e||l){var i=t.length,r=new Array(i);r[0]=m;var o={};for(var s in a)hasOwnProperty.call(a,s)&&(o[s]=a[s]);o.originalType=e,o[g]="string"==typeof e?e:l,r[1]=o;for(var p=2;p<i;p++)r[p]=t[p];return n.createElement.apply(null,r)}return n.createElement.apply(null,t)}m.displayName="MDXCreateElement"},38720:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>u,contentTitle:()=>s,default:()=>c,frontMatter:()=>o,metadata:()=>p,toc:()=>g});t(96540);var n=t(15680);function l(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){return a=null!=a?a:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):function(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))})),e}function r(e,a){if(null==e)return{};var t,n,l=function(e,a){if(null==e)return{};var t,n,l={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(l[t]=e[t]);return l}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(l[t]=e[t])}return l}const o={title:"Airflow Integration",slug:"/lineage/airflow",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/lineage/airflow.md"},s="Airflow Integration",p={unversionedId:"docs/lineage/airflow",id:"docs/lineage/airflow",title:"Airflow Integration",description:"If you're looking to schedule DataHub ingestion using Airflow, see the guide on scheduling ingestion with Airflow.",source:"@site/genDocs/docs/lineage/airflow.md",sourceDirName:"docs/lineage",slug:"/lineage/airflow",permalink:"/docs/lineage/airflow",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/lineage/airflow.md",tags:[],version:"current",frontMatter:{title:"Airflow Integration",slug:"/lineage/airflow",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/lineage/airflow.md"},sidebar:"overviewSidebar",previous:{title:"Sources",permalink:"/docs/metadata-ingestion/source_overview"},next:{title:"Dagster Integration",permalink:"/docs/lineage/dagster"}},u={},g=[{value:"DataHub Plugin Setup",id:"datahub-plugin-setup",level:2},{value:"Installation",id:"installation",level:3},{value:"Configuration",id:"configuration",level:3},{value:"Command Line",id:"command-line",level:4},{value:"Airflow UI",id:"airflow-ui",level:4},{value:"Optional Configurations",id:"optional-configurations",level:4},{value:"Automatic lineage extraction",id:"automatic-lineage-extraction",level:2},{value:"Multi-Statement SQL Parsing",id:"multi-statement-sql-parsing",level:3},{value:"Manual Lineage Annotation",id:"manual-lineage-annotation",level:2},{value:"Using <code>inlets</code> and <code>outlets</code>",id:"using-inlets-and-outlets",level:3},{value:"Native Airflow Assets/Datasets",id:"native-airflow-assetsdatasets",level:3},{value:"Configuration",id:"configuration-1",level:4},{value:"Limitations",id:"limitations",level:4},{value:"Custom Operators",id:"custom-operators",level:3},{value:"Custom SQL Operators with Automatic Lineage",id:"custom-sql-operators-with-automatic-lineage",level:3},{value:"Option 1: Inherit from SQLExecuteQueryOperator (Recommended)",id:"option-1-inherit-from-sqlexecutequeryoperator-recommended",level:4},{value:"Option 2: Implement OpenLineage Interface from Scratch",id:"option-2-implement-openlineage-interface-from-scratch",level:4},{value:"Alternative: Custom Operators with Manual Lineage (Airflow 2.x and 3.x)",id:"alternative-custom-operators-with-manual-lineage-airflow-2x-and-3x",level:3},{value:"Custom Extractors (Advanced - Legacy OpenLineage)",id:"custom-extractors-advanced---legacy-openlineage",level:3},{value:"Cleanup obsolete pipelines and tasks from Datahub",id:"cleanup-obsolete-pipelines-and-tasks-from-datahub",level:2},{value:"Get all dataJobs associated with a dataFlow",id:"get-all-datajobs-associated-with-a-dataflow",level:2},{value:"Emit Lineage Directly",id:"emit-lineage-directly",level:2},{value:"Debugging",id:"debugging",level:2},{value:"Missing lineage",id:"missing-lineage",level:3},{value:"Incorrect URLs",id:"incorrect-urls",level:3},{value:"TypeError ... missing 3 required positional arguments",id:"typeerror--missing-3-required-positional-arguments",level:3},{value:"Scheduler stalling",id:"scheduler-stalling",level:3},{value:"Disabling the DataHub Plugin",id:"disabling-the-datahub-plugin",level:3},{value:"1. Disable via Configuration",id:"1-disable-via-configuration",level:4},{value:"2. Disable via Environment Variable (Kill-Switch)",id:"2-disable-via-environment-variable-kill-switch",level:4},{value:"Compatibility",id:"compatibility",level:2},{value:"Additional references",id:"additional-references",level:2}],d={toc:g},m="wrapper";function c(e){var{components:a}=e,t=r(e,["components"]);return(0,n.yg)(m,i(function(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{},n=Object.keys(t);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(t).filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable})))),n.forEach((function(a){l(e,a,t[a])}))}return e}({},d,t),{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"airflow-integration"},"Airflow Integration"),(0,n.yg)("admonition",{type:"note"},(0,n.yg)("p",{parentName:"admonition"},"If you're looking to schedule DataHub ingestion using Airflow, see the guide on ",(0,n.yg)("a",{parentName:"p",href:"/docs/metadata-ingestion/schedule_docs/airflow"},"scheduling ingestion with Airflow"),".")),(0,n.yg)("p",null,"The DataHub Airflow plugin supports:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Automatic column-level lineage extraction from various operators e.g. SQL operators (including ",(0,n.yg)("inlineCode",{parentName:"li"},"MySqlOperator"),", ",(0,n.yg)("inlineCode",{parentName:"li"},"PostgresOperator"),", ",(0,n.yg)("inlineCode",{parentName:"li"},"SnowflakeOperator"),", ",(0,n.yg)("inlineCode",{parentName:"li"},"BigQueryInsertJobOperator"),", and more), ",(0,n.yg)("inlineCode",{parentName:"li"},"S3FileTransformOperator"),", and more."),(0,n.yg)("li",{parentName:"ul"},"Airflow DAG and tasks, including properties, ownership, and tags."),(0,n.yg)("li",{parentName:"ul"},"Task run information, including task successes and failures."),(0,n.yg)("li",{parentName:"ul"},"Manual lineage annotations using ",(0,n.yg)("inlineCode",{parentName:"li"},"inlets")," and ",(0,n.yg)("inlineCode",{parentName:"li"},"outlets")," on Airflow operators.")),(0,n.yg)("p",null,"The plugin requires Airflow 2.7+ and Python 3.10+. If you're using Airflow older than 2.7, it's possible to use the plugin with older versions of ",(0,n.yg)("inlineCode",{parentName:"p"},"acryl-datahub-airflow-plugin"),". See the ",(0,n.yg)("a",{parentName:"p",href:"#compatibility"},"compatibility section")," for more details."),(0,n.yg)("h2",{id:"datahub-plugin-setup"},"DataHub Plugin Setup"),(0,n.yg)("h3",{id:"installation"},"Installation"),(0,n.yg)("p",null,"The plugin requires Airflow 2.7+ and Python 3.10+. If you don't meet these requirements, see the ",(0,n.yg)("a",{parentName:"p",href:"#compatibility"},"compatibility section")," for other options."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"pip install 'acryl-datahub-airflow-plugin>=1.1.0.4'\n")),(0,n.yg)("h3",{id:"configuration"},"Configuration"),(0,n.yg)("p",null,"Set up a DataHub connection in Airflow, either via command line or the Airflow UI."),(0,n.yg)("h4",{id:"command-line"},"Command Line"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"airflow connections add  --conn-type 'datahub-rest' 'datahub_rest_default' --conn-host 'http://datahub-gms:8080' --conn-password '<optional datahub auth token>'\n")),(0,n.yg)("p",null,"If you are using DataHub Cloud then please use ",(0,n.yg)("inlineCode",{parentName:"p"},"https://YOUR_PREFIX.acryl.io/gms")," as the ",(0,n.yg)("inlineCode",{parentName:"p"},"--conn-host")," parameter."),(0,n.yg)("h4",{id:"airflow-ui"},"Airflow UI"),(0,n.yg)("p",null,'On the Airflow UI, go to Admin -> Connections and click the "+" symbol to create a new connection. Select "DataHub REST Server" from the dropdown for "Connection Type" and enter the appropriate values.'),(0,n.yg)("p",{align:"center"},(0,n.yg)("img",{width:"70%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/airflow/plugin_connection_setup.png"})),(0,n.yg)("h4",{id:"optional-configurations"},"Optional Configurations"),(0,n.yg)("p",null,"No additional configuration is required to use the plugin. However, there are some optional configuration parameters that can be set in the ",(0,n.yg)("inlineCode",{parentName:"p"},"airflow.cfg")," file."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-ini",metastring:'title="airflow.cfg"',title:'"airflow.cfg"'},"[datahub]\n# Optional - additional config here.\nenabled = True  # default\n")),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Name"),(0,n.yg)("th",{parentName:"tr",align:null},"Default value"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"enabled"),(0,n.yg)("td",{parentName:"tr",align:null},"true"),(0,n.yg)("td",{parentName:"tr",align:null},"If the plugin should be enabled.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"conn_id"),(0,n.yg)("td",{parentName:"tr",align:null},"datahub_rest_default"),(0,n.yg)("td",{parentName:"tr",align:null},"The name of the datahub rest connection.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"cluster"),(0,n.yg)("td",{parentName:"tr",align:null},"prod"),(0,n.yg)("td",{parentName:"tr",align:null},"name of the airflow cluster, this is equivalent to the ",(0,n.yg)("inlineCode",{parentName:"td"},"env")," of the instance")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"platform_instance"),(0,n.yg)("td",{parentName:"tr",align:null},"None"),(0,n.yg)("td",{parentName:"tr",align:null},"The instance of the platform that all assets produced by this plugin belong to. It is optional.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"capture_ownership_info"),(0,n.yg)("td",{parentName:"tr",align:null},"true"),(0,n.yg)("td",{parentName:"tr",align:null},"Extract DAG ownership.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"capture_ownership_as_group"),(0,n.yg)("td",{parentName:"tr",align:null},"false"),(0,n.yg)("td",{parentName:"tr",align:null},"When extracting DAG ownership, treat DAG owner as a group rather than a user")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"capture_tags_info"),(0,n.yg)("td",{parentName:"tr",align:null},"true"),(0,n.yg)("td",{parentName:"tr",align:null},"Extract DAG tags.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"capture_executions"),(0,n.yg)("td",{parentName:"tr",align:null},"true"),(0,n.yg)("td",{parentName:"tr",align:null},'Extract task runs and success/failure statuses. This will show up in DataHub "Runs" tab.')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"materialize_iolets"),(0,n.yg)("td",{parentName:"tr",align:null},"true"),(0,n.yg)("td",{parentName:"tr",align:null},"Create or un-soft-delete all entities referenced in lineage.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"enable_extractors"),(0,n.yg)("td",{parentName:"tr",align:null},"true"),(0,n.yg)("td",{parentName:"tr",align:null},"Enable automatic lineage extraction.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"disable_openlineage_plugin"),(0,n.yg)("td",{parentName:"tr",align:null},"true"),(0,n.yg)("td",{parentName:"tr",align:null},"Disable the OpenLineage plugin to avoid duplicative processing.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"enable_multi_statement_sql_parsing"),(0,n.yg)("td",{parentName:"tr",align:null},"false"),(0,n.yg)("td",{parentName:"tr",align:null},"Parse multiple SQL statements within a single task. Resolves temp tables and merges lineage across statements in one execution.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"log_level"),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("em",{parentName:"td"},"no change")),(0,n.yg)("td",{parentName:"tr",align:null},"[debug]"," Set the log level for the plugin.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"debug_emitter"),(0,n.yg)("td",{parentName:"tr",align:null},"false"),(0,n.yg)("td",{parentName:"tr",align:null},"[debug]"," If true, the plugin will log the emitted events.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"dag_filter_str"),(0,n.yg)("td",{parentName:"tr",align:null},'{ "allow": ','[".*"]'," }"),(0,n.yg)("td",{parentName:"tr",align:null},"AllowDenyPattern value in form of JSON string to filter the DAGs from running.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"enable_datajob_lineage"),(0,n.yg)("td",{parentName:"tr",align:null},"true"),(0,n.yg)("td",{parentName:"tr",align:null},"If true, the plugin will emit input/output lineage for DataJobs.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"capture_airflow_assets"),(0,n.yg)("td",{parentName:"tr",align:null},"true"),(0,n.yg)("td",{parentName:"tr",align:null},"Capture native Airflow Assets/Datasets as DataHub lineage. See ",(0,n.yg)("a",{parentName:"td",href:"#native-airflow-assetsdatasets"},"Native Airflow Assets/Datasets"),".")))),(0,n.yg)("h2",{id:"automatic-lineage-extraction"},"Automatic lineage extraction"),(0,n.yg)("p",null,"To automatically extract lineage information, the plugin builds on top of Airflow's built-in ",(0,n.yg)("a",{parentName:"p",href:"https://openlineage.io/docs/integrations/airflow/default-extractors"},"OpenLineage extractors"),".\nAs such, we support a superset of the default operators that Airflow/OpenLineage supports."),(0,n.yg)("p",null,"The SQL-related extractors have been updated to use ",(0,n.yg)("a",{parentName:"p",href:"/docs/lineage/sql_parsing"},"DataHub's SQL lineage parser"),", which is more robust than the built-in one and uses DataHub's metadata information to generate column-level lineage."),(0,n.yg)("p",null,"Supported operators:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"SQLExecuteQueryOperator"),", including any subclasses. Note that in newer versions of Airflow (generally Airflow 2.5+), most SQL operators inherit from this class."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"AthenaOperator")," and ",(0,n.yg)("inlineCode",{parentName:"li"},"AWSAthenaOperator")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"BigQueryOperator")," and ",(0,n.yg)("inlineCode",{parentName:"li"},"BigQueryExecuteQueryOperator")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"BigQueryInsertJobOperator")," (incubating)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"MySqlOperator")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"PostgresOperator")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"RedshiftSQLOperator")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"SnowflakeOperator")," and ",(0,n.yg)("inlineCode",{parentName:"li"},"SnowflakeOperatorAsync")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"SqliteOperator")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"TeradataOperator")," (",(0,n.yg)("em",{parentName:"li"},"Note: Teradata uses two-tier ",(0,n.yg)("inlineCode",{parentName:"em"},"database.table")," naming without a schema level"),")"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"TrinoOperator"))),(0,n.yg)("h3",{id:"multi-statement-sql-parsing"},"Multi-Statement SQL Parsing"),(0,n.yg)("p",null,"When a task executes multiple SQL statements (e.g., ",(0,n.yg)("inlineCode",{parentName:"p"},"CREATE TEMP TABLE ...; INSERT ... FROM temp_table;"),"), enable this to parse all statements together and resolve temporary table dependencies. By default (False), only the first statement is parsed."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-ini",metastring:'title="airflow.cfg"',title:'"airflow.cfg"'},"[datahub]\nenable_multi_statement_sql_parsing = True  # Default: False\n")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Note:")," Use a list of SQL strings (recommended) or semicolon-separated statements in a single string:"),(0,n.yg)("h2",{id:"manual-lineage-annotation"},"Manual Lineage Annotation"),(0,n.yg)("h3",{id:"using-inlets-and-outlets"},"Using ",(0,n.yg)("inlineCode",{parentName:"h3"},"inlets")," and ",(0,n.yg)("inlineCode",{parentName:"h3"},"outlets")),(0,n.yg)("p",null,"You can manually annotate lineage by setting ",(0,n.yg)("inlineCode",{parentName:"p"},"inlets")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"outlets")," on your Airflow operators. This is useful if you're using an operator that doesn't support automatic lineage extraction, or if you want to override the automatic lineage extraction."),(0,n.yg)("p",null,"We have a few code samples that demonstrate how to use ",(0,n.yg)("inlineCode",{parentName:"p"},"inlets")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"outlets"),":"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion-modules/airflow-plugin/src/datahub_airflow_plugin/example_dags/lineage_backend_demo.py"},(0,n.yg)("inlineCode",{parentName:"a"},"lineage_backend_demo.py"))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion-modules/airflow-plugin/src/datahub_airflow_plugin/example_dags/lineage_backend_taskflow_demo.py"},(0,n.yg)("inlineCode",{parentName:"a"},"lineage_backend_taskflow_demo.py"))," - uses the ",(0,n.yg)("a",{parentName:"li",href:"https://airflow.apache.org/docs/apache-airflow/stable/concepts/taskflow.html"},"TaskFlow API"))),(0,n.yg)("p",null,"For more information, take a look at the ",(0,n.yg)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/lineage.html"},"Airflow lineage docs"),"."),(0,n.yg)("h3",{id:"native-airflow-assetsdatasets"},"Native Airflow Assets/Datasets"),(0,n.yg)("p",null,"Starting with Airflow 2.4+, you can use native Airflow ",(0,n.yg)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/datasets.html"},"Datasets")," (renamed to ",(0,n.yg)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/3.0.0/authoring-and-scheduling/assets.html"},"Assets")," in Airflow 3.x) for data-aware scheduling. The DataHub plugin automatically captures these as lineage when used in ",(0,n.yg)("inlineCode",{parentName:"p"},"inlets")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"outlets"),"."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from airflow.sdk.definitions.asset import Asset  # Airflow 3.x\n# or: from airflow.datasets import Dataset as Asset  # Airflow 2.4+\n\ns3_input = Asset("s3://my-bucket/input/data.parquet")\nbigquery_output = Asset("bigquery://my-project/dataset/result_table")\n\ntask = BashOperator(\n    task_id="process_data",\n    bash_command="echo \'Processing\'",\n    inlets=[s3_input],\n    outlets=[bigquery_output],\n)\n')),(0,n.yg)("p",null,"The plugin maps URI schemes to DataHub platforms:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"URI Scheme"),(0,n.yg)("th",{parentName:"tr",align:null},"DataHub Platform"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"s3://"),", ",(0,n.yg)("inlineCode",{parentName:"td"},"s3a://")),(0,n.yg)("td",{parentName:"tr",align:null},"s3")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"gs://"),", ",(0,n.yg)("inlineCode",{parentName:"td"},"gcs://")),(0,n.yg)("td",{parentName:"tr",align:null},"gcs")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"postgresql://")),(0,n.yg)("td",{parentName:"tr",align:null},"postgres")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"mysql://")),(0,n.yg)("td",{parentName:"tr",align:null},"mysql")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"bigquery://")),(0,n.yg)("td",{parentName:"tr",align:null},"bigquery")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"snowflake://")),(0,n.yg)("td",{parentName:"tr",align:null},"snowflake")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"file://")),(0,n.yg)("td",{parentName:"tr",align:null},"file")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hdfs://")),(0,n.yg)("td",{parentName:"tr",align:null},"hdfs")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"abfs://"),", ",(0,n.yg)("inlineCode",{parentName:"td"},"abfss://")),(0,n.yg)("td",{parentName:"tr",align:null},"adls")))),(0,n.yg)("p",null,"Plain name assets (e.g., from the ",(0,n.yg)("inlineCode",{parentName:"p"},"@asset")," decorator) default to the ",(0,n.yg)("inlineCode",{parentName:"p"},"airflow")," platform."),(0,n.yg)("h4",{id:"configuration-1"},"Configuration"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-ini",metastring:'title="airflow.cfg"',title:'"airflow.cfg"'},"[datahub]\n# Set to false to disable capturing Airflow Assets as lineage (default: true)\ncapture_airflow_assets = true\n")),(0,n.yg)("h4",{id:"limitations"},"Limitations"),(0,n.yg)("p",null,"Native Airflow Assets have the following limitations compared to using DataHub's ",(0,n.yg)("inlineCode",{parentName:"p"},"Dataset")," or ",(0,n.yg)("inlineCode",{parentName:"p"},"Urn")," entities directly:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"No ",(0,n.yg)("inlineCode",{parentName:"strong"},"platform_instance")," support"),": The URN generated from an Airflow Asset URI cannot include a platform instance. The plugin only extracts the platform, dataset name, and environment from the URI.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Environment uses global plugin config"),": All native Airflow Assets use the ",(0,n.yg)("inlineCode",{parentName:"p"},"cluster")," setting from the plugin configuration as their environment. You cannot specify a different environment per asset."))),(0,n.yg)("p",null,"If you need ",(0,n.yg)("inlineCode",{parentName:"p"},"platform_instance")," or per-asset environment control, use the DataHub entity classes instead:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from datahub_airflow_plugin.entities import Dataset\n\n# Full control over URN components\ns3_input = Dataset(\n    platform="s3",\n    name="my-bucket/input/data.parquet",\n    env="PROD",\n    platform_instance="us-west-2"  # Specify platform instance\n)\n\ntask = BashOperator(\n    task_id="process_data",\n    bash_command="echo \'Processing\'",\n    inlets=[s3_input],\n)\n')),(0,n.yg)("h3",{id:"custom-operators"},"Custom Operators"),(0,n.yg)("p",null,"If you have created a ",(0,n.yg)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.html"},"custom Airflow operator")," that inherits from the BaseOperator class,\nwhen overriding the ",(0,n.yg)("inlineCode",{parentName:"p"},"execute")," function, set inlets and outlets via ",(0,n.yg)("inlineCode",{parentName:"p"},"context['ti'].task.inlets")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"context['ti'].task.outlets"),".\nThe DataHub Airflow plugin will then pick up those inlets and outlets after the task runs."),(0,n.yg)("p",null,"You can only set table-level lineage using inlets and outlets. For column-level lineage, you need to write a custom extractor for your custom operator."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},"class DbtOperator(BaseOperator):\n    ...\n\n    def execute(self, context):\n        # do something\n        inlets, outlets = self._get_lineage()\n        # inlets/outlets are lists of either datahub_airflow_plugin.entities.Dataset or datahub_airflow_plugin.entities.Urn\n        context['ti'].task.inlets = self.inlets\n        context['ti'].task.outlets = self.outlets\n\n    def _get_lineage(self):\n        # Do some processing to get inlets/outlets\n\n        return inlets, outlets\n")),(0,n.yg)("p",null,"If you override the ",(0,n.yg)("inlineCode",{parentName:"p"},"pre_execute")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"post_execute")," function, ensure they include the ",(0,n.yg)("inlineCode",{parentName:"p"},"@prepare_lineage")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"@apply_lineage")," decorators respectively. Reference the ",(0,n.yg)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/lineage.html#lineage"},"Airflow docs")," for more details."),(0,n.yg)("p",null,"See example implementation of a custom operator using SQL parser to capture table level lineage ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion-modules/airflow-plugin/tests/integration/dags/airflow3/custom_operator_sql_parsing.py"},"here")),(0,n.yg)("h3",{id:"custom-sql-operators-with-automatic-lineage"},"Custom SQL Operators with Automatic Lineage"),(0,n.yg)("p",null,"If you're building a custom SQL operator, you have two approaches depending on your needs:"),(0,n.yg)("h4",{id:"option-1-inherit-from-sqlexecutequeryoperator-recommended"},"Option 1: Inherit from SQLExecuteQueryOperator (Recommended)"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"This is the easiest approach")," - inherit from Airflow's ",(0,n.yg)("inlineCode",{parentName:"p"},"SQLExecuteQueryOperator")," and you automatically get:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"\u2705 OpenLineage support built-in"),(0,n.yg)("li",{parentName:"ul"},"\u2705 DataHub's enhanced SQL parser (via our SQLParser patch)"),(0,n.yg)("li",{parentName:"ul"},"\u2705 Column-level lineage extraction"),(0,n.yg)("li",{parentName:"ul"},"\u2705 No extra code needed")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from typing import Any\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\nclass MyCustomSQLOperator(SQLExecuteQueryOperator):\n    """\n    Custom SQL operator that inherits OpenLineage support.\n\n    DataHub automatically enhances the SQL parsing with column-level lineage!\n    """\n\n    def __init__(self, my_custom_param: str, **kwargs):\n        # Add your custom parameters\n        self.my_custom_param = my_custom_param\n        super().__init__(**kwargs)\n\n    def execute(self, context: Any) -> Any:\n        # Add any custom logic before SQL execution\n        self.log.info(f"Custom param: {self.my_custom_param}")\n\n        # Parent class handles SQL execution + OpenLineage lineage\n        return super().execute(context)\n')),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"How it works:")),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("inlineCode",{parentName:"li"},"SQLExecuteQueryOperator")," already has ",(0,n.yg)("inlineCode",{parentName:"li"},"get_openlineage_facets_on_complete()")," implemented"),(0,n.yg)("li",{parentName:"ol"},"It uses the hook's OpenLineage methods, which internally call ",(0,n.yg)("inlineCode",{parentName:"li"},"SQLParser")),(0,n.yg)("li",{parentName:"ol"},"DataHub patches ",(0,n.yg)("inlineCode",{parentName:"li"},"SQLParser")," globally, so all SQL parsing gets enhanced automatically"),(0,n.yg)("li",{parentName:"ol"},"You get column-level lineage without writing any lineage-specific code!")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"When to use this:")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Building operators for standard SQL databases (Postgres, MySQL, Snowflake, BigQuery, etc.)"),(0,n.yg)("li",{parentName:"ul"},"Want the simplest integration"),(0,n.yg)("li",{parentName:"ul"},"Don't need custom lineage extraction logic")),(0,n.yg)("h4",{id:"option-2-implement-openlineage-interface-from-scratch"},"Option 2: Implement OpenLineage Interface from Scratch"),(0,n.yg)("p",null,"Only needed if you're building a completely custom operator that doesn't fit the ",(0,n.yg)("inlineCode",{parentName:"p"},"SQLExecuteQueryOperator")," pattern:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from typing import Any, Optional\nfrom airflow.models.baseoperator import BaseOperator\n\nclass MyCompletelyCustomOperator(BaseOperator):\n    """\n    For special cases where SQLExecuteQueryOperator doesn\'t fit.\n    """\n\n    def execute(self, context: Any) -> Any:\n        # Your custom SQL execution logic\n        pass\n\n    def get_openlineage_facets_on_complete(\n        self, task_instance: Any\n    ) -> Optional["OperatorLineage"]:\n        """\n        Implement OpenLineage interface manually.\n        DataHub\'s SQLParser patch still enhances this automatically!\n        """\n        from airflow.providers.openlineage.sqlparser import SQLParser\n\n        hook = self.get_db_hook()\n        parser = SQLParser(\n            dialect=hook.get_openlineage_database_dialect(hook.get_connection(self.conn_id)),\n            default_schema=hook.get_openlineage_default_schema(),\n        )\n\n        # This uses DataHub\'s patched SQLParser - column lineage included!\n        return parser.generate_openlineage_metadata_from_sql(\n            sql=self.sql,\n            hook=hook,\n            database_info=hook.get_openlineage_database_info(hook.get_connection(self.conn_id)),\n        )\n')),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"When to use this:")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Building a very specialized operator from scratch"),(0,n.yg)("li",{parentName:"ul"},"Need complete control over lineage extraction"),(0,n.yg)("li",{parentName:"ul"},"The standard SQLExecuteQueryOperator pattern doesn't apply")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Key Point:")," DataHub patches ",(0,n.yg)("inlineCode",{parentName:"p"},"SQLParser.generate_openlineage_metadata_from_sql()")," globally at import time, so ",(0,n.yg)("strong",{parentName:"p"},"any operator")," using OpenLineage's SQLParser automatically gets DataHub's enhanced parsing with column-level lineage!"),(0,n.yg)("h3",{id:"alternative-custom-operators-with-manual-lineage-airflow-2x-and-3x"},"Alternative: Custom Operators with Manual Lineage (Airflow 2.x and 3.x)"),(0,n.yg)("p",null,"If you prefer not to use OpenLineage, or are on older Airflow versions, you can manually extract and set lineage using DataHub's SQL parser:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from typing import Any, List, Tuple, Union\nfrom airflow.models.baseoperator import BaseOperator\nfrom datahub_airflow_plugin._config import get_enable_multi_statement\nfrom datahub_airflow_plugin._sql_parsing_common import parse_sql_with_datahub\nfrom datahub_airflow_plugin.entities import Urn\n\nclass CustomSQLOperator(BaseOperator):\n    def __init__(self, sql: Union[str, List[str]], database: str, **kwargs: Any):\n        super().__init__(**kwargs)\n        self.sql = sql\n        self.database = database\n\n    def execute(self, context: Any) -> Any:\n        # Execute SQL\n        # ...\n\n        # Extract and set lineage\n        inlets, outlets = self._get_lineage()\n        context["ti"].task.inlets = inlets\n        context["ti"].task.outlets = outlets\n\n    def _get_lineage(self) -> Tuple[List, List]:\n        # Get multi-statement config flag\n        enable_multi_statement = get_enable_multi_statement()\n\n        # Parse SQL with multi-statement support\n        # Handles both string and list of SQL statements\n        sql_parsing_result = parse_sql_with_datahub(\n            sql=self.sql,\n            platform="postgres",  # your platform\n            default_database=self.database,\n            env="PROD",\n            default_schema=None,\n            graph=None,\n            enable_multi_statement=enable_multi_statement,\n        )\n\n        inlets = [Urn(table) for table in sql_parsing_result.in_tables]\n        outlets = [Urn(table) for table in sql_parsing_result.out_tables]\n        return inlets, outlets\n')),(0,n.yg)("p",null,"See ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion-modules/airflow-plugin/tests/integration/dags/airflow3/custom_operator_sql_parsing.py"},"full example"),"."),(0,n.yg)("h3",{id:"custom-extractors-advanced---legacy-openlineage"},"Custom Extractors (Advanced - Legacy OpenLineage)"),(0,n.yg)("p",null,"For advanced use cases with the legacy OpenLineage package (",(0,n.yg)("inlineCode",{parentName:"p"},"openlineage-airflow"),"), you can create a custom extractor. This is useful if you're using a built-in Airflow operator for which we don't support automatic lineage extraction."),(0,n.yg)("p",null,"See this ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/pull/10452"},"example PR")," which adds a custom extractor for the ",(0,n.yg)("inlineCode",{parentName:"p"},"BigQueryInsertJobOperator")," operator."),(0,n.yg)("h2",{id:"cleanup-obsolete-pipelines-and-tasks-from-datahub"},"Cleanup obsolete pipelines and tasks from Datahub"),(0,n.yg)("p",null,"There might be a case where the DAGs are removed from the Airflow but the corresponding pipelines and tasks are still there in the Datahub, let's call such pipelines ans tasks, ",(0,n.yg)("inlineCode",{parentName:"p"},"obsolete pipelines and tasks")),(0,n.yg)("p",null,"Following are the steps to cleanup them from the datahub:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"create a DAG named ",(0,n.yg)("inlineCode",{parentName:"li"},"Datahub_Cleanup"),", i.e.")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\n\nfrom datahub_airflow_plugin.entities import Dataset, Urn\n\nwith DAG(\n    "Datahub_Cleanup",\n    start_date=datetime(2024, 1, 1),\n    schedule_interval=None,\n    catchup=False,\n) as dag:\n    task = BashOperator(\n        task_id="cleanup_obsolete_data",\n        dag=dag,\n        bash_command="echo \'cleaning up the obsolete data from datahub\'",\n    )\n\n')),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"ingest this DAG, and it will remove all the obsolete pipelines and tasks from the Datahub based on the ",(0,n.yg)("inlineCode",{parentName:"li"},"cluster")," value set in the ",(0,n.yg)("inlineCode",{parentName:"li"},"airflow.cfg"))),(0,n.yg)("h2",{id:"get-all-datajobs-associated-with-a-dataflow"},"Get all dataJobs associated with a dataFlow"),(0,n.yg)("p",null,"If you are looking to find all tasks (aka DataJobs) that belong to a specific pipeline (aka DataFlow), you can use the following GraphQL query:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-graphql"},'query {\n  dataFlow(urn: "urn:li:dataFlow:(airflow,db_etl,prod)") {\n    childJobs: relationships(\n      input: { types: ["IsPartOf"], direction: INCOMING, start: 0, count: 100 }\n    ) {\n      total\n      relationships {\n        entity {\n          ... on DataJob {\n            urn\n          }\n        }\n      }\n    }\n  }\n}\n')),(0,n.yg)("h2",{id:"emit-lineage-directly"},"Emit Lineage Directly"),(0,n.yg)("p",null,"If you can't use the plugin or annotate inlets/outlets, you can also emit lineage using the ",(0,n.yg)("inlineCode",{parentName:"p"},"DatahubEmitterOperator"),"."),(0,n.yg)("p",null,"Reference ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion-modules/airflow-plugin/src/datahub_airflow_plugin/example_dags/lineage_emission_dag.py"},(0,n.yg)("inlineCode",{parentName:"a"},"lineage_emission_dag.py"))," for a full example."),(0,n.yg)("p",null,"In order to use this example, you must first configure the Datahub hook. Like in ingestion, we support a Datahub REST hook and a Kafka-based hook. See the plugin configuration for examples."),(0,n.yg)("h2",{id:"debugging"},"Debugging"),(0,n.yg)("h3",{id:"missing-lineage"},"Missing lineage"),(0,n.yg)("p",null,"If you're not seeing lineage in DataHub, check the following:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Validate that the plugin is loaded in Airflow. Go to Admin -> Plugins and check that the DataHub plugin is listed."),(0,n.yg)("li",{parentName:"ul"},"It should also print a log line like ",(0,n.yg)("inlineCode",{parentName:"li"},"INFO  [datahub_airflow_plugin.datahub_listener] DataHub plugin using DataHubRestEmitter: configured to talk to <datahub_url>")," during Airflow startup, and the ",(0,n.yg)("inlineCode",{parentName:"li"},"airflow plugins")," command should list ",(0,n.yg)("inlineCode",{parentName:"li"},"datahub_plugin")," with a listener enabled."),(0,n.yg)("li",{parentName:"ul"},"If using the plugin's automatic lineage, ensure that the ",(0,n.yg)("inlineCode",{parentName:"li"},"enable_extractors")," config is set to true and that automatic lineage is supported for your operator."),(0,n.yg)("li",{parentName:"ul"},"If using manual lineage annotation, ensure that you're using the ",(0,n.yg)("inlineCode",{parentName:"li"},"datahub_airflow_plugin.entities.Dataset")," or ",(0,n.yg)("inlineCode",{parentName:"li"},"datahub_airflow_plugin.entities.Urn")," classes for your inlets and outlets.")),(0,n.yg)("h3",{id:"incorrect-urls"},"Incorrect URLs"),(0,n.yg)("p",null,"If your URLs aren't being generated correctly (usually they'll start with ",(0,n.yg)("inlineCode",{parentName:"p"},"http://localhost:8080")," instead of the correct hostname), you may need to set the webserver ",(0,n.yg)("inlineCode",{parentName:"p"},"base_url")," config."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-ini",metastring:'title="airflow.cfg"',title:'"airflow.cfg"'},"[webserver]\nbase_url = http://airflow.mycorp.example.com\n")),(0,n.yg)("h3",{id:"typeerror--missing-3-required-positional-arguments"},"TypeError ... missing 3 required positional arguments"),(0,n.yg)("p",null,"If you see errors like the following:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"ERROR - on_task_instance_success() missing 3 required positional arguments: 'previous_state', 'task_instance', and 'session'\nTraceback (most recent call last):\n  File \"/home/airflow/.local/lib/python3.8/site-packages/datahub_airflow_plugin/datahub_listener.py\", line 124, in wrapper\n    f(*args, **kwargs)\nTypeError: on_task_instance_success() missing 3 required positional arguments: 'previous_state', 'task_instance', and 'session'\n")),(0,n.yg)("p",null,"The solution is to upgrade ",(0,n.yg)("inlineCode",{parentName:"p"},"acryl-datahub-airflow-plugin>=0.12.0.4")," or upgrade ",(0,n.yg)("inlineCode",{parentName:"p"},"pluggy>=1.2.0"),". See this ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/pull/9365"},"PR")," for details."),(0,n.yg)("h3",{id:"scheduler-stalling"},"Scheduler stalling"),(0,n.yg)("p",null,"For extremely large Airflow deployments with thousands of tasks, you may see issues where the plugin interferes with the performance of the Airflow scheduler. In those cases, you can set the ",(0,n.yg)("inlineCode",{parentName:"p"},"DATAHUB_AIRFLOW_PLUGIN_RUN_IN_THREAD_TIMEOUT=0")," environment variable. This makes the DataHub plugin run fully in background threads, but can cause us to miss some metadata if the scheduler shuts down soon after processing a task."),(0,n.yg)("h3",{id:"disabling-the-datahub-plugin"},"Disabling the DataHub Plugin"),(0,n.yg)("p",null,"There are two ways to disable the DataHub Plugin:"),(0,n.yg)("h4",{id:"1-disable-via-configuration"},"1. Disable via Configuration"),(0,n.yg)("p",null,"Set the ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub.enabled")," configuration property to ",(0,n.yg)("inlineCode",{parentName:"p"},"False")," in the ",(0,n.yg)("inlineCode",{parentName:"p"},"airflow.cfg")," file and restart the Airflow environment to reload the configuration and disable the plugin."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-ini",metastring:'title="airflow.cfg"',title:'"airflow.cfg"'},"[datahub]\nenabled = False\n")),(0,n.yg)("h4",{id:"2-disable-via-environment-variable-kill-switch"},"2. Disable via Environment Variable (Kill-Switch)"),(0,n.yg)("p",null,"If a restart is not possible and you need a faster way to disable the plugin, you can use the kill-switch. Set the ",(0,n.yg)("inlineCode",{parentName:"p"},"AIRFLOW_VAR_DATAHUB_AIRFLOW_PLUGIN_DISABLE_LISTENER")," environment variable to ",(0,n.yg)("inlineCode",{parentName:"p"},"true"),". This ensures that the listener won't process anything."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"export AIRFLOW_VAR_DATAHUB_AIRFLOW_PLUGIN_DISABLE_LISTENER=true\n")),(0,n.yg)("p",null,"This will immediately disable the plugin without requiring a restart."),(0,n.yg)("admonition",{title:"Why Environment Variable Instead of Airflow Variable?",type:"note"},(0,n.yg)("p",{parentName:"admonition"},"The plugin uses environment variables instead of Airflow's ",(0,n.yg)("inlineCode",{parentName:"p"},"Variable.get()")," because listener hooks are called during SQLAlchemy's ",(0,n.yg)("inlineCode",{parentName:"p"},"after_flush")," event (before the main transaction commits). Calling ",(0,n.yg)("inlineCode",{parentName:"p"},"Variable.get()")," in this context creates a nested database session that can interfere with the outer transaction and cause data loss, such as missing TaskInstanceHistory records for retried tasks.")),(0,n.yg)("h2",{id:"compatibility"},"Compatibility"),(0,n.yg)("p",null,"We try to support Airflow releases for ~2 years after their release. This is a best-effort guarantee - it's not always possible due to dependency / security issues cropping up in older versions."),(0,n.yg)("p",null,"We no longer officially support Airflow <2.7. However, you can use older versions of ",(0,n.yg)("inlineCode",{parentName:"p"},"acryl-datahub-airflow-plugin")," with older versions of Airflow.\nWe previously had two implementations of the plugin - v1 and v2. The v2 plugin is now the default, and the v1 plugin has since been removed. The v1 plugin had many limitations, chiefly that it does not support automatic lineage extraction. Docs for the v1 plugin can be accessed in our ",(0,n.yg)("a",{parentName:"p",href:"https://docs-website-r5eolot5n-acryldata.vercel.app/docs/lineage/airflow#datahub-plugin-v1"},"docs archive"),"."),(0,n.yg)("p",null,"All recent versions require Python 3.10+."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Airflow 1.10.x, use acryl-datahub-airflow-plugin <= 0.9.1.0 (v1 plugin)."),(0,n.yg)("li",{parentName:"ul"},"Airflow 2.0.x, use acryl-datahub-airflow-plugin <= 0.11.0.1 (v1 plugin)."),(0,n.yg)("li",{parentName:"ul"},"Airflow 2.2.x, use acryl-datahub-airflow-plugin <= 0.14.1.5 (v2 plugin)."),(0,n.yg)("li",{parentName:"ul"},"Airflow 2.3 - 2.4.3, use acryl-datahub-airflow-plugin <= 1.0.0 (v2 plugin)."),(0,n.yg)("li",{parentName:"ul"},"Airflow 2.5 and 2.6, use acryl-datahub-airflow-plugin <= 1.1.0.4 (v2 plugin).")),(0,n.yg)("p",null,"DataHub also previously supported an Airflow ",(0,n.yg)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/2.2.0/lineage.html#lineage-backend"},"lineage backend")," implementation. The lineage backend functionality was pretty limited - it did not support automatic lineage extraction, did not capture task failures, and did not work in AWS MWAA - and so it has been removed from the codebase. The ",(0,n.yg)("a",{parentName:"p",href:"https://docs-website-1wmaehubl-acryldata.vercel.app/docs/lineage/airflow/#using-datahubs-airflow-lineage-backend-deprecated"},"documentation for the lineage backend")," has been archived."),(0,n.yg)("h2",{id:"additional-references"},"Additional references"),(0,n.yg)("p",null,"Related Datahub videos:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://www.youtube.com/watch?v=3wiaqhb8UR0"},"Airflow Lineage")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://www.youtube.com/watch?v=YpUOqDU5ZYg"},"Airflow Run History in DataHub"))))}c.isMDXComponent=!0}}]);