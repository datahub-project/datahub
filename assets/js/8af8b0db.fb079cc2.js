"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[24204],{15680:(e,a,n)=>{n.d(a,{xA:()=>g,yg:()=>y});var t=n(96540);function r(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function i(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),n.push.apply(n,t)}return n}function l(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?i(Object(n),!0).forEach((function(a){r(e,a,n[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))}))}return e}function o(e,a){if(null==e)return{};var n,t,r=function(e,a){if(null==e)return{};var n,t,r={},i=Object.keys(e);for(t=0;t<i.length;t++)n=i[t],a.indexOf(n)>=0||(r[n]=e[n]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)n=i[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=t.createContext({}),p=function(e){var a=t.useContext(s),n=a;return e&&(n="function"==typeof e?e(a):l(l({},a),e)),n},g=function(e){var a=p(e.components);return t.createElement(s.Provider,{value:a},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},m=t.forwardRef((function(e,a){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,g=o(e,["components","mdxType","originalType","parentName"]),u=p(n),m=r,y=u["".concat(s,".").concat(m)]||u[m]||d[m]||i;return n?t.createElement(y,l(l({ref:a},g),{},{components:n})):t.createElement(y,l({ref:a},g))}));function y(e,a){var n=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var i=n.length,l=new Array(i);l[0]=m;var o={};for(var s in a)hasOwnProperty.call(a,s)&&(o[s]=a[s]);o.originalType=e,o[u]="string"==typeof e?e:r,l[1]=o;for(var p=2;p<i;p++)l[p]=n[p];return t.createElement.apply(null,l)}return t.createElement.apply(null,n)}m.displayName="MDXCreateElement"},97009:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>g,contentTitle:()=>s,default:()=>y,frontMatter:()=>o,metadata:()=>p,toc:()=>u});n(96540);var t=n(15680);function r(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function i(e,a){return a=null!=a?a:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):function(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),n.push.apply(n,t)}return n}(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})),e}function l(e,a){if(null==e)return{};var n,t,r=function(e,a){if(null==e)return{};var n,t,r={},i=Object.keys(e);for(t=0;t<i.length;t++)n=i[t],a.indexOf(n)>=0||(r[n]=e[n]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)n=i[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}const o={title:"Airflow 3.x Migration Guide",slug:"/metadata-ingestion-modules/airflow-plugin/airflow_3_migration",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion-modules/airflow-plugin/AIRFLOW_3_MIGRATION.md"},s="Airflow 3.x Migration Guide",p={unversionedId:"metadata-ingestion-modules/airflow-plugin/AIRFLOW_3_MIGRATION",id:"metadata-ingestion-modules/airflow-plugin/AIRFLOW_3_MIGRATION",title:"Airflow 3.x Migration Guide",description:"This document outlines the changes made to support Apache Airflow 3.x and known limitations.",source:"@site/genDocs/metadata-ingestion-modules/airflow-plugin/AIRFLOW_3_MIGRATION.md",sourceDirName:"metadata-ingestion-modules/airflow-plugin",slug:"/metadata-ingestion-modules/airflow-plugin/airflow_3_migration",permalink:"/docs/metadata-ingestion-modules/airflow-plugin/airflow_3_migration",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion-modules/airflow-plugin/AIRFLOW_3_MIGRATION.md",tags:[],version:"current",frontMatter:{title:"Airflow 3.x Migration Guide",slug:"/metadata-ingestion-modules/airflow-plugin/airflow_3_migration",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion-modules/airflow-plugin/AIRFLOW_3_MIGRATION.md"}},g={},u=[{value:"Summary of Major Changes",id:"summary-of-major-changes",level:2},{value:"\ud83c\udfd7\ufe0f Plugin Architecture: Separate Version-Specific Implementations",id:"\ufe0f-plugin-architecture-separate-version-specific-implementations",level:3},{value:"\ud83c\udfaf Key Architectural Change: Moved Away from Operator-Specific Overrides",id:"-key-architectural-change-moved-away-from-operator-specific-overrides",level:3},{value:"TaskInstance Attribute Changes",id:"taskinstance-attribute-changes",level:3},{value:"Other Major Changes",id:"other-major-changes",level:3},{value:"Compatibility Status",id:"compatibility-status",level:3},{value:"Detailed Changes",id:"detailed-changes",level:2},{value:"1. Import Path Updates",id:"1-import-path-updates",level:3},{value:"BaseOperator",id:"baseoperator",level:4},{value:"Operator Type Alias",id:"operator-type-alias",level:4},{value:"EmptyOperator",id:"emptyoperator",level:4},{value:"ExternalTaskSensor",id:"externaltasksensor",level:4},{value:"1b. OpenLineage Provider Changes",id:"1b-openlineage-provider-changes",level:3},{value:"Import Changes",id:"import-changes",level:4},{value:"API Differences",id:"api-differences",level:4},{value:"Compatibility Layer",id:"compatibility-layer",level:4},{value:"2. DAG Parameter Changes",id:"2-dag-parameter-changes",level:3},{value:"2a. Schedule Parameter",id:"2a-schedule-parameter",level:4},{value:"2b. Default View Parameter",id:"2b-default-view-parameter",level:4},{value:"3. API Changes",id:"3-api-changes",level:3},{value:"REST API Version",id:"rest-api-version",level:4},{value:"API Authentication",id:"api-authentication",level:4},{value:"Configuration",id:"configuration",level:4},{value:"4. CLI Command Changes",id:"4-cli-command-changes",level:3},{value:"DAG Trigger",id:"dag-trigger",level:4},{value:"5. Listener Hook Signature Changes",id:"5-listener-hook-signature-changes",level:3},{value:"Task Instance Hooks",id:"task-instance-hooks",level:4},{value:"6. SubDAG Removal",id:"6-subdag-removal",level:3},{value:"Affected Attributes",id:"affected-attributes",level:4},{value:"Compatibility Fix",id:"compatibility-fix",level:4},{value:"7. Database Commit Restrictions in Listener Hooks",id:"7-database-commit-restrictions-in-listener-hooks",level:3},{value:"Problem",id:"problem",level:4},{value:"Solution",id:"solution",level:4},{value:"8. Threading Support in Airflow 3.x",id:"8-threading-support-in-airflow-3x",level:3},{value:"Status: \u2705 Fully Working",id:"status--fully-working",level:4},{value:"Why Threading Works",id:"why-threading-works",level:4},{value:"Configuration",id:"configuration-1",level:4},{value:"9. Template Rendering",id:"9-template-rendering",level:3},{value:"Problem",id:"problem-1",level:4},{value:"Solution",id:"solution-1",level:4},{value:"10. SQL Parser Integration for Airflow 3.x",id:"10-sql-parser-integration-for-airflow-3x",level:3},{value:"Architecture Change",id:"architecture-change",level:4},{value:"Implementation",id:"implementation",level:4},{value:"11. Emitter Initialization: SDK Connection API Instead of BaseHook",id:"11-emitter-initialization-sdk-connection-api-instead-of-basehook",level:3},{value:"Problem: SUPERVISOR_COMMS Limitation",id:"problem-supervisor_comms-limitation",level:4},{value:"Solution: Use SDK Connection API",id:"solution-use-sdk-connection-api",level:4},{value:"12. Operator-Specific Patches for OpenLineage",id:"12-operator-specific-patches-for-openlineage",level:3},{value:"12a. SQLite Operator Patch",id:"12a-sqlite-operator-patch",level:4},{value:"12b. Athena Operator Patch",id:"12b-athena-operator-patch",level:4},{value:"12c. BigQuery InsertJobOperator Patch",id:"12c-bigquery-insertjoboperator-patch",level:4},{value:"Patch Registration",id:"patch-registration",level:4},{value:"13. Column-Level Lineage in Airflow 3.x",id:"13-column-level-lineage-in-airflow-3x",level:3},{value:"Known Limitations",id:"known-limitations",level:2},{value:"1. SubDAG Lineage Not Supported in Airflow 3.x",id:"1-subdag-lineage-not-supported-in-airflow-3x",level:3},{value:"2. Configuration Migration Required",id:"2-configuration-migration-required",level:3},{value:"3. Test Compatibility Matrix",id:"3-test-compatibility-matrix",level:3},{value:"Testing",id:"testing",level:2},{value:"Running Tests Against Airflow 3.x",id:"running-tests-against-airflow-3x",level:3},{value:"Verifying Compatibility",id:"verifying-compatibility",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Import Errors",id:"import-errors",level:3},{value:"Authentication Failures",id:"authentication-failures",level:3},{value:"SubDAG Warnings",id:"subdag-warnings",level:3},{value:"Schedule Parameter Issues",id:"schedule-parameter-issues",level:3},{value:"Default View Parameter Issues",id:"default-view-parameter-issues",level:3},{value:"Migration Checklist",id:"migration-checklist",level:2},{value:"References",id:"references",level:2},{value:"Support",id:"support",level:2},{value:"Version History",id:"version-history",level:2}],d={toc:u},m="wrapper";function y(e){var{components:a}=e,n=l(e,["components"]);return(0,t.yg)(m,i(function(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{},t=Object.keys(n);"function"==typeof Object.getOwnPropertySymbols&&(t=t.concat(Object.getOwnPropertySymbols(n).filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable})))),t.forEach((function(a){r(e,a,n[a])}))}return e}({},d,n),{components:a,mdxType:"MDXLayout"}),(0,t.yg)("h1",{id:"airflow-3x-migration-guide"},"Airflow 3.x Migration Guide"),(0,t.yg)("p",null,"This document outlines the changes made to support Apache Airflow 3.x and known limitations."),(0,t.yg)("h2",{id:"summary-of-major-changes"},"Summary of Major Changes"),(0,t.yg)("p",null,"Apache Airflow 3.0 introduced significant breaking changes. The DataHub Airflow plugin has been fully updated to support both Airflow 2.4+ and 3.x with backward compatibility."),(0,t.yg)("h3",{id:"\ufe0f-plugin-architecture-separate-version-specific-implementations"},"\ud83c\udfd7\ufe0f Plugin Architecture: Separate Version-Specific Implementations"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"The plugin now uses separate implementations for Airflow 2.x and 3.x")," to achieve clean type safety and maintainability:"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Component"),(0,t.yg)("th",{parentName:"tr",align:null},"Airflow 2.x Implementation"),(0,t.yg)("th",{parentName:"tr",align:null},"Airflow 3.x Implementation"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Main Module")),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"airflow2/datahub_listener.py")),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"airflow3/datahub_listener.py"))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Shims/Imports")),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"airflow2/_shims.py")),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"airflow3/_shims.py"))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Lineage Extraction")),(0,t.yg)("td",{parentName:"tr",align:null},"Extractor-based (",(0,t.yg)("inlineCode",{parentName:"td"},"_extractors.py"),")"),(0,t.yg)("td",{parentName:"tr",align:null},"OpenLineage native (",(0,t.yg)("inlineCode",{parentName:"td"},"_airflow3_sql_parser_patch.py"),")")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"OpenLineage Package")),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"openlineage-airflow>=1.2.0")),(0,t.yg)("td",{parentName:"tr",align:null},"Native provider (",(0,t.yg)("inlineCode",{parentName:"td"},"apache-airflow-providers-openlineage"),")")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Type Checking")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Clean Airflow 2.x types"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Clean Airflow 3.x types")))),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Version Dispatcher:")," The main ",(0,t.yg)("inlineCode",{parentName:"p"},"datahub_listener.py")," automatically imports the correct implementation at runtime:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"from datahub_airflow_plugin._airflow_version_specific import IS_AIRFLOW_3_OR_HIGHER\n\nif IS_AIRFLOW_3_OR_HIGHER:\n    from datahub_airflow_plugin.airflow3.datahub_listener import (\n        DataHubListener,\n        get_airflow_plugin_listener,\n    )\nelse:\n    from datahub_airflow_plugin.airflow2.datahub_listener import (\n        DataHubListener,\n        get_airflow_plugin_listener,\n    )\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Benefits:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Type Safety")," - Each version can be properly type-checked against its respective Airflow API without conflicts"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Maintainability")," - No complex version conditionals scattered throughout the code"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Clarity")," - Clear separation of version-specific logic"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Testing")," - Each version can be tested independently with its specific type requirements")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Package Installation:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# For Airflow 2.x (uses standalone openlineage-airflow package)\npip install 'acryl-datahub-airflow-plugin[airflow2]'\n\n# For Airflow 3.x (uses native OpenLineage provider)\npip install 'acryl-datahub-airflow-plugin[airflow3]'\n\n# For compatibility across versions (installs base package with native provider)\npip install 'acryl-datahub-airflow-plugin'  # Works with both Airflow 2.7+ and 3.x\n")),(0,t.yg)("h3",{id:"-key-architectural-change-moved-away-from-operator-specific-overrides"},"\ud83c\udfaf Key Architectural Change: Moved Away from Operator-Specific Overrides"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"The most significant change")," is how lineage extraction works:"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Aspect"),(0,t.yg)("th",{parentName:"tr",align:null},"Airflow 2.x"),(0,t.yg)("th",{parentName:"tr",align:null},"Airflow 3.x"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Lineage Extraction")),(0,t.yg)("td",{parentName:"tr",align:null},"Operator-specific extractors"),(0,t.yg)("td",{parentName:"tr",align:null},"Unified SQLParser patch")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Customization Point")),(0,t.yg)("td",{parentName:"tr",align:null},"Custom extractor per operator"),(0,t.yg)("td",{parentName:"tr",align:null},"Single SQL parser integration")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Column Lineage")),(0,t.yg)("td",{parentName:"tr",align:null},"Extractor-dependent"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Consistent across all SQL operators")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Maintenance")),(0,t.yg)("td",{parentName:"tr",align:null},"Multiple extractors to maintain"),(0,t.yg)("td",{parentName:"tr",align:null},"Single integration point")))),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"In Airflow 2.x"),", we used operator-specific extractors:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"# Different extractor for each SQL operator\nSnowflakeExtractor, PostgresExtractor, MySQLExtractor, etc.\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"In Airflow 3.x"),", we use a ",(0,t.yg)("strong",{parentName:"p"},"unified SQLParser patch"),":"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"# Single patch point for ALL SQL operators\nSQLParser.generate_openlineage_metadata_from_sql = datahub_enhanced_version\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Benefits:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Better consistency")," - All SQL operators use the same lineage extraction logic"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Easier maintenance")," - One integration point instead of many extractors"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Column-level lineage for all")," - Works across all SQL operators uniformly"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Future-proof")," - New SQL operators automatically get DataHub lineage")),(0,t.yg)("h3",{id:"taskinstance-attribute-changes"},"TaskInstance Attribute Changes"),(0,t.yg)("p",null,"Airflow 3.0 introduced ",(0,t.yg)("inlineCode",{parentName:"p"},"RuntimeTaskInstance")," which has a different structure than Airflow 2.x's ",(0,t.yg)("inlineCode",{parentName:"p"},"TaskInstance"),":"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Attribute"),(0,t.yg)("th",{parentName:"tr",align:null},"Airflow 2.x"),(0,t.yg)("th",{parentName:"tr",align:null},"Airflow 3.0 RuntimeTaskInstance"),(0,t.yg)("th",{parentName:"tr",align:null},"Status"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"run_id")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Database field"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Base class"),(0,t.yg)("td",{parentName:"tr",align:null},"Available in both")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"start_date")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Database field"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 RuntimeTI field"),(0,t.yg)("td",{parentName:"tr",align:null},"Available in both")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"try_number")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Database field"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Base class"),(0,t.yg)("td",{parentName:"tr",align:null},"Available in both")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"state")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Database field"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 RuntimeTI field"),(0,t.yg)("td",{parentName:"tr",align:null},"Available in both")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"task_id")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Database field"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Base class"),(0,t.yg)("td",{parentName:"tr",align:null},"Available in both")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"dag_id")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Database field"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Base class"),(0,t.yg)("td",{parentName:"tr",align:null},"Available in both")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"max_tries")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Database field"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 RuntimeTI field"),(0,t.yg)("td",{parentName:"tr",align:null},"Available in both")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"end_date")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Database field"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 RuntimeTI field"),(0,t.yg)("td",{parentName:"tr",align:null},"Available in both")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"log_url")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Property"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 RuntimeTI field"),(0,t.yg)("td",{parentName:"tr",align:null},"Available in both")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"execution_date")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Database field"),(0,t.yg)("td",{parentName:"tr",align:null},"\u274c Renamed"),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Renamed to ",(0,t.yg)("inlineCode",{parentName:"strong"},"logical_date")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"duration")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Database field"),(0,t.yg)("td",{parentName:"tr",align:null},"\u274c Not available"),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Missing - must be calculated"))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"external_executor_id")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Database field"),(0,t.yg)("td",{parentName:"tr",align:null},"\u274c Not available"),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Missing in Airflow 3.0"))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"operator")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Database field (string)"),(0,t.yg)("td",{parentName:"tr",align:null},"\u26a0\ufe0f Different"),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Has ",(0,t.yg)("inlineCode",{parentName:"strong"},"task")," (BaseOperator) instead"))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"priority_weight")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Database field"),(0,t.yg)("td",{parentName:"tr",align:null},"\u274c Not available"),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Missing in Airflow 3.0"))))),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Key Changes:")),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"RuntimeTaskInstance is not database-backed")," - It's a Pydantic model created at runtime"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Minimal base attributes")," - Base ",(0,t.yg)("inlineCode",{parentName:"li"},"TaskInstance")," only has: ",(0,t.yg)("inlineCode",{parentName:"li"},"id"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"task_id"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"dag_id"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"run_id"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"try_number"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"map_index"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"hostname")),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"execution_date \u2192 logical_date")," - The familiar ",(0,t.yg)("inlineCode",{parentName:"li"},"execution_date")," was renamed"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Missing fields")," - Several metadata fields are not available in the runtime context"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"task vs operator")," - Airflow 3.0 provides direct access to the task object, not just its string name")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Plugin Compatibility:")),(0,t.yg)("p",null,"The plugin uses ",(0,t.yg)("inlineCode",{parentName:"p"},"hasattr()")," checks to gracefully handle missing attributes:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'def get_task_instance_attributes(ti: "TaskInstance") -> Dict[str, str]:\n    attributes = {}\n\n    # Safe attribute access - works in both versions\n    if hasattr(ti, "run_id"):\n        attributes["run_id"] = str(ti.run_id)\n\n    # Handle renamed attribute\n    if hasattr(ti, "execution_date"):\n        attributes["execution_date"] = str(ti.execution_date)\n    elif hasattr(ti, "logical_date"):\n        attributes["logical_date"] = str(ti.logical_date)\n\n    # Handle missing attributes gracefully\n    if hasattr(ti, "duration"):\n        attributes["duration"] = str(ti.duration)\n\n    return attributes\n')),(0,t.yg)("p",null,"This approach ensures the plugin works correctly with both Airflow 2.x and 3.x task instances."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files Updated:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/_airflow_version_specific.py:21-74")," - Version-compatible attribute extraction")),(0,t.yg)("h3",{id:"other-major-changes"},"Other Major Changes"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Import Path Updates")," - Airflow 3.x reorganized modules under new SDK structure"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"API Changes")," - JWT authentication instead of Basic Auth; v1 API removed"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"DAG Parameters")," - ",(0,t.yg)("inlineCode",{parentName:"li"},"schedule_interval")," \u2192 ",(0,t.yg)("inlineCode",{parentName:"li"},"schedule"),"; ",(0,t.yg)("inlineCode",{parentName:"li"},"default_view")," removed"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Listener Hook Signatures")," - Session parameter removed; error parameter added"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Database Access Restrictions")," - No Variable.get() in hooks (use env vars instead)"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Template Rendering")," - Skip deepcopy for RuntimeTaskInstance (already rendered)"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"SubDAG Removal")," - SubDAGs completely removed (use TaskGroups)"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Log URL Format")," - Simplified URL structure and different config key"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Threading Support")," - \u2705 Works in Airflow 3.x (contrary to initial concerns)")),(0,t.yg)("h3",{id:"compatibility-status"},"Compatibility Status"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Feature"),(0,t.yg)("th",{parentName:"tr",align:null},"Airflow 2.x"),(0,t.yg)("th",{parentName:"tr",align:null},"Airflow 3.x"),(0,t.yg)("th",{parentName:"tr",align:null},"Status"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Task lineage"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Fully working")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Column lineage"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Fully working")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"DAG metadata"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Fully working")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Execution tracking"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Fully working")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Threading"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Fully working")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"SubDAG support"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"\u274c"),(0,t.yg)("td",{parentName:"tr",align:null},"Removed in Airflow 3.x")))),(0,t.yg)("h2",{id:"detailed-changes"},"Detailed Changes"),(0,t.yg)("p",null,"This section provides in-depth information about each change made for Airflow 3.x compatibility."),(0,t.yg)("h3",{id:"1-import-path-updates"},"1. Import Path Updates"),(0,t.yg)("p",null,"Airflow 3.x reorganized many modules under a new SDK structure. The plugin now uses conditional imports with fallbacks."),(0,t.yg)("h4",{id:"baseoperator"},"BaseOperator"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"# Airflow 3.x (preferred)\nfrom airflow.sdk.bases.operator import BaseOperator\n\n# Airflow 2.x (fallback)\nfrom airflow.models.baseoperator import BaseOperator\n")),(0,t.yg)("h4",{id:"operator-type-alias"},"Operator Type Alias"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"# Airflow 3.x (preferred)\nfrom airflow.sdk.types import Operator\n\n# Airflow 2.x (fallback)\nfrom airflow.models.operator import Operator\n")),(0,t.yg)("h4",{id:"emptyoperator"},"EmptyOperator"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"# Airflow 3.x (preferred)\nfrom airflow.providers.standard.operators.empty import EmptyOperator\n\n# Airflow 2.x (fallback)\nfrom airflow.operators.empty import EmptyOperator\n\n# Airflow <2.2 (fallback)\nfrom airflow.operators.dummy import DummyOperator\n")),(0,t.yg)("h4",{id:"externaltasksensor"},"ExternalTaskSensor"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"# Airflow 3.x (preferred)\nfrom airflow.providers.standard.sensors.external_task import ExternalTaskSensor\n\n# Airflow 2.x (fallback)\nfrom airflow.sensors.external_task import ExternalTaskSensor\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files Updated:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/airflow2/_shims.py")," - Clean Airflow 2.x imports"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/airflow3/_shims.py")," - Clean Airflow 3.x imports"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/_airflow_shims.py")," - Pure dispatcher (no logic, just routes to version-specific shims)"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/client/airflow_generator.py")," - Import from shims")),(0,t.yg)("h3",{id:"1b-openlineage-provider-changes"},"1b. OpenLineage Provider Changes"),(0,t.yg)("p",null,"Airflow 2.7+ introduced native OpenLineage support, and Airflow 3.x completely removed support for the old ",(0,t.yg)("inlineCode",{parentName:"p"},"openlineage-airflow")," package. The native provider has a different API and doesn't include SQL extractors."),(0,t.yg)("h4",{id:"import-changes"},"Import Changes"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"# Airflow 2.7+ and 3.x (native provider)\nfrom airflow.providers.openlineage.extractors import OperatorLineage as TaskMetadata\nfrom airflow.providers.openlineage.extractors.base import BaseExtractor\nfrom airflow.providers.openlineage.extractors.manager import ExtractorManager\n\n# Airflow < 2.7 (old openlineage-airflow package)\nfrom openlineage.airflow.extractors import TaskMetadata, BaseExtractor, ExtractorManager\nfrom openlineage.airflow.extractors.sql_extractor import SqlExtractor\n")),(0,t.yg)("h4",{id:"api-differences"},"API Differences"),(0,t.yg)("p",null,"The native provider's ",(0,t.yg)("inlineCode",{parentName:"p"},"ExtractorManager")," has a different API:"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Feature"),(0,t.yg)("th",{parentName:"tr",align:null},"Old Package (< 2.7)"),(0,t.yg)("th",{parentName:"tr",align:null},"Native Provider (2.7+)"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Extractor registry"),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"self.task_to_extractor.extractors")),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"self.extractors"))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Add extractor"),(0,t.yg)("td",{parentName:"tr",align:null},"Direct dict assignment"),(0,t.yg)("td",{parentName:"tr",align:null},"Direct dict assignment or ",(0,t.yg)("inlineCode",{parentName:"td"},"add_extractor()"))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"SQL extractor"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Included"),(0,t.yg)("td",{parentName:"tr",align:null},"\u274c Not included")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Task metadata type"),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"TaskMetadata")),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"OperatorLineage")," (aliased as ",(0,t.yg)("inlineCode",{parentName:"td"},"TaskMetadata"),")")))),(0,t.yg)("h4",{id:"compatibility-layer"},"Compatibility Layer"),(0,t.yg)("p",null,"The plugin implements a compatibility layer that:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Detects which OpenLineage implementation is available"),(0,t.yg)("li",{parentName:"ol"},"Uses the appropriate API for registering extractors"),(0,t.yg)("li",{parentName:"ol"},"Implements custom SQL extraction for Airflow 2.7+ (since native provider doesn't include ",(0,t.yg)("inlineCode",{parentName:"li"},"SqlExtractor"),")")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"# Compatibility check in ExtractorManager.__init__\nif hasattr(self, 'task_to_extractor'):\n    # Old openlineage-airflow (Airflow < 2.7)\n    extractors_dict = self.task_to_extractor.extractors\nelse:\n    # Native provider (Airflow 2.7+)\n    extractors_dict = self.extractors\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Impact:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"The plugin automatically selects the correct OpenLineage implementation based on Airflow version"),(0,t.yg)("li",{parentName:"ul"},"SQL lineage extraction works seamlessly across all supported Airflow versions"),(0,t.yg)("li",{parentName:"ul"},"No user action required - the plugin handles version differences internally")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files Updated:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/_extractors.py")," - Conditional OpenLineage imports and API compatibility layer")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Known Limitations:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"The native provider (Airflow 2.7+) doesn't include SQL extractors, so the plugin provides its own implementation"),(0,t.yg)("li",{parentName:"ul"},"Some advanced OpenLineage features from the old package may not be available in the native provider")),(0,t.yg)("h3",{id:"2-dag-parameter-changes"},"2. DAG Parameter Changes"),(0,t.yg)("h4",{id:"2a-schedule-parameter"},"2a. Schedule Parameter"),(0,t.yg)("p",null,"Airflow 3.x removed the ",(0,t.yg)("inlineCode",{parentName:"p"},"schedule_interval")," parameter in favor of ",(0,t.yg)("inlineCode",{parentName:"p"},"schedule"),"."),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'# Airflow 3.x (required)\nDAG("my_dag", schedule=None, ...)\n\n# Airflow 2.4+ (deprecated but supported)\nDAG("my_dag", schedule_interval=None, ...)\n\n# Airflow <2.4 (only option)\nDAG("my_dag", schedule_interval=None, ...)\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Note:")," The ",(0,t.yg)("inlineCode",{parentName:"p"},"schedule")," parameter was introduced in Airflow 2.4.0, so test DAGs use ",(0,t.yg)("inlineCode",{parentName:"p"},"schedule=")," which works in both Airflow 2.4+ and 3.x."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files Updated:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"All test DAG files in ",(0,t.yg)("inlineCode",{parentName:"li"},"tests/integration/dags/*.py"))),(0,t.yg)("h4",{id:"2b-default-view-parameter"},"2b. Default View Parameter"),(0,t.yg)("p",null,"Airflow 3.x removed the ",(0,t.yg)("inlineCode",{parentName:"p"},"default_view")," parameter from the DAG constructor."),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'# Airflow 2.x (supported)\nDAG("my_dag", default_view="tree", ...)  # Set default UI view\n\n# Airflow 3.x (removed)\nDAG("my_dag", ...)  # default_view parameter no longer accepted\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Reason for Removal:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"User preferences are now persistent")," - The Airflow UI remembers each user's preferred view per DAG"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Separation of concerns")," - DAG definition (pipeline logic) should be separate from UI presentation"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Cleaner API")," - Removes UI-specific parameters from the core DAG class")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Valid ",(0,t.yg)("inlineCode",{parentName:"strong"},"default_view")," values in Airflow 2.x:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},'"tree"')," - Tree view (hierarchical task structure)"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},'"graph"')," - Graph view (visual DAG graph)"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},'"duration"')," - Duration view"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},'"gantt"')," - Gantt chart view"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},'"landing_times"')," - Landing times view")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Migration:")," Simply remove the ",(0,t.yg)("inlineCode",{parentName:"p"},"default_view")," parameter from DAG definitions when upgrading to Airflow 3.x."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files Updated:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"tests/integration/dags/airflow3/datahub_emitter_operator_jinja_template_dag.py")," - Removed ",(0,t.yg)("inlineCode",{parentName:"li"},'default_view="tree"'))),(0,t.yg)("h3",{id:"3-api-changes"},"3. API Changes"),(0,t.yg)("h4",{id:"rest-api-version"},"REST API Version"),(0,t.yg)("p",null,"Airflow 3.x removed the v1 API and only supports v2."),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'# Airflow 3.x\napi_version = "v2"\n\n# Airflow 2.x\napi_version = "v1"\n')),(0,t.yg)("h4",{id:"api-authentication"},"API Authentication"),(0,t.yg)("p",null,"Airflow 3.x uses JWT token-based authentication instead of HTTP Basic Auth."),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'# Airflow 3.x - Get JWT token\nresponse = requests.post(\n    f"{airflow_url}/auth/token",\n    data={"username": username, "password": password}\n)\ntoken = response.json()["access_token"]\nsession.headers["Authorization"] = f"Bearer {token}"\n\n# Airflow 2.x - HTTP Basic Auth\nsession.auth = (username, password)\n')),(0,t.yg)("h4",{id:"configuration"},"Configuration"),(0,t.yg)("p",null,"Airflow 3.x moved some configuration options:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# Airflow 3.x\nAIRFLOW__API__PORT=8080\nAIRFLOW__API__BASE_URL=http://airflow.example.com  # Used for log URLs\n\n# Airflow 2.x\nAIRFLOW__WEBSERVER__WEB_SERVER_PORT=8080\nAIRFLOW__WEBSERVER__BASE_URL=http://airflow.example.com  # Used for log URLs\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Log URL Format Changes:")),(0,t.yg)("p",null,"Airflow 3.x changed the log URL format and configuration:"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Aspect"),(0,t.yg)("th",{parentName:"tr",align:null},"Airflow 2.x"),(0,t.yg)("th",{parentName:"tr",align:null},"Airflow 3.x"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Config key"),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"webserver.base_url")),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"api.base_url"))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"URL format"),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"http://host/dags/{dag_id}/grid?dag_run_id={run_id}&task_id={task_id}&base_date={date}&tab=logs")),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"http://host/dags/{dag_id}/runs/{run_id}/tasks/{task_id}?try_number={try_number}"))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Source"),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"TaskInstance.log_url")," property"),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"TaskInstance.log_url")," property")))),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Example Log URLs:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'# Airflow 2.x\n"http://airflow.example.com/dags/my_dag/grid?dag_run_id=manual_run&task_id=my_task&base_date=2023-09-27T21%3A34%3A38%2B0000&tab=logs"\n\n# Airflow 3.x\n"http://airflow.example.com/dags/my_dag/runs/manual_run/tasks/my_task?try_number=1"\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Impact on DataHub:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"The ",(0,t.yg)("inlineCode",{parentName:"li"},"log_url")," in DataHub's DataProcessInstance will reflect the Airflow version's format"),(0,t.yg)("li",{parentName:"ul"},"Both formats link correctly to the Airflow UI task logs"),(0,t.yg)("li",{parentName:"ul"},"The plugin automatically uses the correct configuration key for each version")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files Updated:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"tests/integration/test_plugin.py")," - Authentication, API version logic, and base URL configuration"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/_airflow_version_specific.py")," - Task instance attribute extraction including log_url")),(0,t.yg)("h3",{id:"4-cli-command-changes"},"4. CLI Command Changes"),(0,t.yg)("h4",{id:"dag-trigger"},"DAG Trigger"),(0,t.yg)("p",null,"Airflow 3.x renamed the ",(0,t.yg)("inlineCode",{parentName:"p"},"--exec-date")," parameter to ",(0,t.yg)("inlineCode",{parentName:"p"},"--logical-date"),"."),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},'# Airflow 3.x\nairflow dags trigger --logical-date "2023-09-27T21:34:38+00:00" my_dag\n\n# Airflow 2.x\nairflow dags trigger --exec-date "2023-09-27T21:34:38+00:00" my_dag\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files Updated:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"tests/integration/test_plugin.py")," - Conditional CLI parameter")),(0,t.yg)("h3",{id:"5-listener-hook-signature-changes"},"5. Listener Hook Signature Changes"),(0,t.yg)("p",null,"Airflow 3.x changed the signatures of listener hooks to remove the ",(0,t.yg)("inlineCode",{parentName:"p"},"session")," parameter and add new parameters."),(0,t.yg)("h4",{id:"task-instance-hooks"},"Task Instance Hooks"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Airflow 3.x signatures:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"on_task_instance_running(previous_state, task_instance)\non_task_instance_success(previous_state, task_instance)\non_task_instance_failed(previous_state, task_instance, error)\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Airflow 2.x signatures:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"on_task_instance_running(previous_state, task_instance, session)\non_task_instance_success(previous_state, task_instance, session)\non_task_instance_failed(previous_state, task_instance, session)\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Compatibility Fix:"),"\nThe plugin uses ",(0,t.yg)("inlineCode",{parentName:"p"},"**kwargs")," to handle both versions without breaking pluggy's hook matching:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'@hookimpl\ndef on_task_instance_running(self, previous_state, task_instance, **kwargs):\n    # Extract session if present (Airflow 2.x)\n    session = kwargs.get("session")\n    ...\n\n@hookimpl\ndef on_task_instance_failed(self, previous_state, task_instance, **kwargs):\n    # Extract error and session from kwargs (Airflow 3.x passes error, 2.x passes session)\n    session = kwargs.get("session")\n    error = kwargs.get("error")\n    ...\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Important:")," Using default parameters like ",(0,t.yg)("inlineCode",{parentName:"p"},"session=None")," in Airflow 3.0 causes pluggy to fail to match the hook spec, preventing hooks from being called."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files Updated:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/datahub_listener.py:559-772")," - Listener hook signatures")),(0,t.yg)("h3",{id:"6-subdag-removal"},"6. SubDAG Removal"),(0,t.yg)("p",null,"Airflow 3.x completely removed SubDAGs (deprecated since Airflow 2.0)."),(0,t.yg)("h4",{id:"affected-attributes"},"Affected Attributes"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"dag.is_subdag")," - \u274c Removed"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"dag.parent_dag")," - \u274c Removed"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"task.subdag")," - \u274c Removed"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"SubDagOperator")," - \u274c Removed")),(0,t.yg)("h4",{id:"compatibility-fix"},"Compatibility Fix"),(0,t.yg)("p",null,"The plugin uses defensive attribute access:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'# Safe for both Airflow 2.x and 3.x\nif getattr(dag, "is_subdag", False) and dag.parent_dag is not None:\n    # Handle subdag (only executes in Airflow 2.x)\n    ...\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files Updated:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/client/airflow_generator.py:76")," - SubDAG handling")),(0,t.yg)("h3",{id:"7-database-commit-restrictions-in-listener-hooks"},"7. Database Commit Restrictions in Listener Hooks"),(0,t.yg)("p",null,"Airflow listener hooks are called during SQLAlchemy's ",(0,t.yg)("inlineCode",{parentName:"p"},"after_flush")," event, before the main transaction commits. Any database operations that create new sessions and commit them can interfere with the outer transaction and cause data loss."),(0,t.yg)("h4",{id:"problem"},"Problem"),(0,t.yg)("p",null,"The kill switch feature originally used ",(0,t.yg)("inlineCode",{parentName:"p"},"Variable.get()")," to check if the plugin should be disabled:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'# This causes issues:\n# - Airflow 3.x: RuntimeError: UNEXPECTED COMMIT - THIS WILL BREAK HA LOCKS!\n# - Airflow 2.x: Can cause TaskInstanceHistory records to not be persisted\n#   (see: https://github.com/apache/airflow/pull/48780)\nif Variable.get("datahub_airflow_plugin_disable_listener", "false").lower() == "true":\n    return True\n')),(0,t.yg)("p",null,(0,t.yg)("inlineCode",{parentName:"p"},"Variable.get()")," uses the ",(0,t.yg)("inlineCode",{parentName:"p"},"@provide_session")," decorator which creates a new database session and commits it. When called from listener hooks (which execute during ",(0,t.yg)("inlineCode",{parentName:"p"},"after_flush"),", before the main transaction commits), this nested commit can corrupt the outer transaction state and cause data loss."),(0,t.yg)("h4",{id:"solution"},"Solution"),(0,t.yg)("p",null,"Both Airflow 2.x and 3.x versions now use environment variables instead of database queries:"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Both versions")," (",(0,t.yg)("inlineCode",{parentName:"p"},"airflow2/datahub_listener.py")," and ",(0,t.yg)("inlineCode",{parentName:"p"},"airflow3/datahub_listener.py"),"):"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'def check_kill_switch(self) -> bool:\n    """\n    Check kill switch using environment variable.\n\n    We use os.getenv() instead of Variable.get() because Variable.get()\n    creates a new database session and commits it. When called from listener\n    hooks (which execute during SQLAlchemy\'s after_flush event, before the\n    main transaction commits), this nested commit can corrupt the outer\n    transaction state and cause data loss.\n    """\n    if (\n        os.getenv(\n            f"AIRFLOW_VAR_{KILL_SWITCH_VARIABLE_NAME}".upper(), "false"\n        ).lower()\n        == "true"\n    ):\n        logger.debug("DataHub listener disabled by kill switch (env var)")\n        return True\n    return False\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"To disable the plugin (both Airflow 2.x and 3.x):")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"export AIRFLOW_VAR_DATAHUB_AIRFLOW_PLUGIN_DISABLE_LISTENER=true\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files Updated:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/airflow2/datahub_listener.py")," - Kill switch using env var"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/airflow3/datahub_listener.py")," - Kill switch using env var")),(0,t.yg)("h3",{id:"8-threading-support-in-airflow-3x"},"8. Threading Support in Airflow 3.x"),(0,t.yg)("h4",{id:"status--fully-working"},"Status: \u2705 Fully Working"),(0,t.yg)("p",null,"Threading is ",(0,t.yg)("strong",{parentName:"p"},"enabled by default")," in both Airflow 2.x and 3.x. Initial concerns about ",(0,t.yg)("inlineCode",{parentName:"p"},"RuntimeTaskInstance")," unpickleable objects were unfounded."),(0,t.yg)("h4",{id:"why-threading-works"},"Why Threading Works"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Key insight:")," ",(0,t.yg)("inlineCode",{parentName:"p"},"threading.Thread")," does ",(0,t.yg)("strong",{parentName:"p"},"NOT")," pickle its arguments - it passes object references directly within the same process. Only ",(0,t.yg)("inlineCode",{parentName:"p"},"multiprocessing.Process")," requires pickling."),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"# This works fine - no pickling required\nthread = threading.Thread(target=f, args=(task_instance,))\nthread.start()\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Verification:")," Tests pass successfully with ",(0,t.yg)("inlineCode",{parentName:"p"},"DATAHUB_AIRFLOW_PLUGIN_RUN_IN_THREAD=true")," in Airflow 3.0."),(0,t.yg)("h4",{id:"configuration-1"},"Configuration"),(0,t.yg)("p",null,"Threading is enabled by default for performance benefits:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'# Default: threading enabled\n_RUN_IN_THREAD = os.getenv(\n    "DATAHUB_AIRFLOW_PLUGIN_RUN_IN_THREAD", "true"\n).lower() in ("true", "1")\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"To disable threading")," (if needed):"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"export DATAHUB_AIRFLOW_PLUGIN_RUN_IN_THREAD=false\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Benefits of Threading:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Prevents slow lineage extraction from blocking task completion"),(0,t.yg)("li",{parentName:"ul"},"Non-blocking metadata emission to DataHub"),(0,t.yg)("li",{parentName:"ul"},"Better performance for complex SQL parsing and lineage extraction")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files Updated:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/datahub_listener.py:147-152")," - Threading configuration")),(0,t.yg)("h3",{id:"9-template-rendering"},"9. Template Rendering"),(0,t.yg)("p",null,"Airflow 3.x's ",(0,t.yg)("inlineCode",{parentName:"p"},"RuntimeTaskInstance")," cannot be deep-copied due to unpickleable objects."),(0,t.yg)("h4",{id:"problem-1"},"Problem"),(0,t.yg)("p",null,"The plugin previously deep-copied task instances to render Jinja templates:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"# This fails in Airflow 3.x: TypeError: cannot pickle '_thread.lock' object\ntask_instance_copy = copy.deepcopy(task_instance)\ntask_instance_copy.render_templates()\n")),(0,t.yg)("h4",{id:"solution-1"},"Solution"),(0,t.yg)("p",null,"The plugin now has separate template rendering implementations for each version:"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Airflow 2.x")," (",(0,t.yg)("inlineCode",{parentName:"p"},"airflow2/datahub_listener.py"),"):"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'def _render_templates(task_instance: "TaskInstance") -> "TaskInstance":\n    # Render templates in a copy of the task instance\n    try:\n        task_instance_copy = copy.deepcopy(task_instance)\n        task_instance_copy.render_templates()\n        return task_instance_copy\n    except Exception as e:\n        logger.info(f"Error rendering templates: {e}")\n        return task_instance\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Airflow 3.x")," (",(0,t.yg)("inlineCode",{parentName:"p"},"airflow3/datahub_listener.py"),"):"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'def _render_templates(task_instance: "TaskInstance") -> "TaskInstance":\n    """\n    Templates are already rendered in Airflow 3.x by the task execution system.\n\n    RuntimeTaskInstance contains unpickleable thread locks, so we cannot use deepcopy.\n    RuntimeTaskInstance.task contains the operator with rendered templates.\n    """\n    logger.debug(\n        "Skipping template rendering for Airflow 3.0+ (already rendered by task worker)"\n    )\n    return task_instance\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Impact:")," Jinja-templated SQL queries are correctly parsed in both Airflow 2.x and 3.x."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files Updated:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/airflow2/datahub_listener.py")," - Template rendering for Airflow 2.x"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/airflow3/datahub_listener.py")," - Template rendering for Airflow 3.x")),(0,t.yg)("h3",{id:"10-sql-parser-integration-for-airflow-3x"},"10. SQL Parser Integration for Airflow 3.x"),(0,t.yg)("p",null,"Airflow 3.x removed the extractor-based SQL parsing mechanism. SQL operators now call ",(0,t.yg)("inlineCode",{parentName:"p"},"SQLParser.generate_openlineage_metadata_from_sql()")," directly."),(0,t.yg)("h4",{id:"architecture-change"},"Architecture Change"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Airflow 2.x:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre"},"SQL Operator \u2192 OpenLineage Extractor \u2192 DataHub Extractor \u2192 DataHub SQL Parser \u2192 Lineage\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Airflow 3.x:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre"},"SQL Operator \u2192 SQLParser.generate_openlineage_metadata_from_sql() \u2192 [Patched by DataHub] \u2192 DataHub SQL Parser \u2192 Lineage\n")),(0,t.yg)("h4",{id:"implementation"},"Implementation"),(0,t.yg)("p",null,"The plugin patches ",(0,t.yg)("inlineCode",{parentName:"p"},"SQLParser.generate_openlineage_metadata_from_sql()")," to use DataHub's SQL parser:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"def patch_sqlparser():\n    from airflow.providers.openlineage.sqlparser import SQLParser\n\n    # Store original method for fallback\n    SQLParser._original_generate_openlineage_metadata_from_sql = (\n        SQLParser.generate_openlineage_metadata_from_sql\n    )\n\n    # Replace with DataHub-enhanced version\n    SQLParser.generate_openlineage_metadata_from_sql = (\n        _datahub_generate_openlineage_metadata_from_sql\n    )\n")),(0,t.yg)("p",null,"The patched method:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Calls DataHub's SQL parser to extract column-level lineage"),(0,t.yg)("li",{parentName:"ol"},"Converts DataHub URNs to OpenLineage Dataset objects"),(0,t.yg)("li",{parentName:"ol"},"Stores the SQL parsing result in ",(0,t.yg)("inlineCode",{parentName:"li"},"run_facets")," for later retrieval"),(0,t.yg)("li",{parentName:"ol"},"Falls back to the original implementation on errors")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Column-Level Lineage:")," The SQL parsing result is stored in the ",(0,t.yg)("inlineCode",{parentName:"p"},"OperatorLineage.run_facets")," dictionary:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'DATAHUB_SQL_PARSING_RESULT_KEY = "datahub_sql_parsing_result"\nrun_facets = {\n    DATAHUB_SQL_PARSING_RESULT_KEY: sql_parsing_result\n}\n\noperator_lineage = OperatorLineage(\n    inputs=inputs,\n    outputs=outputs,\n    job_facets={"sql": SqlJobFacet(query=sql)},\n    run_facets=run_facets,\n)\n')),(0,t.yg)("p",null,"The listener retrieves it and processes column lineage:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"if DATAHUB_SQL_PARSING_RESULT_KEY in operator_lineage.run_facets:\n    sql_parsing_result = operator_lineage.run_facets[DATAHUB_SQL_PARSING_RESULT_KEY]\n\n    # Process column lineage\n    if sql_parsing_result.column_lineage:\n        fine_grained_lineages.extend(\n            FineGrainedLineageClass(\n                upstreamType=FineGrainedLineageUpstreamTypeClass.FIELD_SET,\n                downstreamType=FineGrainedLineageDownstreamTypeClass.FIELD,\n                upstreams=[...],\n                downstreams=[...],\n            )\n            for column_lineage in sql_parsing_result.column_lineage\n        )\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Important:")," ",(0,t.yg)("inlineCode",{parentName:"p"},"OperatorLineage")," uses ",(0,t.yg)("inlineCode",{parentName:"p"},"@define")," (attrs library) which creates a frozen dataclass. We cannot add arbitrary attributes to it, so we use the ",(0,t.yg)("inlineCode",{parentName:"p"},"run_facets")," dictionary instead."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Supported Databases:")," All databases supported by DataHub's SQL parser:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Snowflake, BigQuery, Redshift, PostgreSQL, MySQL, Oracle, SQL Server, Athena, Presto, Trino, etc.")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files Updated:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/_airflow3_sql_parser_patch.py")," - SQLParser patch implementation"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/datahub_listener.py:433-439")," - Retrieve sql_parsing_result from run_facets"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/_config.py")," - Enable SQL parser patch for Airflow 3.x")),(0,t.yg)("h3",{id:"11-emitter-initialization-sdk-connection-api-instead-of-basehook"},"11. Emitter Initialization: SDK Connection API Instead of BaseHook"),(0,t.yg)("p",null,"In Airflow 3.x, the DataHub plugin uses the SDK's ",(0,t.yg)("inlineCode",{parentName:"p"},"Connection.get()")," method to initialize the emitter instead of relying on ",(0,t.yg)("inlineCode",{parentName:"p"},"BaseHook.get_connection()"),"."),(0,t.yg)("h4",{id:"problem-supervisor_comms-limitation"},"Problem: SUPERVISOR_COMMS Limitation"),(0,t.yg)("p",null,"The ",(0,t.yg)("inlineCode",{parentName:"p"},"BaseHook.get_connection()")," method requires ",(0,t.yg)("inlineCode",{parentName:"p"},"SUPERVISOR_COMMS")," to be available in the execution context. However, in Airflow 3.x listener hooks (such as ",(0,t.yg)("inlineCode",{parentName:"p"},"on_dag_start"),", ",(0,t.yg)("inlineCode",{parentName:"p"},"on_dag_run_running"),"), the ",(0,t.yg)("inlineCode",{parentName:"p"},"SUPERVISOR_COMMS")," context is not available, causing connection retrieval to fail:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"# This fails in listener context:\n# ImportError: cannot import name 'SUPERVISOR_COMMS' from 'airflow.sdk.execution_time.task_runner'\nhook = self.config.make_emitter_hook()\nemitter = hook.make_emitter()  # \u274c Fails: SUPERVISOR_COMMS not available\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Why SUPERVISOR_COMMS is unavailable:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Listener hooks run in a different context than task execution"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"SUPERVISOR_COMMS")," is only available during actual task execution (when tasks are running)"),(0,t.yg)("li",{parentName:"ul"},"Listener hooks are called by the scheduler/webserver, not by task workers"),(0,t.yg)("li",{parentName:"ul"},"The supervisor communication mechanism is not initialized in listener context")),(0,t.yg)("h4",{id:"solution-use-sdk-connection-api"},"Solution: Use SDK Connection API"),(0,t.yg)("p",null,"The plugin now uses the Airflow SDK's ",(0,t.yg)("inlineCode",{parentName:"p"},"Connection.get()")," method to retrieve connection details:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'def _create_single_emitter_from_connection(self, conn_id: str):\n    """\n    Create a single emitter from a connection ID.\n\n    Uses Connection.get() from SDK which works in all contexts.\n    """\n    from airflow.sdk import Connection\n\n    # Get connection using SDK API (works in all contexts)\n    conn = Connection.get(conn_id)\n    if not conn:\n        logger.warning(\n            f"Connection \'{conn_id}\' not found in secrets backend or environment variables"\n        )\n        return None\n\n    # Build emitter from connection details\n    host = conn.host or ""\n    if not host:\n        logger.warning(f"Connection \'{conn_id}\' has no host configured")\n        return None\n\n    # Parse URL and add port if needed\n    from urllib.parse import urlparse, urlunparse\n    parsed = urlparse(host if "://" in host else f"http://{host}")\n    netloc = parsed.netloc\n    if conn.port and not parsed.port:\n        netloc = f"{parsed.hostname}:{conn.port}"\n\n    host = urlunparse((\n        parsed.scheme or "http",\n        netloc,\n        parsed.path,\n        parsed.params,\n        parsed.query,\n        parsed.fragment\n    ))\n\n    token = conf.get("datahub", "token", fallback=None) or conn.password\n\n    return DataHubRestEmitter(\n        host, token,\n        client_mode=ClientMode.INGESTION,\n        datahub_component="airflow-plugin",\n        **conn.extra_dejson\n    )\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Why ",(0,t.yg)("inlineCode",{parentName:"strong"},"Connection.get()")," from SDK:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Official Airflow 3.x API")," - ",(0,t.yg)("inlineCode",{parentName:"li"},"airflow.sdk.Connection")," is the proper SDK method"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Not deprecated")," - Unlike ",(0,t.yg)("inlineCode",{parentName:"li"},"Connection.get_connection_from_secrets()")," from ",(0,t.yg)("inlineCode",{parentName:"li"},"airflow.models")),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Works in all contexts")," - Listener hooks, task execution, DAG parsing"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"No SUPERVISOR_COMMS dependency")," - Checks environment variables, secrets backends, and database through proper APIs"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"More reliable")," - Doesn't depend on execution context being fully initialized"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Consistent behavior")," - Same initialization method regardless of when it's called"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Cleaner imports")," - Uses SDK module structure instead of legacy models module")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Import Path:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"# Airflow 3.x (correct, non-deprecated)\nfrom airflow.sdk import Connection\n\n# Airflow 2.x (deprecated in Airflow 3.x)\nfrom airflow.models import Connection\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Benefits:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Uses Python's ",(0,t.yg)("inlineCode",{parentName:"li"},"urllib.parse")," for robust URL handling (handles IPv6, paths, query strings)"),(0,t.yg)("li",{parentName:"ul"},"Supports all DataHub connection types (REST, Kafka, File)"),(0,t.yg)("li",{parentName:"ul"},"Handles multiple comma-separated connection IDs via ",(0,t.yg)("inlineCode",{parentName:"li"},"CompositeEmitter"))),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Token Configuration:")),(0,t.yg)("p",null,"The plugin supports token configuration via ",(0,t.yg)("inlineCode",{parentName:"p"},"airflow.cfg")," (takes precedence) or connection password:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-ini"},"# airflow.cfg\n[datahub]\ntoken = your_datahub_token_here\n")),(0,t.yg)("p",null,"If not set in ",(0,t.yg)("inlineCode",{parentName:"p"},"airflow.cfg"),", the connection password field is used as a fallback."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files Updated:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/airflow3/datahub_listener.py:297-398")," - Emitter initialization using SDK Connection API")),(0,t.yg)("h3",{id:"12-operator-specific-patches-for-openlineage"},"12. Operator-Specific Patches for OpenLineage"),(0,t.yg)("p",null,"In Airflow 3.x, some operators require specific patches to enable proper lineage extraction because they either:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Don't implement required OpenLineage methods (",(0,t.yg)("inlineCode",{parentName:"li"},"get_openlineage_database_info()"),")"),(0,t.yg)("li",{parentName:"ol"},"Use non-standard SQL storage mechanisms (e.g., configuration dictionaries)"),(0,t.yg)("li",{parentName:"ol"},"Use generic SQL dialects that don't support column-level lineage")),(0,t.yg)("p",null,"The plugin patches these operators to provide full lineage support."),(0,t.yg)("h4",{id:"12a-sqlite-operator-patch"},"12a. SQLite Operator Patch"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Problem:")," ",(0,t.yg)("inlineCode",{parentName:"p"},"SqliteHook")," doesn't implement ",(0,t.yg)("inlineCode",{parentName:"p"},"get_openlineage_database_info()"),", causing lineage extraction to fail."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Solution:")," Patch ",(0,t.yg)("inlineCode",{parentName:"p"},"SqliteHook.get_openlineage_database_info()")," to return proper database info:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'def get_openlineage_database_info(connection: Connection) -> DatabaseInfo:\n    # Extract database name from SQLite file path\n    db_path = connection.host\n    db_name = os.path.splitext(os.path.basename(db_path))[0]\n\n    return DatabaseInfo(\n        scheme="sqlite",\n        authority=None,  # SQLite doesn\'t have host:port\n        database=db_name,\n        normalize_name_method=lambda x: x.lower(),\n    )\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files:")," ",(0,t.yg)("inlineCode",{parentName:"p"},"src/datahub_airflow_plugin/airflow3/_sqlite_openlineage_patch.py")),(0,t.yg)("h4",{id:"12b-athena-operator-patch"},"12b. Athena Operator Patch"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Problem:")," ",(0,t.yg)("inlineCode",{parentName:"p"},"AthenaOperator")," uses ",(0,t.yg)("inlineCode",{parentName:"p"},"SQLParser")," with ",(0,t.yg)("inlineCode",{parentName:"p"},'dialect="generic"'),", which doesn't provide column-level lineage."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Solution:")," Wrap ",(0,t.yg)("inlineCode",{parentName:"p"},"AthenaOperator.get_openlineage_facets_on_complete()")," to:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Call the original OpenLineage implementation"),(0,t.yg)("li",{parentName:"ol"},"Enhance it with DataHub's SQL parser for column-level lineage")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'def get_openlineage_facets_on_complete(self, task_instance):\n    # Get original OpenLineage result\n    operator_lineage = original_method(self, task_instance)\n\n    # Enhance with DataHub SQL parsing\n    sql_parsing_result = create_lineage_sql_parsed_result(\n        query=self.query,\n        platform="athena",\n        default_db=self.database,\n    )\n\n    # Store result in run_facets for DataHub listener\n    operator_lineage.run_facets["datahub_sql_parsing_result"] = sql_parsing_result\n\n    return operator_lineage\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files:")," ",(0,t.yg)("inlineCode",{parentName:"p"},"src/datahub_airflow_plugin/airflow3/_athena_openlineage_patch.py")),(0,t.yg)("h4",{id:"12c-bigquery-insertjoboperator-patch"},"12c. BigQuery InsertJobOperator Patch"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Problem:")," ",(0,t.yg)("inlineCode",{parentName:"p"},"BigQueryInsertJobOperator")," stores SQL in a ",(0,t.yg)("inlineCode",{parentName:"p"},"configuration")," dictionary, not as a direct attribute. This means:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"The standard SQLParser patch (Section 10) can't intercept it because it doesn't go through ",(0,t.yg)("inlineCode",{parentName:"li"},"SQLParser.generate_openlineage_metadata_from_sql()")),(0,t.yg)("li",{parentName:"ul"},"The official OpenLineage implementation extracts table-level lineage from BigQuery's job metadata/API response, ",(0,t.yg)("strong",{parentName:"li"},"not by parsing SQL")),(0,t.yg)("li",{parentName:"ul"},"Therefore, the official implementation provides table-level lineage but ",(0,t.yg)("strong",{parentName:"li"},"no column-level lineage"))),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Solution:")," Wrap ",(0,t.yg)("inlineCode",{parentName:"p"},"get_openlineage_facets_on_complete()")," to:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Call the original OpenLineage implementation (gets table-level lineage from BigQuery job metadata)"),(0,t.yg)("li",{parentName:"ol"},"Extract SQL from ",(0,t.yg)("inlineCode",{parentName:"li"},'self.configuration.get("query", {}).get("query")')),(0,t.yg)("li",{parentName:"ol"},"Run DataHub's SQL parser with BigQuery dialect to add column-level lineage"),(0,t.yg)("li",{parentName:"ol"},"Handle destination table from configuration"),(0,t.yg)("li",{parentName:"ol"},"Store result in ",(0,t.yg)("inlineCode",{parentName:"li"},"run_facets"))),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'def get_openlineage_facets_on_complete(self, task_instance):\n    # Extract SQL from configuration\n    sql = self.configuration.get("query", {}).get("query")\n\n    # Get original result\n    operator_lineage = original_method(self, task_instance)\n\n    # Run DataHub parser\n    sql_parsing_result = create_lineage_sql_parsed_result(\n        query=sql,\n        platform="bigquery",\n        default_db=self.project_id,\n    )\n\n    # Add destination table if specified in configuration\n    destination_table = self.configuration.get("query", {}).get("destinationTable")\n    if destination_table:\n        # Add to output tables\n        ...\n\n    operator_lineage.run_facets["datahub_sql_parsing_result"] = sql_parsing_result\n    return operator_lineage\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files:")," ",(0,t.yg)("inlineCode",{parentName:"p"},"src/datahub_airflow_plugin/airflow3/_bigquery_openlineage_patch.py")),(0,t.yg)("h4",{id:"patch-registration"},"Patch Registration"),(0,t.yg)("p",null,"All patches are automatically applied when the plugin is loaded in Airflow 3.x:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},"# In _airflow_compat.py\nfrom datahub_airflow_plugin.airflow3._sqlite_openlineage_patch import patch_sqlite_hook\nfrom datahub_airflow_plugin.airflow3._athena_openlineage_patch import patch_athena_operator\nfrom datahub_airflow_plugin.airflow3._bigquery_openlineage_patch import patch_bigquery_insert_job_operator\n\npatch_sqlite_hook()\npatch_athena_operator()\npatch_bigquery_insert_job_operator()\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Key Points:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"\u2705 Patches are applied at import time, before any DAGs are loaded"),(0,t.yg)("li",{parentName:"ul"},"\u2705 Patches are idempotent (safe to call multiple times)"),(0,t.yg)("li",{parentName:"ul"},"\u2705 Patches gracefully handle missing providers (no error if provider not installed)"),(0,t.yg)("li",{parentName:"ul"},"\u2705 Each patch wraps the original method, ensuring compatibility with Airflow's OpenLineage implementation"),(0,t.yg)("li",{parentName:"ul"},"\u2705 Column-level lineage is consistently stored in ",(0,t.yg)("inlineCode",{parentName:"li"},'run_facets["datahub_sql_parsing_result"]')," for the listener to process")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Files Updated:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/airflow3/_airflow_compat.py")," - Patch registration"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/airflow3/_sqlite_openlineage_patch.py")," - SQLite hook patch"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/airflow3/_athena_openlineage_patch.py")," - Athena operator patch"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"src/datahub_airflow_plugin/airflow3/_bigquery_openlineage_patch.py")," - BigQuery operator patch")),(0,t.yg)("h3",{id:"13-column-level-lineage-in-airflow-3x"},"13. Column-Level Lineage in Airflow 3.x"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Status:")," \u2705 Fully Working"),(0,t.yg)("p",null,"Column-level (fine-grained) lineage is now supported in Airflow 3.x through:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"The ",(0,t.yg)("strong",{parentName:"li"},"SQLParser patch")," (Section 10) for standard SQL operators"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Operator-specific patches")," (Section 12) for special-case operators")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Example:")," For a SQL query like:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO processed_costs (id, month, total_cost, area, cost_per_area)\nSELECT id, month, total_cost, area, total_cost / area\nFROM costs\n")),(0,t.yg)("p",null,"The plugin generates fine-grained lineage:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"costs.id \u2192 processed_costs.id")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"costs.month \u2192 processed_costs.month")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"costs.total_cost \u2192 processed_costs.total_cost")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"costs.area \u2192 processed_costs.area")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"costs.area + costs.total_cost \u2192 processed_costs.cost_per_area")," (derived column)")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Verification:")," Run the Snowflake operator test:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},'tox -e py311-airflow302 -- -k "v2_snowflake_operator_airflow3"\n')),(0,t.yg)("p",null,"Check the golden file for ",(0,t.yg)("inlineCode",{parentName:"p"},"fineGrainedLineages"),":"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},'grep -A 10 "fineGrainedLineages" tests/integration/goldens/v2_snowflake_operator_airflow3.json\n')),(0,t.yg)("h2",{id:"known-limitations"},"Known Limitations"),(0,t.yg)("h3",{id:"1-subdag-lineage-not-supported-in-airflow-3x"},"1. SubDAG Lineage Not Supported in Airflow 3.x"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Impact:")," If you're upgrading from Airflow 2.x and using SubDAGs, lineage tracking for subdags will no longer work in Airflow 3.x."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Reason:")," SubDAGs were completely removed from Airflow 3.x."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Migration Path:")," Use TaskGroups instead of SubDAGs. TaskGroups provide visual grouping without creating separate DAG runs."),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'# Old (Airflow 2.x) - SubDAG\nfrom airflow.operators.subdag import SubDagOperator\n\ndef subdag(parent_dag_name, child_dag_name, args):\n    dag = DAG(f"{parent_dag_name}.{child_dag_name}", **args)\n    # Add tasks...\n    return dag\n\nwith DAG("parent_dag") as dag:\n    subdag_task = SubDagOperator(\n        task_id="subdag",\n        subdag=subdag("parent_dag", "subdag", default_args)\n    )\n\n# New (Airflow 3.x) - TaskGroup\nfrom airflow.utils.task_group import TaskGroup\n\nwith DAG("parent_dag") as dag:\n    with TaskGroup("task_group") as tg:\n        # Add tasks to the group...\n        task1 = BashOperator(...)\n        task2 = BashOperator(...)\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Lineage Note:")," TaskGroup lineage is tracked at the task level, not as a separate DAG entity."),(0,t.yg)("h3",{id:"2-configuration-migration-required"},"2. Configuration Migration Required"),(0,t.yg)("p",null,"When upgrading to Airflow 3.x, update your configuration:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# Old Airflow 2.x config\nAIRFLOW__WEBSERVER__WEB_SERVER_PORT=8080\nAIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.basic_auth\n\n# New Airflow 3.x config\nAIRFLOW__API__PORT=8080\n# AUTH_BACKEND no longer needed - uses SimpleAuthManager by default\n")),(0,t.yg)("h3",{id:"3-test-compatibility-matrix"},"3. Test Compatibility Matrix"),(0,t.yg)("p",null,"The DataHub Airflow plugin tests are designed to work with:"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Airflow Version"),(0,t.yg)("th",{parentName:"tr",align:null},"Test Support"),(0,t.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"2.3.x"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Limited"),(0,t.yg)("td",{parentName:"tr",align:null},"Only v1 plugin tested")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"2.4.x - 2.9.x"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Both v1 and v2 plugins")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"3.0.x+"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"v2 plugin only")))),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Note:")," Airflow 3.x requires the v2 plugin (listener-based). The v1 plugin is not compatible."),(0,t.yg)("h2",{id:"testing"},"Testing"),(0,t.yg)("h3",{id:"running-tests-against-airflow-3x"},"Running Tests Against Airflow 3.x"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# Test with Airflow 3.0.x\ntox -e py311-airflow310\n\n# Test with Airflow 3.1.x\ntox -e py311-airflow31\n")),(0,t.yg)("h3",{id:"verifying-compatibility"},"Verifying Compatibility"),(0,t.yg)("p",null,"The following checks are performed in tests:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"\u2705 Import compatibility - All modules import without warnings"),(0,t.yg)("li",{parentName:"ol"},"\u2705 DAG parsing - DAGs parse successfully without errors"),(0,t.yg)("li",{parentName:"ol"},"\u2705 API authentication - JWT token authentication works"),(0,t.yg)("li",{parentName:"ol"},"\u2705 Lineage extraction - Inlets/outlets are correctly extracted"),(0,t.yg)("li",{parentName:"ol"},"\u2705 Listener functionality - All listener hooks execute properly")),(0,t.yg)("h2",{id:"troubleshooting"},"Troubleshooting"),(0,t.yg)("h3",{id:"import-errors"},"Import Errors"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Error:")," ",(0,t.yg)("inlineCode",{parentName:"p"},"ImportError: cannot import name 'BaseOperator' from 'airflow.models.baseoperator'")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Solution:")," This is expected in Airflow 3.x. The plugin's shims handle this automatically. If you see this error, ensure you're using the latest version of the plugin."),(0,t.yg)("h3",{id:"authentication-failures"},"Authentication Failures"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Error:")," ",(0,t.yg)("inlineCode",{parentName:"p"},"401 Unauthorized")," when calling Airflow API"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Solution:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Airflow 3.x requires JWT authentication"),(0,t.yg)("li",{parentName:"ul"},"Ensure the username/password are correct"),(0,t.yg)("li",{parentName:"ul"},"Check that the ",(0,t.yg)("inlineCode",{parentName:"li"},"/auth/token")," endpoint is accessible")),(0,t.yg)("h3",{id:"subdag-warnings"},"SubDAG Warnings"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Warning:")," Code references ",(0,t.yg)("inlineCode",{parentName:"p"},"is_subdag")," attribute"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Solution:")," This is expected and safe. The plugin uses ",(0,t.yg)("inlineCode",{parentName:"p"},'getattr(dag, "is_subdag", False)')," which returns ",(0,t.yg)("inlineCode",{parentName:"p"},"False")," in Airflow 3.x without errors."),(0,t.yg)("h3",{id:"schedule-parameter-issues"},"Schedule Parameter Issues"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Error:")," ",(0,t.yg)("inlineCode",{parentName:"p"},"TypeError: __init__() got an unexpected keyword argument 'schedule_interval'")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Solution:")," Update DAG definitions to use ",(0,t.yg)("inlineCode",{parentName:"p"},"schedule=")," instead of ",(0,t.yg)("inlineCode",{parentName:"p"},"schedule_interval="),". The ",(0,t.yg)("inlineCode",{parentName:"p"},"schedule")," parameter is supported in Airflow 2.4+ and required in Airflow 3.x."),(0,t.yg)("h3",{id:"default-view-parameter-issues"},"Default View Parameter Issues"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Error:")," ",(0,t.yg)("inlineCode",{parentName:"p"},"TypeError: __init__() got an unexpected keyword argument 'default_view'")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Solution:")," Remove the ",(0,t.yg)("inlineCode",{parentName:"p"},"default_view")," parameter from DAG definitions. This parameter was removed in Airflow 3.x. User view preferences are now persistent in the UI."),(0,t.yg)("h2",{id:"migration-checklist"},"Migration Checklist"),(0,t.yg)("p",null,"When upgrading to Airflow 3.x:"),(0,t.yg)("ul",{className:"contains-task-list"},(0,t.yg)("li",{parentName:"ul",className:"task-list-item"},(0,t.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Update all DAG definitions to use ",(0,t.yg)("inlineCode",{parentName:"li"},"schedule=")," instead of ",(0,t.yg)("inlineCode",{parentName:"li"},"schedule_interval=")),(0,t.yg)("li",{parentName:"ul",className:"task-list-item"},(0,t.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Remove ",(0,t.yg)("inlineCode",{parentName:"li"},"default_view=")," parameter from all DAG definitions"),(0,t.yg)("li",{parentName:"ul",className:"task-list-item"},(0,t.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Replace SubDAGs with TaskGroups"),(0,t.yg)("li",{parentName:"ul",className:"task-list-item"},(0,t.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Update Airflow configuration (port, auth settings)"),(0,t.yg)("li",{parentName:"ul",className:"task-list-item"},(0,t.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Update any custom operators to use new import paths"),(0,t.yg)("li",{parentName:"ul",className:"task-list-item"},(0,t.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Test lineage extraction with sample DAGs"),(0,t.yg)("li",{parentName:"ul",className:"task-list-item"},(0,t.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Verify API authentication works"),(0,t.yg)("li",{parentName:"ul",className:"task-list-item"},(0,t.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Update CI/CD pipelines to use Airflow 3.x")),(0,t.yg)("h2",{id:"references"},"References"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"https://airflow.apache.org/docs/apache-airflow/stable/release_notes.html#airflow-3-0-0"},"Airflow 3.0 Release Notes")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"https://airflow.apache.org/docs/apache-airflow/stable/migration-guide-to-3.html"},"Airflow 3.0 Migration Guide")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#taskgroups"},"TaskGroups Documentation")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"https://datahubproject.io/docs/metadata-ingestion/integration_docs/airflow/"},"DataHub Airflow Plugin Documentation"))),(0,t.yg)("h2",{id:"support"},"Support"),(0,t.yg)("p",null,"For issues related to Airflow 3.x compatibility:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Check this migration guide"),(0,t.yg)("li",{parentName:"ol"},"Review the ",(0,t.yg)("a",{parentName:"li",href:"https://github.com/datahub-project/datahub/issues"},"GitHub Issues")),(0,t.yg)("li",{parentName:"ol"},"Ask in the ",(0,t.yg)("a",{parentName:"li",href:"https://slack.datahubproject.io/"},"DataHub Slack"))),(0,t.yg)("h2",{id:"version-history"},"Version History"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Version"),(0,t.yg)("th",{parentName:"tr",align:null},"Date"),(0,t.yg)("th",{parentName:"tr",align:null},"Changes"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"1.0"),(0,t.yg)("td",{parentName:"tr",align:null},"2025-01-XX"),(0,t.yg)("td",{parentName:"tr",align:null},"Initial Airflow 3.x support added")))))}y.isMDXComponent=!0}}]);