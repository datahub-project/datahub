"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[48685],{22447:(a,t,e)=>{e.r(t),e.d(t,{assets:()=>m,contentTitle:()=>u,default:()=>w,frontMatter:()=>s,metadata:()=>p,toc:()=>b});e(96540);var o=e(15680),n=e(53720),r=e(5400);function l(a,t,e){return t in a?Object.defineProperty(a,t,{value:e,enumerable:!0,configurable:!0,writable:!0}):a[t]=e,a}function i(a,t){return t=null!=t?t:{},Object.getOwnPropertyDescriptors?Object.defineProperties(a,Object.getOwnPropertyDescriptors(t)):function(a,t){var e=Object.keys(a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(a);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(a,t).enumerable}))),e.push.apply(e,o)}return e}(Object(t)).forEach((function(e){Object.defineProperty(a,e,Object.getOwnPropertyDescriptor(t,e))})),a}function d(a,t){if(null==a)return{};var e,o,n=function(a,t){if(null==a)return{};var e,o,n={},r=Object.keys(a);for(o=0;o<r.length;o++)e=r[o],t.indexOf(e)>=0||(n[e]=a[e]);return n}(a,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(a);for(o=0;o<r.length;o++)e=r[o],t.indexOf(e)>=0||Object.prototype.propertyIsEnumerable.call(a,e)&&(n[e]=a[e])}return n}const s={title:"DataFlow & DataJob",slug:"/api/tutorials/dataflow-datajob",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/api/tutorials/dataflow-datajob.md"},u="DataFlow & DataJob",p={unversionedId:"docs/api/tutorials/dataflow-datajob",id:"docs/api/tutorials/dataflow-datajob",title:"DataFlow & DataJob",description:"Why Would You Use DataFlow and DataJob?",source:"@site/genDocs/docs/api/tutorials/dataflow-datajob.md",sourceDirName:"docs/api/tutorials",slug:"/api/tutorials/dataflow-datajob",permalink:"/docs/api/tutorials/dataflow-datajob",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/api/tutorials/dataflow-datajob.md",tags:[],version:"current",frontMatter:{title:"DataFlow & DataJob",slug:"/api/tutorials/dataflow-datajob",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/api/tutorials/dataflow-datajob.md"},sidebar:"overviewSidebar",previous:{title:"Dashboard & Chart",permalink:"/docs/api/tutorials/dashboard-chart"},next:{title:"MLModel & MLModelGroup",permalink:"/docs/api/tutorials/mlmodel-mlmodelgroup"}},m={},b=[{value:"Why Would You Use DataFlow and DataJob?",id:"why-would-you-use-dataflow-and-datajob",level:2},{value:"Goal Of This Guide",id:"goal-of-this-guide",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Create DataFlow",id:"create-dataflow",level:2},{value:"Create DataJob",id:"create-datajob",level:2},{value:"Read DataFlow",id:"read-dataflow",level:2},{value:"Example Output",id:"example-output",level:4},{value:"Read DataJob",id:"read-datajob",level:2},{value:"Example Output",id:"example-output-1",level:4}],f={toc:b},c="wrapper";function w(a){var{components:t}=a,e=d(a,["components"]);return(0,o.yg)(c,i(function(a){for(var t=1;t<arguments.length;t++){var e=null!=arguments[t]?arguments[t]:{},o=Object.keys(e);"function"==typeof Object.getOwnPropertySymbols&&(o=o.concat(Object.getOwnPropertySymbols(e).filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable})))),o.forEach((function(t){l(a,t,e[t])}))}return a}({},f,e),{components:t,mdxType:"MDXLayout"}),(0,o.yg)("h1",{id:"dataflow--datajob"},"DataFlow & DataJob"),(0,o.yg)("h2",{id:"why-would-you-use-dataflow-and-datajob"},"Why Would You Use DataFlow and DataJob?"),(0,o.yg)("p",null,"The DataFlow and DataJob entities are used to represent data processing pipelines and jobs within a data ecosystem. They allow users to define, manage, and monitor the flow of data through various stages of processing, from ingestion to transformation and storage."),(0,o.yg)("h3",{id:"goal-of-this-guide"},"Goal Of This Guide"),(0,o.yg)("p",null,"This guide will show you how to"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Create a DataFlow."),(0,o.yg)("li",{parentName:"ul"},"Create a Datajob with a DataFlow.")),(0,o.yg)("h2",{id:"prerequisites"},"Prerequisites"),(0,o.yg)("p",null,"For this tutorial, you need to deploy DataHub Quickstart and ingest sample data.\nFor detailed steps, please refer to ",(0,o.yg)("a",{parentName:"p",href:"/docs/quickstart"},"Datahub Quickstart Guide"),"."),(0,o.yg)("h2",{id:"create-dataflow"},"Create DataFlow"),(0,o.yg)(n.A,{mdxType:"Tabs"},(0,o.yg)(r.A,{value:"python",label:"Python",default:!0,mdxType:"TabItem"},(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'# Inlined from /metadata-ingestion/examples/library/dataflow_create.py\nfrom datahub.metadata.urns import TagUrn\nfrom datahub.sdk import DataFlow, DataHubClient\n\nclient = DataHubClient.from_env()\n\ndataflow = DataFlow(\n    name="example_dataflow",\n    platform="airflow",\n    description="airflow pipeline for production",\n    tags=[TagUrn(name="production"), TagUrn(name="data_engineering")],\n)\n\nclient.entities.upsert(dataflow)\n\n')))),(0,o.yg)("h2",{id:"create-datajob"},"Create DataJob"),(0,o.yg)("p",null,"DataJob must be associated with a DataFlow. You can create a DataJob by providing the DataFlow object or the DataFlow URN and its platform instance."),(0,o.yg)(n.A,{mdxType:"Tabs"},(0,o.yg)(r.A,{value:"create-datajob",label:"Create DataJob with a DataFlow Object",default:!0,mdxType:"TabItem"},(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'# Inlined from /metadata-ingestion/examples/library/datajob_create_full.py\nfrom datahub.metadata.urns import DatasetUrn, TagUrn\nfrom datahub.sdk import DataFlow, DataHubClient, DataJob\n\nclient = DataHubClient.from_env()\n\n# datajob will inherit the platform and platform instance from the flow\n\ndataflow = DataFlow(\n    platform="airflow",\n    name="example_dag",\n    platform_instance="PROD",\n    description="example dataflow",\n    tags=[TagUrn(name="tag1"), TagUrn(name="tag2")],\n)\n\ndatajob = DataJob(\n    name="example_datajob",\n    flow=dataflow,\n    inlets=[\n        DatasetUrn(platform="hdfs", name="dataset1", env="PROD"),\n    ],\n    outlets=[\n        DatasetUrn(platform="hdfs", name="dataset2", env="PROD"),\n    ],\n)\n\nclient.entities.upsert(dataflow)\nclient.entities.upsert(datajob)\n\n'))),(0,o.yg)(r.A,{value:"create-datajob-urn",label:"Create DataJob with DataFlow URN",mdxType:"TabItem"},(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'# Inlined from /metadata-ingestion/examples/library/datajob_create_with_flow_urn.py\nfrom datahub.metadata.urns import DataFlowUrn, DatasetUrn\nfrom datahub.sdk import DataHubClient, DataJob\n\nclient = DataHubClient.from_env()\n\n# datajob will inherit the platform and platform instance from the flow\n\ndatajob = DataJob(\n    name="example_datajob",\n    flow_urn=DataFlowUrn(\n        orchestrator="airflow",\n        flow_id="example_dag",\n        cluster="PROD",\n    ),\n    platform_instance="PROD",\n    inlets=[\n        DatasetUrn(platform="hdfs", name="dataset1", env="PROD"),\n    ],\n    outlets=[\n        DatasetUrn(platform="hdfs", name="dataset2", env="PROD"),\n    ],\n)\n\nclient.entities.upsert(datajob)\n\n')))),(0,o.yg)("h2",{id:"read-dataflow"},"Read DataFlow"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'# Inlined from /metadata-ingestion/examples/library/dataflow_read.py\nfrom datahub.sdk import DataFlowUrn, DataHubClient\n\nclient = DataHubClient.from_env()\n\n# Or get this from the UI (share -> copy urn) and use DataFlowUrn.from_string(...)\ndataflow_urn = DataFlowUrn("airflow", "example_dataflow_id")\n\ndataflow_entity = client.entities.get(dataflow_urn)\nprint("DataFlow name:", dataflow_entity.name)\nprint("DataFlow platform:", dataflow_entity.platform)\nprint("DataFlow description:", dataflow_entity.description)\n\n')),(0,o.yg)("h4",{id:"example-output"},"Example Output"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},">> DataFlow name: example_dataflow\n>> DataFlow platform: urn:li:dataPlatform:airflow\n>> DataFlow description: airflow pipeline for production\n")),(0,o.yg)("h2",{id:"read-datajob"},"Read DataJob"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'# Inlined from /metadata-ingestion/examples/library/datajob_read.py\nfrom datahub.sdk import DataHubClient, DataJobUrn\n\nclient = DataHubClient.from_env()\n\n# Or get this from the UI (share -> copy urn) and use DataJobUrn.from_string(...)\ndatajob_urn = DataJobUrn("airflow", "example_dag", "example_datajob_id")\n\ndatajob_entity = client.entities.get(datajob_urn)\nprint("DataJob name:", datajob_entity.name)\nprint("DataJob Flow URN:", datajob_entity.flow_urn)\nprint("DataJob description:", datajob_entity.description)\n\n')),(0,o.yg)("h4",{id:"example-output-1"},"Example Output"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},">> DataJob name: example_datajob\n>> DataJob Flow URN: urn:li:dataFlow:(airflow,PROD.example_dag,PROD)\n>> DataJob description: example datajob\n")))}w.isMDXComponent=!0}}]);