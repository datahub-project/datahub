"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[20448],{15680:(e,a,t)=>{t.d(a,{xA:()=>d,yg:()=>c});var n=t(96540);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function l(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?l(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):l(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function i(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},l=Object.keys(e);for(n=0;n<l.length;n++)t=l[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)t=l[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=n.createContext({}),p=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},d=function(e){var a=p(e.components);return n.createElement(s.Provider,{value:a},e.children)},g="mdxType",u={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},m=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,l=e.originalType,s=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),g=p(t),m=r,c=g["".concat(s,".").concat(m)]||g[m]||u[m]||l;return t?n.createElement(c,o(o({ref:a},d),{},{components:t})):n.createElement(c,o({ref:a},d))}));function c(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var l=t.length,o=new Array(l);o[0]=m;var i={};for(var s in a)hasOwnProperty.call(a,s)&&(i[s]=a[s]);i.originalType=e,i[g]="string"==typeof e?e:r,o[1]=i;for(var p=2;p<l;p++)o[p]=t[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,t)}m.displayName="MDXCreateElement"},8896:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>c,frontMatter:()=>i,metadata:()=>p,toc:()=>g});t(96540);var n=t(15680);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function l(e,a){return a=null!=a?a:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):function(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))})),e}function o(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},l=Object.keys(e);for(n=0;n<l.length;n++)t=l[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)t=l[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}const i={title:"DataFlow Entity",slug:"/metadata-integration/java/docs/sdk-v2/dataflow-entity",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/docs/sdk-v2/dataflow-entity.md"},s="DataFlow Entity",p={unversionedId:"metadata-integration/java/docs/sdk-v2/dataflow-entity",id:"metadata-integration/java/docs/sdk-v2/dataflow-entity",title:"DataFlow Entity",description:"The DataFlow entity represents a data processing pipeline or workflow in DataHub. It models orchestrated workflows from tools like Apache Airflow, Apache Spark, dbt, Apache Flink, and other data orchestration platforms.",source:"@site/genDocs/metadata-integration/java/docs/sdk-v2/dataflow-entity.md",sourceDirName:"metadata-integration/java/docs/sdk-v2",slug:"/metadata-integration/java/docs/sdk-v2/dataflow-entity",permalink:"/docs/metadata-integration/java/docs/sdk-v2/dataflow-entity",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/docs/sdk-v2/dataflow-entity.md",tags:[],version:"current",frontMatter:{title:"DataFlow Entity",slug:"/metadata-integration/java/docs/sdk-v2/dataflow-entity",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/docs/sdk-v2/dataflow-entity.md"},sidebar:"overviewSidebar",previous:{title:"Container Entity",permalink:"/docs/metadata-integration/java/docs/sdk-v2/container-entity"},next:{title:"DataJob Entity",permalink:"/docs/metadata-integration/java/docs/sdk-v2/datajob-entity"}},d={},g=[{value:"Overview",id:"overview",level:2},{value:"URN Structure",id:"urn-structure",level:2},{value:"Creating a DataFlow",id:"creating-a-dataflow",level:2},{value:"Basic Creation",id:"basic-creation",level:3},{value:"With Custom Properties",id:"with-custom-properties",level:3},{value:"Properties",id:"properties",level:2},{value:"Core Properties",id:"core-properties",level:3},{value:"Additional Properties",id:"additional-properties",level:3},{value:"Operations",id:"operations",level:2},{value:"Ownership",id:"ownership",level:3},{value:"Tags",id:"tags",level:3},{value:"Glossary Terms",id:"glossary-terms",level:3},{value:"Domain",id:"domain",level:3},{value:"Custom Properties",id:"custom-properties",level:3},{value:"Description and Display Name",id:"description-and-display-name",level:3},{value:"Timestamps and URLs",id:"timestamps-and-urls",level:3},{value:"Orchestrator-Specific Examples",id:"orchestrator-specific-examples",level:2},{value:"Apache Airflow",id:"apache-airflow",level:3},{value:"Apache Spark",id:"apache-spark",level:3},{value:"dbt",id:"dbt",level:3},{value:"Apache Flink (Streaming)",id:"apache-flink-streaming",level:3},{value:"Fluent API",id:"fluent-api",level:2},{value:"Relationship with DataJob",id:"relationship-with-datajob",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Complete Example",id:"complete-example",level:2},{value:"See Also",id:"see-also",level:2}],u={toc:g},m="wrapper";function c(e){var{components:a}=e,t=o(e,["components"]);return(0,n.yg)(m,l(function(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{},n=Object.keys(t);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(t).filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable})))),n.forEach((function(a){r(e,a,t[a])}))}return e}({},u,t),{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"dataflow-entity"},"DataFlow Entity"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"DataFlow")," entity represents a data processing pipeline or workflow in DataHub. It models orchestrated workflows from tools like Apache Airflow, Apache Spark, dbt, Apache Flink, and other data orchestration platforms."),(0,n.yg)("h2",{id:"overview"},"Overview"),(0,n.yg)("p",null,"A DataFlow is a logical grouping of data processing tasks that work together to achieve a data transformation or movement goal. DataFlows are typically:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Scheduled workflows")," (Airflow DAGs)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Batch processing jobs")," (Spark applications)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Transformation projects")," (dbt projects)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Streaming pipelines")," (Flink jobs)")),(0,n.yg)("p",null,"DataFlows serve as parent containers for ",(0,n.yg)("inlineCode",{parentName:"p"},"DataJob")," entities, representing the overall pipeline while individual jobs represent specific tasks within that pipeline."),(0,n.yg)("h2",{id:"urn-structure"},"URN Structure"),(0,n.yg)("p",null,"DataFlow URNs follow this format:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"urn:li:dataFlow:(orchestrator,flowId,cluster)\n")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Components:")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"orchestrator"),': The platform or tool running the workflow (e.g., "airflow", "spark", "dbt", "flink")'),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"flowId"),": The unique identifier for the flow within the orchestrator (e.g., DAG ID, job name, project name)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"cluster"),': The cluster or environment where the flow runs (e.g., "prod", "prod-us-west-2", "emr-cluster")')),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Examples:")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"urn:li:dataFlow:(airflow,customer_etl_daily,prod)\nurn:li:dataFlow:(spark,ml_feature_generation,emr-prod-cluster)\nurn:li:dataFlow:(dbt,marketing_analytics,prod)\n")),(0,n.yg)("h2",{id:"creating-a-dataflow"},"Creating a DataFlow"),(0,n.yg)("h3",{id:"basic-creation"},"Basic Creation"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'DataFlow dataflow = DataFlow.builder()\n    .orchestrator("airflow")\n    .flowId("my_dag_id")\n    .cluster("prod")\n    .displayName("My ETL Pipeline")\n    .description("Daily ETL pipeline for customer data")\n    .build();\n\nclient.entities().upsert(dataflow);\n')),(0,n.yg)("h3",{id:"with-custom-properties"},"With Custom Properties"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'Map<String, String> properties = new HashMap<>();\nproperties.put("schedule", "0 2 * * *");\nproperties.put("team", "data-engineering");\nproperties.put("sla_hours", "4");\n\nDataFlow dataflow = DataFlow.builder()\n    .orchestrator("airflow")\n    .flowId("customer_pipeline")\n    .cluster("prod")\n    .customProperties(properties)\n    .build();\n')),(0,n.yg)("h2",{id:"properties"},"Properties"),(0,n.yg)("h3",{id:"core-properties"},"Core Properties"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Property"),(0,n.yg)("th",{parentName:"tr",align:null},"Type"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"),(0,n.yg)("th",{parentName:"tr",align:null},"Example"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"orchestrator")),(0,n.yg)("td",{parentName:"tr",align:null},"String"),(0,n.yg)("td",{parentName:"tr",align:null},"Platform running the flow (required)"),(0,n.yg)("td",{parentName:"tr",align:null},'"airflow", "spark", "dbt"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"flowId")),(0,n.yg)("td",{parentName:"tr",align:null},"String"),(0,n.yg)("td",{parentName:"tr",align:null},"Unique flow identifier (required)"),(0,n.yg)("td",{parentName:"tr",align:null},'"my_dag_id", "my_job_name"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"cluster")),(0,n.yg)("td",{parentName:"tr",align:null},"String"),(0,n.yg)("td",{parentName:"tr",align:null},"Cluster/environment (required)"),(0,n.yg)("td",{parentName:"tr",align:null},'"prod", "dev", "prod-us-west-2"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"displayName")),(0,n.yg)("td",{parentName:"tr",align:null},"String"),(0,n.yg)("td",{parentName:"tr",align:null},"Human-readable name"),(0,n.yg)("td",{parentName:"tr",align:null},'"Customer ETL Pipeline"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"description")),(0,n.yg)("td",{parentName:"tr",align:null},"String"),(0,n.yg)("td",{parentName:"tr",align:null},"Flow description"),(0,n.yg)("td",{parentName:"tr",align:null},'"Processes customer data daily"')))),(0,n.yg)("h3",{id:"additional-properties"},"Additional Properties"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Property"),(0,n.yg)("th",{parentName:"tr",align:null},"Type"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"externalUrl")),(0,n.yg)("td",{parentName:"tr",align:null},"String"),(0,n.yg)("td",{parentName:"tr",align:null},"Link to flow in orchestration tool")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"project")),(0,n.yg)("td",{parentName:"tr",align:null},"String"),(0,n.yg)("td",{parentName:"tr",align:null},"Associated project or namespace")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"customProperties")),(0,n.yg)("td",{parentName:"tr",align:null},"Map<String, String>"),(0,n.yg)("td",{parentName:"tr",align:null},"Key-value metadata")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"created")),(0,n.yg)("td",{parentName:"tr",align:null},"Long"),(0,n.yg)("td",{parentName:"tr",align:null},"Creation timestamp (milliseconds)")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"lastModified")),(0,n.yg)("td",{parentName:"tr",align:null},"Long"),(0,n.yg)("td",{parentName:"tr",align:null},"Last modified timestamp (milliseconds)")))),(0,n.yg)("h2",{id:"operations"},"Operations"),(0,n.yg)("h3",{id:"ownership"},"Ownership"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Add owners\ndataflow.addOwner("urn:li:corpuser:johndoe", OwnershipType.TECHNICAL_OWNER);\ndataflow.addOwner("urn:li:corpuser:analytics_team", OwnershipType.BUSINESS_OWNER);\n\n// Remove owner\ndataflow.removeOwner("urn:li:corpuser:johndoe");\n')),(0,n.yg)("h3",{id:"tags"},"Tags"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Add tags (with or without "urn:li:tag:" prefix)\ndataflow.addTag("etl");\ndataflow.addTag("production");\ndataflow.addTag("urn:li:tag:pii");\n\n// Remove tag\ndataflow.removeTag("etl");\n')),(0,n.yg)("h3",{id:"glossary-terms"},"Glossary Terms"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Add terms\ndataflow.addTerm("urn:li:glossaryTerm:ETL");\ndataflow.addTerm("urn:li:glossaryTerm:DataPipeline");\n\n// Remove term\ndataflow.removeTerm("urn:li:glossaryTerm:ETL");\n')),(0,n.yg)("h3",{id:"domain"},"Domain"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Set domain\ndataflow.setDomain("urn:li:domain:DataEngineering");\n\n// Remove specific domain\ndataflow.removeDomain("urn:li:domain:DataEngineering");\n\n// Or clear all domains\ndataflow.clearDomains();\n')),(0,n.yg)("h3",{id:"custom-properties"},"Custom Properties"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Add individual properties\ndataflow.addCustomProperty("schedule", "0 2 * * *");\ndataflow.addCustomProperty("team", "data-engineering");\n\n// Remove property\ndataflow.removeCustomProperty("schedule");\n\n// Set all properties (replaces existing)\nMap<String, String> props = new HashMap<>();\nprops.put("key1", "value1");\nprops.put("key2", "value2");\ndataflow.setCustomProperties(props);\n')),(0,n.yg)("h3",{id:"description-and-display-name"},"Description and Display Name"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Set description\ndataflow.setDescription("Daily ETL pipeline for customer data");\n\n// Set display name\ndataflow.setDisplayName("Customer ETL Pipeline");\n\n// Get description\nString description = dataflow.getDescription();\n\n// Get display name\nString displayName = dataflow.getDisplayName();\n')),(0,n.yg)("h3",{id:"timestamps-and-urls"},"Timestamps and URLs"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Set external URL\ndataflow.setExternalUrl("https://airflow.example.com/dags/my_dag");\n\n// Set project\ndataflow.setProject("customer_analytics");\n\n// Set timestamps\ndataflow.setCreated(System.currentTimeMillis() - 86400000L); // 1 day ago\ndataflow.setLastModified(System.currentTimeMillis());\n')),(0,n.yg)("h2",{id:"orchestrator-specific-examples"},"Orchestrator-Specific Examples"),(0,n.yg)("h3",{id:"apache-airflow"},"Apache Airflow"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'DataFlow airflowFlow = DataFlow.builder()\n    .orchestrator("airflow")\n    .flowId("customer_etl_daily")\n    .cluster("prod")\n    .displayName("Customer ETL Pipeline")\n    .description("Daily pipeline processing customer data from MySQL to Snowflake")\n    .build();\n\nairflowFlow\n    .addTag("etl")\n    .addTag("production")\n    .addCustomProperty("schedule", "0 2 * * *")\n    .addCustomProperty("catchup", "false")\n    .addCustomProperty("max_active_runs", "1")\n    .setExternalUrl("https://airflow.company.com/dags/customer_etl_daily");\n')),(0,n.yg)("h3",{id:"apache-spark"},"Apache Spark"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'DataFlow sparkFlow = DataFlow.builder()\n    .orchestrator("spark")\n    .flowId("ml_feature_generation")\n    .cluster("emr-prod-cluster")\n    .displayName("ML Feature Generation Job")\n    .description("Large-scale Spark job generating ML features")\n    .build();\n\nsparkFlow\n    .addTag("spark")\n    .addTag("machine-learning")\n    .addCustomProperty("spark.executor.memory", "8g")\n    .addCustomProperty("spark.driver.memory", "4g")\n    .addCustomProperty("spark.executor.cores", "4")\n    .setDomain("urn:li:domain:MachineLearning");\n')),(0,n.yg)("h3",{id:"dbt"},"dbt"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'DataFlow dbtFlow = DataFlow.builder()\n    .orchestrator("dbt")\n    .flowId("marketing_analytics")\n    .cluster("prod")\n    .displayName("Marketing Analytics Models")\n    .description("dbt transformations for marketing data")\n    .build();\n\ndbtFlow\n    .addTag("dbt")\n    .addTag("transformation")\n    .addCustomProperty("dbt_version", "1.5.0")\n    .addCustomProperty("target", "production")\n    .addCustomProperty("models_count", "87")\n    .setProject("marketing")\n    .setExternalUrl("https://github.com/company/dbt-marketing");\n')),(0,n.yg)("h3",{id:"apache-flink-streaming"},"Apache Flink (Streaming)"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'DataFlow flinkFlow = DataFlow.builder()\n    .orchestrator("flink")\n    .flowId("real_time_fraud_detection")\n    .cluster("prod-flink-cluster")\n    .displayName("Real-time Fraud Detection")\n    .description("Real-time streaming pipeline for fraud detection")\n    .build();\n\nflinkFlow\n    .addTag("streaming")\n    .addTag("real-time")\n    .addTag("fraud-detection")\n    .addCustomProperty("parallelism", "16")\n    .addCustomProperty("checkpoint_interval", "60000")\n    .setDomain("urn:li:domain:Security");\n')),(0,n.yg)("h2",{id:"fluent-api"},"Fluent API"),(0,n.yg)("p",null,"All mutation methods return ",(0,n.yg)("inlineCode",{parentName:"p"},"this")," to support method chaining:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'DataFlow dataflow = DataFlow.builder()\n    .orchestrator("airflow")\n    .flowId("sales_pipeline")\n    .cluster("prod")\n    .build();\n\ndataflow\n    .addTag("etl")\n    .addTag("production")\n    .addOwner("urn:li:corpuser:owner1", OwnershipType.TECHNICAL_OWNER)\n    .addOwner("urn:li:corpuser:owner2", OwnershipType.BUSINESS_OWNER)\n    .addTerm("urn:li:glossaryTerm:Sales")\n    .setDomain("urn:li:domain:Sales")\n    .setDescription("Sales data pipeline")\n    .addCustomProperty("schedule", "0 2 * * *")\n    .addCustomProperty("team", "sales-analytics");\n\nclient.entities().upsert(dataflow);\n')),(0,n.yg)("h2",{id:"relationship-with-datajob"},"Relationship with DataJob"),(0,n.yg)("p",null,"DataFlows are parent entities to DataJobs. A DataJob represents a specific task or step within a DataFlow:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Create the parent DataFlow\nDataFlow dataflow = DataFlow.builder()\n    .orchestrator("airflow")\n    .flowId("customer_etl")\n    .cluster("prod")\n    .build();\n\nclient.entities().upsert(dataflow);\n\n// Create child DataJobs that reference the parent flow\nDataJob extractJob = DataJob.builder()\n    .flow(dataflow.getUrn())  // References parent DataFlow\n    .jobId("extract_customers")\n    .build();\n\nDataJob transformJob = DataJob.builder()\n    .flow(dataflow.getUrn())\n    .jobId("transform_customers")\n    .build();\n\nclient.entities().upsert(extractJob);\nclient.entities().upsert(transformJob);\n')),(0,n.yg)("p",null,"This hierarchy allows you to:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Model the overall pipeline (DataFlow)"),(0,n.yg)("li",{parentName:"ul"},"Model individual tasks within the pipeline (DataJob)"),(0,n.yg)("li",{parentName:"ul"},"Track task-level lineage and dependencies"),(0,n.yg)("li",{parentName:"ul"},"Organize governance metadata at both levels")),(0,n.yg)("h2",{id:"best-practices"},"Best Practices"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Use consistent naming"),': Keep orchestrator names consistent across your organization (e.g., always use "airflow", not "Airflow" or "AIRFLOW")')),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Choose appropriate clusters"),': Use meaningful cluster names that indicate environment and region (e.g., "prod-us-west-2", "staging-eu-central-1")')),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Add scheduling information"),": Include schedule expressions in custom properties for batch workflows")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Link to source systems"),": Always set ",(0,n.yg)("inlineCode",{parentName:"p"},"externalUrl")," to link back to the orchestration tool's UI")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Set ownership early"),": Assign technical and business owners when creating flows")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Use tags for categorization"),": Tag flows by type (etl, streaming, ml), environment (production, staging), and criticality")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Document SLAs"),": Use custom properties to document SLA requirements and alert channels")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Track versions"),": For versioned workflows (like dbt), include version information in custom properties"))),(0,n.yg)("h2",{id:"complete-example"},"Complete Example"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Initialize client\nDataHubClientConfigV2 config = DataHubClientConfigV2.builder()\n    .server("http://localhost:8080")\n    .token(System.getenv("DATAHUB_TOKEN"))\n    .build();\n\ntry (DataHubClientV2 client = new DataHubClientV2(config)) {\n\n    // Create comprehensive DataFlow\n    Map<String, String> customProps = new HashMap<>();\n    customProps.put("schedule", "0 2 * * *");\n    customProps.put("catchup", "false");\n    customProps.put("team", "data-engineering");\n    customProps.put("sla_hours", "4");\n    customProps.put("alert_channel", "#data-alerts");\n\n    DataFlow dataflow = DataFlow.builder()\n        .orchestrator("airflow")\n        .flowId("production_etl_pipeline")\n        .cluster("prod-us-east-1")\n        .displayName("Production ETL Pipeline")\n        .description("Main ETL pipeline for customer data processing")\n        .customProperties(customProps)\n        .build();\n\n    dataflow\n        .addTag("etl")\n        .addTag("production")\n        .addTag("pii")\n        .addOwner("urn:li:corpuser:data_eng_team", OwnershipType.TECHNICAL_OWNER)\n        .addOwner("urn:li:corpuser:product_owner", OwnershipType.BUSINESS_OWNER)\n        .addTerm("urn:li:glossaryTerm:ETL")\n        .addTerm("urn:li:glossaryTerm:CustomerData")\n        .setDomain("urn:li:domain:DataEngineering")\n        .setProject("customer_analytics")\n        .setExternalUrl("https://airflow.company.com/dags/production_etl_pipeline")\n        .setCreated(System.currentTimeMillis() - 86400000L * 30)\n        .setLastModified(System.currentTimeMillis());\n\n    // Upsert to DataHub\n    client.entities().upsert(dataflow);\n\n    System.out.println("Created DataFlow: " + dataflow.getUrn());\n}\n')),(0,n.yg)("h2",{id:"see-also"},"See Also"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"/docs/metadata-integration/java/docs/sdk-v2/datajob-entity"},"DataJob Entity")," - Child tasks within a DataFlow"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"/docs/metadata-integration/java/docs/sdk-v2/dataset-entity"},"Dataset Entity")," - Data sources and targets for DataFlows")))}c.isMDXComponent=!0}}]);