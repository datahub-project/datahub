"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[7287],{15680:(e,t,n)=>{n.d(t,{xA:()=>g,yg:()=>m});var r=n(96540);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=r.createContext({}),d=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},g=function(e){var t=d(e.components);return r.createElement(s.Provider,{value:t},e.children)},c="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,a=e.originalType,s=e.parentName,g=l(e,["components","mdxType","originalType","parentName"]),c=d(n),u=i,m=c["".concat(s,".").concat(u)]||c[u]||p[u]||a;return n?r.createElement(m,o(o({ref:t},g),{},{components:n})):r.createElement(m,o({ref:t},g))}));function m(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var a=n.length,o=new Array(a);o[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[c]="string"==typeof e?e:i,o[1]=l;for(var d=2;d<a;d++)o[d]=n[d];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},16581:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>g,contentTitle:()=>s,default:()=>m,frontMatter:()=>l,metadata:()=>d,toc:()=>c});n(96540);var r=n(15680);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){return t=null!=t?t:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):function(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})),e}function o(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}const l={title:"Switching Embedding Providers",slug:"/dev-guides/semantic-search/switching_providers",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/dev-guides/semantic-search/SWITCHING_PROVIDERS.md"},s="Switching Embedding Providers",d={unversionedId:"docs/dev-guides/semantic-search/SWITCHING_PROVIDERS",id:"docs/dev-guides/semantic-search/SWITCHING_PROVIDERS",title:"Switching Embedding Providers",description:"This guide explains how to migrate from one embedding provider to another. Switching providers requires deleting the semantic index and re-ingesting all documents because different models produce vectors with incompatible dimensions.",source:"@site/genDocs/docs/dev-guides/semantic-search/SWITCHING_PROVIDERS.md",sourceDirName:"docs/dev-guides/semantic-search",slug:"/dev-guides/semantic-search/switching_providers",permalink:"/docs/dev-guides/semantic-search/switching_providers",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/dev-guides/semantic-search/SWITCHING_PROVIDERS.md",tags:[],version:"current",frontMatter:{title:"Switching Embedding Providers",slug:"/dev-guides/semantic-search/switching_providers",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/dev-guides/semantic-search/SWITCHING_PROVIDERS.md"},sidebar:"overviewSidebar",previous:{title:"Semantic Search Configuration Guide",permalink:"/docs/dev-guides/semantic-search/configuration"},next:{title:"Extending the Metadata Model",permalink:"/docs/metadata-modeling/extending-the-metadata-model"}},g={},c=[{value:"When You Need This Guide",id:"when-you-need-this-guide",level:2},{value:"Provider and Model Reference",id:"provider-and-model-reference",level:2},{value:"Migration Steps",id:"migration-steps",level:2},{value:"Step 1: Stop DataHub Services",id:"step-1-stop-datahub-services",level:3},{value:"Step 2: Delete the Semantic Index",id:"step-2-delete-the-semantic-index",level:3},{value:"Step 3: Update Provider Configuration",id:"step-3-update-provider-configuration",level:3},{value:"Step 4: Update Index Configuration",id:"step-4-update-index-configuration",level:3},{value:"Step 5: Start DataHub",id:"step-5-start-datahub",level:3},{value:"Step 6: Re-ingest Documents",id:"step-6-re-ingest-documents",level:3},{value:"Step 7: Verify",id:"step-7-verify",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"&quot;No embeddings found&quot; after switching",id:"no-embeddings-found-after-switching",level:3},{value:"&quot;Dimension mismatch&quot; errors",id:"dimension-mismatch-errors",level:3},{value:"&quot;Invalid API key&quot; errors",id:"invalid-api-key-errors",level:3},{value:"Query returns no results but documents exist",id:"query-returns-no-results-but-documents-exist",level:3},{value:"Best Practices",id:"best-practices",level:2}],p={toc:c},u="wrapper";function m(e){var{components:t}=e,n=o(e,["components"]);return(0,r.yg)(u,a(function(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{},r=Object.keys(n);"function"==typeof Object.getOwnPropertySymbols&&(r=r.concat(Object.getOwnPropertySymbols(n).filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable})))),r.forEach((function(t){i(e,t,n[t])}))}return e}({},p,n),{components:t,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"switching-embedding-providers"},"Switching Embedding Providers"),(0,r.yg)("p",null,"This guide explains how to migrate from one embedding provider to another. Switching providers requires deleting the semantic index and re-ingesting all documents because different models produce vectors with incompatible dimensions."),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"For initial setup of semantic search (including all provider configurations), see ",(0,r.yg)("a",{parentName:"p",href:"/docs/how-to/semantic-search-configuration"},"Semantic Search Configuration"),".")),(0,r.yg)("h2",{id:"when-you-need-this-guide"},"When You Need This Guide"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Switching from OpenAI to AWS Bedrock (or vice versa)"),(0,r.yg)("li",{parentName:"ul"},"Switching from one model to another with different vector dimensions"),(0,r.yg)("li",{parentName:"ul"},"Changing from Cohere direct API to AWS Bedrock-managed Cohere")),(0,r.yg)("h2",{id:"provider-and-model-reference"},"Provider and Model Reference"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Provider"),(0,r.yg)("th",{parentName:"tr",align:null},"Model"),(0,r.yg)("th",{parentName:"tr",align:null},"Model Key"),(0,r.yg)("th",{parentName:"tr",align:null},"Dimensions"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"OpenAI"),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"text-embedding-3-large")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"text_embedding_3_large")),(0,r.yg)("td",{parentName:"tr",align:null},"3072")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"OpenAI"),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"text-embedding-3-small")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"text_embedding_3_small")),(0,r.yg)("td",{parentName:"tr",align:null},"1536")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"AWS Bedrock"),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"cohere.embed-english-v3")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"cohere_embed_v3")),(0,r.yg)("td",{parentName:"tr",align:null},"1024")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Cohere"),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"embed-english-v3.0")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"embed_english_v3_0")),(0,r.yg)("td",{parentName:"tr",align:null},"1024")))),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},(0,r.yg)("strong",{parentName:"p"},"Important:")," The model key is derived from the model name by replacing ",(0,r.yg)("inlineCode",{parentName:"p"},"-")," and ",(0,r.yg)("inlineCode",{parentName:"p"},".")," with ",(0,r.yg)("inlineCode",{parentName:"p"},"_"),". Both the ingestion connector and GMS must use the same model to ensure query embeddings match document embeddings.")),(0,r.yg)("h2",{id:"migration-steps"},"Migration Steps"),(0,r.yg)("h3",{id:"step-1-stop-datahub-services"},"Step 1: Stop DataHub Services"),(0,r.yg)("p",null,"Stop GMS and any ingestion jobs to prevent writes during migration:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"# Docker Compose\ndocker stop datahub-gms\n\n# Kubernetes\nkubectl scale deployment datahub-gms --replicas=0\n")),(0,r.yg)("h3",{id:"step-2-delete-the-semantic-index"},"Step 2: Delete the Semantic Index"),(0,r.yg)("p",null,"Delete the existing semantic index from OpenSearch:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'# Check existing semantic indices\ncurl -s "http://localhost:9200/_cat/indices/*semantic*?v"\n\n# Delete the semantic index (adjust index name as needed)\ncurl -X DELETE "http://localhost:9200/documentindex_v2_semantic"\n')),(0,r.yg)("h3",{id:"step-3-update-provider-configuration"},"Step 3: Update Provider Configuration"),(0,r.yg)("p",null,"Update your configuration with the new provider settings. See ",(0,r.yg)("a",{parentName:"p",href:"/docs/how-to/semantic-search-configuration"},"Semantic Search Configuration")," for the full configuration options for each provider (Helm charts and environment variables)."),(0,r.yg)("p",null,"Make sure to update:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Provider type")," (",(0,r.yg)("inlineCode",{parentName:"li"},"EMBEDDING_PROVIDER_TYPE"),")"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"API credentials")," (API key or IAM role)"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Vector dimension")," (",(0,r.yg)("inlineCode",{parentName:"li"},"ELASTICSEARCH_SEMANTIC_VECTOR_DIMENSION"),") to match the new model")),(0,r.yg)("h3",{id:"step-4-update-index-configuration"},"Step 4: Update Index Configuration"),(0,r.yg)("p",null,"If using ",(0,r.yg)("inlineCode",{parentName:"p"},"application.yaml"),", update the model entry to match the new provider:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"elasticsearch:\n  entityIndex:\n    semanticSearch:\n      models:\n        # Use the model key that matches your new provider\n        text_embedding_3_large:\n          vectorDimension: 3072 # Must match model output\n          knnEngine: faiss\n          spaceType: cosinesimil\n          efConstruction: 128\n          m: 16\n")),(0,r.yg)("p",null,"Or via environment variable:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"ELASTICSEARCH_SEMANTIC_VECTOR_DIMENSION=3072\n")),(0,r.yg)("h3",{id:"step-5-start-datahub"},"Step 5: Start DataHub"),(0,r.yg)("p",null,"Start GMS \u2014 the system update job will automatically recreate the semantic index:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"# Docker Compose\ndocker start datahub-gms\n\n# Kubernetes\nkubectl scale deployment datahub-gms --replicas=1\n")),(0,r.yg)("p",null,"The system update job runs automatically on startup and will:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Detect the missing semantic index"),(0,r.yg)("li",{parentName:"ol"},"Create it with the correct mapping for your new embedding model"),(0,r.yg)("li",{parentName:"ol"},"Log progress to the GMS logs")),(0,r.yg)("h3",{id:"step-6-re-ingest-documents"},"Step 6: Re-ingest Documents"),(0,r.yg)("p",null,"After the index is recreated, re-ingest your documents to generate new embeddings:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"datahub ingest -c your-recipe.yaml\n")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Important:")," Make sure your ingestion recipe also uses the same embedding model. The ingestion connector generates document embeddings, while GMS generates query embeddings \u2014 both must use the same model."),(0,r.yg)("h3",{id:"step-7-verify"},"Step 7: Verify"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'# Check the index exists with correct mapping\ncurl -s "http://localhost:9200/documentindex_v2_semantic/_mapping?pretty" | head -50\n\n# Check documents have embeddings\ncurl -s "http://localhost:9200/documentindex_v2_semantic/_search" \\\n  -H "Content-Type: application/json" \\\n  -d \'{"size": 1, "_source": ["urn", "embeddings"]}\' | head -30\n\n# Test semantic search via GraphQL or the UI\n')),(0,r.yg)("h2",{id:"troubleshooting"},"Troubleshooting"),(0,r.yg)("h3",{id:"no-embeddings-found-after-switching"},'"No embeddings found" after switching'),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Cause:")," Documents were ingested before the provider switch and have embeddings from the old model."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Solution:")," Re-run ingestion to generate new embeddings with the new provider."),(0,r.yg)("h3",{id:"dimension-mismatch-errors"},'"Dimension mismatch" errors'),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Cause:")," The index was created with a different vector dimension than the new model produces."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Solution:")," Delete the semantic index and let it be recreated (Steps 2-5 above)."),(0,r.yg)("h3",{id:"invalid-api-key-errors"},'"Invalid API key" errors'),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Cause:")," API key not set or incorrect."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Solution:")," Verify your API key is correctly set in the environment:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"# Check the environment variable is set (in the container)\ndocker exec datahub-gms env | grep -E 'OPENAI_API_KEY|COHERE_API_KEY'\n")),(0,r.yg)("h3",{id:"query-returns-no-results-but-documents-exist"},"Query returns no results but documents exist"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Cause:")," Model mismatch between ingestion and query time."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Solution:")," Ensure both the ingestion connector AND GMS use the same embedding model. Check:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"The provider-specific model env var (",(0,r.yg)("inlineCode",{parentName:"li"},"BEDROCK_EMBEDDING_MODEL"),", ",(0,r.yg)("inlineCode",{parentName:"li"},"OPENAI_EMBEDDING_MODEL"),", or ",(0,r.yg)("inlineCode",{parentName:"li"},"COHERE_EMBEDDING_MODEL"),") in GMS config"),(0,r.yg)("li",{parentName:"ul"},"Embedding model in your ingestion recipe")),(0,r.yg)("h2",{id:"best-practices"},"Best Practices"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Use the same model everywhere"),": Ensure ingestion connectors and GMS use identical embedding models."),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Test in development first"),": Switch providers in a dev environment before production."),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Plan for re-ingestion"),": Switching providers requires re-generating all embeddings, which can take time for large datasets."),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Monitor costs"),": Different providers have different pricing. OpenAI and Cohere charge per token/request."),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Keep backups"),": Before deleting indices, consider backing up if you might need to rollback.")))}m.isMDXComponent=!0}}]);