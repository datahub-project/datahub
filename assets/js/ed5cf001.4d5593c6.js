"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[27379],{7653:(e,a,t)=>{t.d(a,{A:()=>n});const n={icon:{tag:"svg",attrs:{"fill-rule":"evenodd",viewBox:"64 64 896 896",focusable:"false"},children:[{tag:"path",attrs:{d:"M512 64c247.4 0 448 200.6 448 448S759.4 960 512 960 64 759.4 64 512 264.6 64 512 64zm127.98 274.82h-.04l-.08.06L512 466.75 384.14 338.88c-.04-.05-.06-.06-.08-.06a.12.12 0 00-.07 0c-.03 0-.05.01-.09.05l-45.02 45.02a.2.2 0 00-.05.09.12.12 0 000 .07v.02a.27.27 0 00.06.06L466.75 512 338.88 639.86c-.05.04-.06.06-.06.08a.12.12 0 000 .07c0 .03.01.05.05.09l45.02 45.02a.2.2 0 00.09.05.12.12 0 00.07 0c.02 0 .04-.01.08-.05L512 557.25l127.86 127.87c.04.04.06.05.08.05a.12.12 0 00.07 0c.03 0 .05-.01.09-.05l45.02-45.02a.2.2 0 00.05-.09.12.12 0 000-.07v-.02a.27.27 0 00-.05-.06L557.25 512l127.87-127.86c.04-.04.05-.06.05-.08a.12.12 0 000-.07c0-.03-.01-.05-.05-.09l-45.02-45.02a.2.2 0 00-.09-.05.12.12 0 00-.07 0z"}}]},name:"close-circle",theme:"filled"}},4732:(e,a,t)=>{t.d(a,{A:()=>s});var n=t(89379),r=t(96540),l=t(7653),i=t(89990),o=function(e,a){return r.createElement(i.A,(0,n.A)((0,n.A)({},e),{},{ref:a,icon:l.A}))};const s=r.forwardRef(o)},43655:(e,a,t)=>{t.d(a,{A:()=>f});var n=t(96540),r=t(20053);const l="availabilityCard_P5od",i="managedIcon_AxXO",o="platform_wqXv",s="platformAvailable_Y8lN";var c=t(4732),g=t(89379);const p={icon:{tag:"svg",attrs:{viewBox:"64 64 896 896",focusable:"false"},children:[{tag:"path",attrs:{d:"M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64zm193.5 301.7l-210.6 292a31.8 31.8 0 01-51.7 0L318.5 484.9c-3.8-5.3 0-12.7 6.5-12.7h46.9c10.2 0 19.9 4.9 25.9 13.3l71.2 98.8 157.2-218c6-8.3 15.6-13.3 25.9-13.3H699c6.5 0 10.3 7.4 6.5 12.7z"}}]},name:"check-circle",theme:"filled"};var u=t(89990),d=function(e,a){return n.createElement(u.A,(0,g.A)((0,g.A)({},e),{},{ref:a,icon:p}))};const m=n.forwardRef(d);const y={icon:{tag:"svg",attrs:{viewBox:"64 64 896 896",focusable:"false"},children:[{tag:"path",attrs:{d:"M811.4 418.7C765.6 297.9 648.9 212 512.2 212S258.8 297.8 213 418.6C127.3 441.1 64 519.1 64 612c0 110.5 89.5 200 199.9 200h496.2C870.5 812 960 722.5 960 612c0-92.7-63.1-170.7-148.6-193.3zm36.3 281a123.07 123.07 0 01-87.6 36.3H263.9c-33.1 0-64.2-12.9-87.6-36.3A123.3 123.3 0 01140 612c0-28 9.1-54.3 26.2-76.3a125.7 125.7 0 0166.1-43.7l37.9-9.9 13.9-36.6c8.6-22.8 20.6-44.1 35.7-63.4a245.6 245.6 0 0152.4-49.9c41.1-28.9 89.5-44.2 140-44.2s98.9 15.3 140 44.2c19.9 14 37.5 30.8 52.4 49.9 15.1 19.3 27.1 40.7 35.7 63.4l13.8 36.5 37.8 10c54.3 14.5 92.1 63.8 92.1 120 0 33.1-12.9 64.3-36.3 87.7z"}}]},name:"cloud",theme:"outlined"};var b=function(e,a){return n.createElement(u.A,(0,g.A)((0,g.A)({},e),{},{ref:a,icon:y}))};const h=n.forwardRef(b),f=({saasOnly:e,ossOnly:a})=>n.createElement("div",{className:(0,r.A)(l,"card")},n.createElement("strong",null,"Feature Availability"),n.createElement("div",null,n.createElement("span",{className:(0,r.A)(o,!e&&s)},"Self-Hosted DataHub ",e?n.createElement(c.A,null):n.createElement(m,null))),n.createElement("div",null,n.createElement(h,{className:i}),n.createElement("span",{className:(0,r.A)(o,!a&&s)},"DataHub Cloud ",a?n.createElement(c.A,null):n.createElement(m,null))))},17985:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>p,default:()=>h,frontMatter:()=>g,metadata:()=>u,toc:()=>m});t(96540);var n=t(15680),r=t(43655),l=t(53720),i=t(5400);function o(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function s(e,a){return a=null!=a?a:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):function(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))})),e}function c(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},l=Object.keys(e);for(n=0;n<l.length;n++)t=l[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)t=l[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}const g={title:"DataHub Iceberg Catalog",sidebar_label:"Iceberg Catalog",slug:"/iceberg-catalog",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/iceberg-catalog.md"},p="DataHub Iceberg Catalog",u={unversionedId:"docs/iceberg-catalog",id:"version-1.1.0/docs/iceberg-catalog",title:"DataHub Iceberg Catalog",description:"Note that this feature is currently in open Beta. With any questions or issues, please reach out to your DataHub",source:"@site/versioned_docs/version-1.1.0/docs/iceberg-catalog.md",sourceDirName:"docs",slug:"/iceberg-catalog",permalink:"/docs/1.1.0/iceberg-catalog",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/iceberg-catalog.md",tags:[],version:"1.1.0",frontMatter:{title:"DataHub Iceberg Catalog",sidebar_label:"Iceberg Catalog",slug:"/iceberg-catalog",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/iceberg-catalog.md"},sidebar:"overviewSidebar",previous:{title:"Metadata Standards",permalink:"/docs/1.1.0/metadata-standards"},next:{title:"OpenLineage",permalink:"/docs/1.1.0/lineage/openlineage"}},d={},m=[{value:"Introduction",id:"introduction",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"Conceptual Mapping",id:"conceptual-mapping",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Required AWS Permissions",id:"required-aws-permissions",level:3},{value:"Setup Instructions",id:"setup-instructions",level:2},{value:"1. Provision a Warehouse",id:"1-provision-a-warehouse",level:3},{value:"2. Create a Table",id:"2-create-a-table",level:3},{value:"3. Write Data",id:"3-write-data",level:3},{value:"4. Read Data",id:"4-read-data",level:3},{value:"Reference Information",id:"reference-information",level:2},{value:"Trust Policy Example",id:"trust-policy-example",level:3},{value:"Namespace and Configuration Requirements",id:"namespace-and-configuration-requirements",level:3},{value:"Public access of Iceberg Tables",id:"public-access-of-iceberg-tables",level:3},{value:"DataHub Iceberg CLI",id:"datahub-iceberg-cli",level:3},{value:"Migrating from Other Iceberg Catalogs",id:"migrating-from-other-iceberg-catalogs",level:3},{value:"Security and Permissions",id:"security-and-permissions",level:3},{value:"Iceberg tables in DataHub",id:"iceberg-tables-in-datahub",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Q: How do I verify my warehouse was created successfully?",id:"q-how-do-i-verify-my-warehouse-was-created-successfully",level:3},{value:"Q: What permissions are needed for S3 access?",id:"q-what-permissions-are-needed-for-s3-access",level:3},{value:"Q: How does my compute engine authenticate with DataHub?",id:"q-how-does-my-compute-engine-authenticate-with-datahub",level:3},{value:"Q: What should I do if table creation fails?",id:"q-what-should-i-do-if-table-creation-fails",level:3},{value:"Known Limitations",id:"known-limitations",level:2},{value:"Future Enhancements",id:"future-enhancements",level:2},{value:"Related Documentation",id:"related-documentation",level:2}],y={toc:m},b="wrapper";function h(e){var{components:a}=e,t=c(e,["components"]);return(0,n.yg)(b,s(function(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{},n=Object.keys(t);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(t).filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable})))),n.forEach((function(a){o(e,a,t[a])}))}return e}({},y,t),{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"datahub-iceberg-catalog"},"DataHub Iceberg Catalog"),(0,n.yg)(r.A,{mdxType:"FeatureAvailability"}),(0,n.yg)("blockquote",null,(0,n.yg)("p",{parentName:"blockquote"},"Note that this feature is currently in open ",(0,n.yg)("strong",{parentName:"p"},"Beta"),". With any questions or issues, please reach out to your DataHub\nrepresentative.")),(0,n.yg)("blockquote",null,(0,n.yg)("p",{parentName:"blockquote"},"Open Source DataHub: 1.0.0")),(0,n.yg)("h2",{id:"introduction"},"Introduction"),(0,n.yg)("p",null,"DataHub Iceberg Catalog provides integration with Apache Iceberg, allowing you to manage Iceberg tables through DataHub. This tutorial walks through setting up and using the DataHub Iceberg Catalog to create, read, and manage Iceberg tables. The catalog provides a secure way to manage Iceberg tables through DataHub's permissions model while also enabling discovery use-cases for humans instantly."),(0,n.yg)("h2",{id:"use-cases"},"Use Cases"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Create and manage Iceberg tables through DataHub"),(0,n.yg)("li",{parentName:"ul"},"Maintain consistent metadata across DataHub and Iceberg"),(0,n.yg)("li",{parentName:"ul"},"Facilitate data discovery by exposing Iceberg table metadata in DataHub"),(0,n.yg)("li",{parentName:"ul"},"Enable secure access to Iceberg tables through DataHub's permissions model")),(0,n.yg)("h2",{id:"conceptual-mapping"},"Conceptual Mapping"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Iceberg Concept"),(0,n.yg)("th",{parentName:"tr",align:null},"DataHub Concept"),(0,n.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Overall platform"),(0,n.yg)("td",{parentName:"tr",align:null},"dataPlatform"),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"iceberg")," platform")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Warehouse"),(0,n.yg)("td",{parentName:"tr",align:null},"dataPlatformInstance"),(0,n.yg)("td",{parentName:"tr",align:null},"Stores info such as storage credentials")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Namespace"),(0,n.yg)("td",{parentName:"tr",align:null},"container"),(0,n.yg)("td",{parentName:"tr",align:null},'Hierarchical containers of subtype "Namespace"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Tables, Views"),(0,n.yg)("td",{parentName:"tr",align:null},"dataset"),(0,n.yg)("td",{parentName:"tr",align:null},"Dataset URN is UUID that persists across renames")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Table/View Names"),(0,n.yg)("td",{parentName:"tr",align:null},"platformResource"),(0,n.yg)("td",{parentName:"tr",align:null},"Points to dataset, changes with rename operations")))),(0,n.yg)("h2",{id:"prerequisites"},"Prerequisites"),(0,n.yg)("p",null,"Before starting, ensure you have:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"DataHub installed and running locally, and ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub")," cli is installed and setup to point to it.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"AWS credentials and appropriate permissions configured.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"The following environment variables set:"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-bash"},'DH_ICEBERG_CLIENT_ID="your_client_id"\nDH_ICEBERG_CLIENT_SECRET="your_client_secret"\nDH_ICEBERG_AWS_ROLE="arn:aws:iam::123456789012:role/your-role-name"  # Example format\nDH_ICEBERG_DATA_ROOT="s3://your-bucket/path"\n\n')),(0,n.yg)("p",{parentName:"li"},"The ",(0,n.yg)("inlineCode",{parentName:"p"},"DH_ICEBERG_CLIENT_ID")," is the ",(0,n.yg)("inlineCode",{parentName:"p"},"AWS_ACCESS_KEY_ID")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"DH_ICEBERG_CLIENT_SECRET")," is the ",(0,n.yg)("inlineCode",{parentName:"p"},"AWS_SECRET_ACCESS_KEY"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"If using pyiceberg, configure pyiceberg to use your local datahub using one of its supported ways. For example, create ",(0,n.yg)("inlineCode",{parentName:"p"},"~/.pyiceberg.yaml")," with"))),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-commandline"},"catalog:\n  local_datahub:\n    uri: http://localhost:8080/iceberg\n    warehouse: arctic_warehouse\n")),(0,n.yg)("p",null,"Note:\nThe python code snippets in this tutorial are based on the code available in the ",(0,n.yg)("inlineCode",{parentName:"p"},"metadata-ingestion/examples/iceberg")," folder of the DataHub repository. These snippets require ",(0,n.yg)("inlineCode",{parentName:"p"},"pyiceberg[duckdb] >=0.8.1")," to be installed.\nFor the spark examples, the tested version of spark is 3.5.3_2.12"),(0,n.yg)("h3",{id:"required-aws-permissions"},"Required AWS Permissions"),(0,n.yg)("p",null,"The AWS role must have read and write permissions for the S3 location specified in ",(0,n.yg)("inlineCode",{parentName:"p"},"DH_ICEBERG_DATA_ROOT"),"."),(0,n.yg)("p",null,"Note: These permissions must be granted for the specific S3 bucket and path prefix where your Iceberg tables will be stored (as specified in ",(0,n.yg)("inlineCode",{parentName:"p"},"DH_ICEBERG_DATA_ROOT"),"). Additionally, the role must have a trust policy that allows it to be assumed using the AWS credentials provided in ",(0,n.yg)("inlineCode",{parentName:"p"},"DH_ICEBERG_CLIENT_ID")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"DH_ICEBERG_CLIENT_SECRET"),"."),(0,n.yg)("h2",{id:"setup-instructions"},"Setup Instructions"),(0,n.yg)("h3",{id:"1-provision-a-warehouse"},"1. Provision a Warehouse"),(0,n.yg)("p",null,"Create an Iceberg warehouse in DataHub"),(0,n.yg)(l.A,{mdxType:"Tabs"},(0,n.yg)(i.A,{value:"cli",label:"CLI",default:!0,mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},'datahub iceberg create -w arctic_warehouse -d $DH_ICEBERG_DATA_ROOT -i $DH_ICEBERG_CLIENT_ID --client_secret $DH_ICEBERG_CLIENT_SECRET --region "us-east-1" --role $DH_ICEBERG_AWS_ROLE\n'))),(0,n.yg)(i.A,{value:"python",label:"Python (pyiceberg)",mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'# File: provision_warehouse.py\nimport os\n\nfrom constants import warehouse\n\n# Assert that env variables are present\n\nassert os.environ.get("DH_ICEBERG_CLIENT_ID"), (\n   "DH_ICEBERG_CLIENT_ID variable is not present"\n)\nassert os.environ.get("DH_ICEBERG_CLIENT_SECRET"), (\n   "DH_ICEBERG_CLIENT_SECRET variable is not present"\n)\nassert os.environ.get("DH_ICEBERG_AWS_ROLE"), (\n   "DH_ICEBERG_AWS_ROLE variable is not present"\n)\nassert os.environ.get("DH_ICEBERG_DATA_ROOT"), (\n   "DH_ICEBERG_DATA_ROOT variable is not present"\n)\n\nassert os.environ.get("DH_ICEBERG_DATA_ROOT", "").startswith("s3://")\n\nos.system(\n   f"datahub iceberg create --warehouse {warehouse} --data_root $DH_ICEBERG_DATA_ROOT/{warehouse} --client_id $DH_ICEBERG_CLIENT_ID --client_secret $DH_ICEBERG_CLIENT_SECRET --region \'us-east-1\' --role $DH_ICEBERG_AWS_ROLE"\n)\n')))),(0,n.yg)("p",null,"After provisioning the warehouse, ensure your DataHub user has the following privileges to the resource type Data Platform Instance, which were introduced with Iceberg support:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATA_MANAGE_VIEWS_PRIVILEGE")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATA_MANAGE_TABLES_PRIVILEGE")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATA_MANAGE_NAMESPACES_PRIVILEGE")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATA_LIST_ENTITIES_PRIVILEGE"))),(0,n.yg)("p",null,"You can grant these privileges through the DataHub UI under the Policies section."),(0,n.yg)("h3",{id:"2-create-a-table"},"2. Create a Table"),(0,n.yg)("p",null,"You can create Iceberg tables using PyIceberg with a defined schema. Here's an example creating a ski resort metrics table:"),(0,n.yg)(l.A,{mdxType:"Tabs"},(0,n.yg)(i.A,{value:"spark",label:"spark-sql",default:!0,mdxType:"TabItem"},(0,n.yg)("p",null,"Connect to the DataHub Iceberg Catalog using Spark SQL by defining ",(0,n.yg)("inlineCode",{parentName:"p"},"$GMS_HOST"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"$GMS_PORT"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"$WAREHOUSE")," to connect to and ",(0,n.yg)("inlineCode",{parentName:"p"},"$USER_PAT")," - the DataHub Personal Access Token used to connect to the catalog.",(0,n.yg)("br",{parentName:"p"}),"\n","When using DataHub Cloud, the Iceberg Catalog URL is ",(0,n.yg)("inlineCode",{parentName:"p"},"https://<your-instance>.acryl.io/gms/iceberg/"),"\nIf you're running DataHub locally, set ",(0,n.yg)("inlineCode",{parentName:"p"},"GMS_HOST")," to ",(0,n.yg)("inlineCode",{parentName:"p"},"localhost")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"GMS_PORT")," to ",(0,n.yg)("inlineCode",{parentName:"p"},"8080"),"."),(0,n.yg)("p",null,"For this example, set ",(0,n.yg)("inlineCode",{parentName:"p"},"WAREHOUSE")," to ",(0,n.yg)("inlineCode",{parentName:"p"},"arctic_warehouse")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-cli"},"\nspark-sql --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1,org.apache.iceberg:iceberg-aws-bundle:1.6.1 \\\n    --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\\n    --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \\\n    --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \\\n    --conf spark.sql.catalog.local.type=rest \\\n    --conf spark.sql.catalog.local.uri=http://$GMS_HOST:$GMS_PORT/iceberg/ \\\n    --conf spark.sql.catalog.local.warehouse=$WAREHOUSE \\\n    --conf spark.sql.catalog.local.token=$USER_PAT \\\n    --conf spark.sql.catalog.local.rest-metrics-reporting-enabled=false \\\n    --conf spark.sql.catalog.local.header.X-Iceberg-Access-Delegation=vended-credentials \\\n    --conf spark.sql.defaultCatalog=local\n\n")),(0,n.yg)("p",null,"Use the following SQL via spark-sql"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-sql"},"CREATE NAMESPACE alpine_db;\nCREATE TABLE alpine_db.ski_resorts (\n    resort_id BIGINT NOT NULL COMMENT 'Unique identifier for each ski resort',\n    resort_name STRING NOT NULL COMMENT 'Official name of the ski resort',\n    daily_snowfall BIGINT COMMENT 'Amount of new snow in inches during the last 24 hours',\n    conditions STRING COMMENT 'Current snow conditions description',\n    last_updated TIMESTAMP COMMENT 'Timestamp of when the snow report was last updated'\n);\n"))),(0,n.yg)(i.A,{value:"python",label:"Python (pyiceberg)",mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from constants import namespace, table_name, warehouse\n\nfrom pyiceberg.schema import Schema\nfrom pyiceberg.types import LongType, NestedField, StringType, TimestampType\nfrom pyiceberg.catalog import load_catalog\nfrom datahub.ingestion.graph.client import get_default_graph\n\n# Get DataHub graph client for authentication\ngraph = get_default_graph()\n\n# Define schema with documentation\nschema = Schema(\n    NestedField(\n        field_id=1,\n        name="resort_id",\n        field_type=LongType(),\n        required=True,\n        doc="Unique identifier for each ski resort"\n    ),\n    NestedField(\n        field_id=2,\n        name="resort_name",\n        field_type=StringType(),\n        required=True,\n        doc="Official name of the ski resort"\n    ),\n    NestedField(\n        field_id=3,\n        name="daily_snowfall",\n        field_type=LongType(),\n        required=False,\n        doc="Amount of new snow in inches during the last 24 hours"\n    ),\n    NestedField(\n        field_id=4,\n        name="conditions",\n        field_type=StringType(),\n        required=False,\n        doc="Current snow conditions description"\n    ),\n    NestedField(\n        field_id=5,\n        name="last_updated",\n        field_type=TimestampType(),\n        required=False,\n        doc="Timestamp of when the snow report was last updated"\n    )\n)\n\n# Load catalog and create table\ncatalog = load_catalog("local_datahub", warehouse=warehouse, token=graph.config.token)\ncatalog.create_namespace(namespace)\ncatalog.create_table(f"{namespace}.{table_name}", schema)\n')))),(0,n.yg)("h3",{id:"3-write-data"},"3. Write Data"),(0,n.yg)(l.A,{mdxType:"Tabs"},(0,n.yg)(i.A,{value:"spark",label:"spark-sql",default:!0,mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO alpine_db.ski_resorts (resort_id, resort_name, daily_snowfall, conditions, last_updated)\nVALUES\n    (1, 'Snowpeak Resort', 12, 'Powder', CURRENT_TIMESTAMP()),\n    (2, 'Alpine Valley', 8, 'Packed', CURRENT_TIMESTAMP()),\n    (3, 'Glacier Heights', 15, 'Fresh Powder', CURRENT_TIMESTAMP());\n"))),(0,n.yg)(i.A,{value:"python",label:"Python (pyiceberg)",mdxType:"TabItem"},(0,n.yg)("p",null,"You can write data to your Iceberg table using PyArrow. Note the importance of matching the schema exactly:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from constants import namespace, table_name, warehouse\nfrom pyiceberg.catalog import load_catalog\nfrom datahub.ingestion.graph.client import get_default_graph\n\nimport pyarrow as pa\nfrom datetime import datetime\n\ngraph = get_default_graph()\ncatalog = load_catalog("local_datahub", warehouse=warehouse, token=graph.config.token)\n\n# Create PyArrow schema to match Iceberg schema\npa_schema = pa.schema([\n    ("resort_id", pa.int64(), False),  # False means not nullable\n    ("resort_name", pa.string(), False),\n    ("daily_snowfall", pa.int64(), True),\n    ("conditions", pa.string(), True),\n    ("last_updated", pa.timestamp("us"), True),\n])\n\n# Create sample data\nsample_data = pa.Table.from_pydict(\n    {\n        "resort_id": [1, 2, 3],\n        "resort_name": ["Snowpeak Resort", "Alpine Valley", "Glacier Heights"],\n        "daily_snowfall": [12, 8, 15],\n        "conditions": ["Powder", "Packed", "Fresh Powder"],\n        "last_updated": [\n            datetime.now(),\n            datetime.now(),\n            datetime.now()\n        ]\n    },\n    schema=pa_schema\n)\n\n# Write to table\ntable = catalog.load_table(f"{namespace}.{table_name}")\ntable.overwrite(sample_data)\n\n# Refresh table to see changes\ntable.refresh()\n')))),(0,n.yg)("h3",{id:"4-read-data"},"4. Read Data"),(0,n.yg)(l.A,{mdxType:"Tabs"},(0,n.yg)(i.A,{value:"spark",label:"spark-sql",default:!0,mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-sql"},"SELECT * from alpine_db.resort_metrics;\n"))),(0,n.yg)(i.A,{value:"python",label:"Python (pyiceberg)",mdxType:"TabItem"},(0,n.yg)("p",null,"Reading data from an Iceberg table using DuckDB integration:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from pyiceberg.catalog import load_catalog\nfrom constants import namespace, table_name, warehouse\n\nfrom datahub.ingestion.graph.client import get_default_graph\n\n# Get DataHub graph client for authentication\ngraph = get_default_graph()\n\ncatalog = load_catalog("local_datahub", warehouse=warehouse, token=graph.config.token)\ntable = catalog.load_table(f"{namespace}.{table_name}")\ncon = table.scan().to_duckdb(table_name=table_name)\n\n# Query the data\nprint("\\nResort Metrics Data:")\nprint("-" * 50)\nfor row in con.execute(f"SELECT * FROM {table_name}").fetchall():\n    print(row)\n')))),(0,n.yg)("h2",{id:"reference-information"},"Reference Information"),(0,n.yg)("h3",{id:"trust-policy-example"},"Trust Policy Example"),(0,n.yg)("p",null,"When setting up your AWS role, you'll need to configure a trust policy. Here's an example:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "Version": "2012-10-17",\n  "Statement": [\n    {\n      "Effect": "Allow",\n      "Principal": {\n        "AWS": "arn:aws:iam::123456789012:user/iceberg-user" // The IAM user or role associated with your credentials\n      },\n      "Action": "sts:AssumeRole"\n    }\n  ]\n}\n')),(0,n.yg)("h3",{id:"namespace-and-configuration-requirements"},"Namespace and Configuration Requirements"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Namespace Creation"),": A namespace must be created before any tables can be created within it.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Spark SQL Configuration"),": To simplify SQL statements by avoiding the need to prefix catalog name and namespace, you can set defaults in your Spark configuration:"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre"},"--conf spark.sql.defaultCatalog=<default-catalog-name> \\\n--conf spark.sql.catalog.local.default-namespace=<default-namespace>\n")))),(0,n.yg)("h3",{id:"public-access-of-iceberg-tables"},"Public access of Iceberg Tables"),(0,n.yg)("p",null,"It is possible to enable public read-only access of specific Iceberg tables if needed.\nEnabling public access requires the following steps."),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Ensure that the DATA ROOT folder in s3 has public read access policy set to enable the files with that prefix to be read without AWS credentials.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Update the GMS Configuration to enable public access, ensure the GMS service is run with the following environment variables set."),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Set the env var ",(0,n.yg)("inlineCode",{parentName:"li"},"ENABLE_PUBLIC_READ")," to ",(0,n.yg)("inlineCode",{parentName:"li"},"true")," to enable the capability. If unset, this is by default ",(0,n.yg)("inlineCode",{parentName:"li"},"false"),"."),(0,n.yg)("li",{parentName:"ul"},"Set the env var ",(0,n.yg)("inlineCode",{parentName:"li"},"PUBLICLY_READABLE_TAG")," to a specific Tag name that indicates public access when applied to an Iceberg DataSet. If unset, this defaults to the tag ",(0,n.yg)("inlineCode",{parentName:"li"},"PUBLICLY_READABLE"))),(0,n.yg)("p",{parentName:"li"},"Alternatively, these can be set in metadata-service/configuration/src/main/resources/application.yaml under ",(0,n.yg)("inlineCode",{parentName:"p"},"icebergCatalog")," key. The defaults are populated under that key.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Once GMS is started with enabling the public read capability, apply the Tag defined for public access on each Dataset that should be accessible without authentication."))),(0,n.yg)("p",null,"To access these tables that have public access, start the spark-sql with the following settings"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-commandline"},"spark-sql --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1,org.apache.iceberg:iceberg-aws-bundle:1.6.1\\\n    --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\\n    --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \\\n    --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \\\n    --conf spark.sql.catalog.local.type=rest \\\n    --conf spark.sql.catalog.local.uri=http://${GMS_HOST}:${GMS_PORT}/public-iceberg/ \\\n    --conf spark.sql.catalog.local.warehouse=arctic_warehouse \\\n    --conf spark.sql.catalog.local.header.X-Iceberg-Access-Delegation=false \\\n    --conf spark.sql.catalog.local.rest-metrics-reporting-enabled=false \\\n    --conf spark.sql.catalog.local.client.region=us-east-1 \\\n    --conf spark.sql.catalog.local.client.credentials-provider=software.amazon.awssdk.auth.credentials.AnonymousCredentialsProvider \\\n    --conf spark.sql.defaultCatalog=local \\\n    --conf spark.sql.catalog.local.default-namespace=alpine_db\n")),(0,n.yg)("p",null,"Note the specific differences from the authenticated access:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"The REST Catalog URI is gms_host:port/",(0,n.yg)("b",null,"public-iceberg/")),(0,n.yg)("li",{parentName:"ul"},"The DATA ROOT AWS s3 region is specified via ",(0,n.yg)("inlineCode",{parentName:"li"},"spark.sql.catalog.local.client.region")),(0,n.yg)("li",{parentName:"ul"},"The ",(0,n.yg)("inlineCode",{parentName:"li"},"X-Iceberg-Access-Delegation")," header is set to ",(0,n.yg)("b",null,(0,n.yg)("inlineCode",{parentName:"li"},"false"))," instead of ",(0,n.yg)("inlineCode",{parentName:"li"},"vended-credentials")),(0,n.yg)("li",{parentName:"ul"},"The ",(0,n.yg)("inlineCode",{parentName:"li"},"credentials-provider")," is set to ",(0,n.yg)("inlineCode",{parentName:"li"},"credentials-provider=software.amazon.awssdk.auth.credentials.AnonymousCredentialsProvider")," and no Personal Access Token is provided.")),(0,n.yg)("p",null,"In such unauthenticated sessions, attempts to access tables that do not have access will fail with a NoSuchTableException error instead of an authorization failure."),(0,n.yg)("h3",{id:"datahub-iceberg-cli"},"DataHub Iceberg CLI"),(0,n.yg)("p",null,"The DataHub CLI provides several commands for managing Iceberg warehouses:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"List Warehouses:"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre"},"datahub iceberg list\n"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Update Warehouse Configuration:"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre"},"datahub iceberg update \\\n  -w $WAREHOUSE_NAME \\\n  -d $DH_ICEBERG_DATA_ROOT \\\n  -i $DH_ICEBERG_CLIENT_ID \\\n  --client_secret $DH_ICEBERG_CLIENT_ID \\\n  --region $DH_ICEBERG_REGION \\\n  --role DH_ICEBERG_AWS_ROLE\n"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Delete Warehouse:"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre"},"datahub iceberg delete -w $WAREHOUSE_NAME\n")),(0,n.yg)("p",{parentName:"li"},"Note: This command deletes Containers associated with Namespaces and Datasets associated with Tables and Views in this warehouse. However, it does not delete any backing data in the warehouse data_root location - it only deletes entries from the catalog."))),(0,n.yg)("p",null,"For additional options and help, run:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"datahub iceberg [command] --help\n")),(0,n.yg)("h3",{id:"migrating-from-other-iceberg-catalogs"},"Migrating from Other Iceberg Catalogs"),(0,n.yg)("p",null,"When migrating from another Iceberg catalog, you can register existing Iceberg tables using the ",(0,n.yg)("inlineCode",{parentName:"p"},"system.register_table")," command. This allows you to start managing existing Iceberg tables through DataHub without moving the underlying data."),(0,n.yg)("p",null,"Example of registering an existing table:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"call system.register_table('myTable', 's3://my-s3-bucket/my-data-root/myNamespace/myTable/metadata/00000-f9dbba67-df4f-4742-9ba5-123aa2bb4076.metadata.json');\n\n-- Read from newly registered table\nselect * from myTable;\n")),(0,n.yg)("h3",{id:"security-and-permissions"},"Security and Permissions"),(0,n.yg)("p",null,"DataHub provides granular access control for Iceberg operations through policies. The following privileges were introduced with Iceberg support:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Operation"),(0,n.yg)("th",{parentName:"tr",align:null},"Required Privilege"),(0,n.yg)("th",{parentName:"tr",align:null},"Resource Type"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"CREATE or DROP namespaces"),(0,n.yg)("td",{parentName:"tr",align:null},"Manage Namespaces"),(0,n.yg)("td",{parentName:"tr",align:null},"Data Platform Instance")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"CREATE, ALTER or DROP tables"),(0,n.yg)("td",{parentName:"tr",align:null},"Manage Tables"),(0,n.yg)("td",{parentName:"tr",align:null},"Data Platform Instance")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"CREATE, ALTER or DROP views"),(0,n.yg)("td",{parentName:"tr",align:null},"Manage Views"),(0,n.yg)("td",{parentName:"tr",align:null},"Data Platform Instance")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"SELECT from tables or views"),(0,n.yg)("td",{parentName:"tr",align:null},"Read Only data-access"),(0,n.yg)("td",{parentName:"tr",align:null},"Dataset")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"INSERT, UPDATE, DELETE or ALTER tables"),(0,n.yg)("td",{parentName:"tr",align:null},"Read-write data-access"),(0,n.yg)("td",{parentName:"tr",align:null},"Dataset")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"List tables or views"),(0,n.yg)("td",{parentName:"tr",align:null},"List tables, views and namespaces"),(0,n.yg)("td",{parentName:"tr",align:null},"Data Platform Instance")))),(0,n.yg)("p",null,"To configure access:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Create policies with the appropriate privileges for Iceberg users"),(0,n.yg)("p",null,(0,n.yg)("img",{width:"70%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/cec184aa1e3cb15c087625ffc997b4345a858c8b/imgs/iceberg-policy-type.png"}))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Scope the policies by:"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Selecting specific warehouse Data Platform Instances for namespace, table management, and listing privileges"),(0,n.yg)("li",{parentName:"ul"},"Selecting specific DataSets for table and view data access privileges"),(0,n.yg)("li",{parentName:"ul"},"Selecting Tags that may be applied to DataSets or Data Platform Instances")))),(0,n.yg)("p",null,(0,n.yg)("img",{width:"70%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/cec184aa1e3cb15c087625ffc997b4345a858c8b/imgs/iceberg-policy-privileges.png"})),(0,n.yg)("ol",{start:3},(0,n.yg)("li",{parentName:"ol"},"Assign the policies to relevant users or groups",(0,n.yg)("p",null,(0,n.yg)("img",{width:"70%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/cec184aa1e3cb15c087625ffc997b4345a858c8b/imgs/iceberg-policy-users-groups.png"})))),(0,n.yg)("h3",{id:"iceberg-tables-in-datahub"},"Iceberg tables in DataHub"),(0,n.yg)("p",null,"Once you create tables in iceberg, each of those tables show up in DataHub as a DataSet"),(0,n.yg)("p",null,(0,n.yg)("img",{width:"70%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/cec184aa1e3cb15c087625ffc997b4345a858c8b/imgs/iceberg-dataset-schema.png"})),(0,n.yg)("h2",{id:"troubleshooting"},"Troubleshooting"),(0,n.yg)("h3",{id:"q-how-do-i-verify-my-warehouse-was-created-successfully"},"Q: How do I verify my warehouse was created successfully?"),(0,n.yg)("p",null,"A: Check the DataHub UI under Data Platform Instances, or try creating and reading a table using the provided scripts."),(0,n.yg)("h3",{id:"q-what-permissions-are-needed-for-s3-access"},"Q: What permissions are needed for S3 access?"),(0,n.yg)("p",null,"A: The role specified in ",(0,n.yg)("inlineCode",{parentName:"p"},"DH_ICEBERG_AWS_ROLE")," should have permissions to read and write to the S3 bucket specified in ",(0,n.yg)("inlineCode",{parentName:"p"},"DH_ICEBERG_DATA_ROOT"),"."),(0,n.yg)("h3",{id:"q-how-does-my-compute-engine-authenticate-with-datahub"},"Q: How does my compute engine authenticate with DataHub?"),(0,n.yg)("p",null,"A: A DataHub Personal Access Token can be used via bearer token to authenticate with DataHub."),(0,n.yg)("h3",{id:"q-what-should-i-do-if-table-creation-fails"},"Q: What should I do if table creation fails?"),(0,n.yg)("p",null,"A: Check that:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"All required environment variables are set correctly"),(0,n.yg)("li",{parentName:"ol"},"Your AWS role has the necessary permissions"),(0,n.yg)("li",{parentName:"ol"},"The namespace exists (create it if needed)"),(0,n.yg)("li",{parentName:"ol"},"The table name doesn't already exist")),(0,n.yg)("h2",{id:"known-limitations"},"Known Limitations"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"AWS S3 only support"),(0,n.yg)("li",{parentName:"ul"},"Concurrency - supports single instance of GMS"),(0,n.yg)("li",{parentName:"ul"},"Multi-table transactional ",(0,n.yg)("inlineCode",{parentName:"li"},"/commit")," is not yet supported"),(0,n.yg)("li",{parentName:"ul"},"Table operation purge is not yet implemented"),(0,n.yg)("li",{parentName:"ul"},"Metric collection and credential refresh mechanisms are still in development")),(0,n.yg)("h2",{id:"future-enhancements"},"Future Enhancements"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Improved concurrency with multiple replicas of GMS"),(0,n.yg)("li",{parentName:"ul"},"Support for Iceberg multi-table transactional ",(0,n.yg)("inlineCode",{parentName:"li"},"/commit")),(0,n.yg)("li",{parentName:"ul"},"Additional table APIs (scan, drop table purge)"),(0,n.yg)("li",{parentName:"ul"},"Azure and GCP Support"),(0,n.yg)("li",{parentName:"ul"},"Namespace permissions"),(0,n.yg)("li",{parentName:"ul"},"Enhanced metrics"),(0,n.yg)("li",{parentName:"ul"},"Credential refresh"),(0,n.yg)("li",{parentName:"ul"},"Proxy to another REST Catalog to use its capabilities while DataHub has real-time metadata updates.")),(0,n.yg)("h2",{id:"related-documentation"},"Related Documentation"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://iceberg.apache.org/"},"Apache Iceberg Documentation")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://py.iceberg.apache.org/"},"PyIceberg Documentation")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"/docs/"},"DataHub Documentation"))))}h.isMDXComponent=!0}}]);