"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[16767],{15680:(e,a,t)=>{t.d(a,{xA:()=>u,yg:()=>m});var n=t(96540);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function l(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function o(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=n.createContext({}),d=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):l(l({},a),e)),t},u=function(e){var a=d(e.components);return n.createElement(s.Provider,{value:a},e.children)},p="mdxType",g={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},c=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),p=d(t),c=r,m=p["".concat(s,".").concat(c)]||p[c]||g[c]||i;return t?n.createElement(m,l(l({ref:a},u),{},{components:t})):n.createElement(m,l({ref:a},u))}));function m(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var i=t.length,l=new Array(i);l[0]=c;var o={};for(var s in a)hasOwnProperty.call(a,s)&&(o[s]=a[s]);o.originalType=e,o[p]="string"==typeof e?e:r,l[1]=o;for(var d=2;d<i;d++)l[d]=t[d];return n.createElement.apply(null,l)}return n.createElement.apply(null,t)}c.displayName="MDXCreateElement"},18804:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>u,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>d,toc:()=>p});t(96540);var n=t(15680);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){return a=null!=a?a:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):function(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))})),e}function l(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}const o={toc_max_heading_level:4,title:"DataHub CLI",sidebar_label:"CLI",slug:"/cli",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/cli.md"},s="DataHub CLI",d={unversionedId:"docs/cli",id:"docs/cli",title:"DataHub CLI",description:"DataHub comes with a friendly cli called datahub that allows you to perform a lot of common operations using just the command line. DataHub maintains the pypi package for datahub.",source:"@site/genDocs/docs/cli.md",sourceDirName:"docs",slug:"/cli",permalink:"/docs/cli",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/cli.md",tags:[],version:"current",frontMatter:{toc_max_heading_level:4,title:"DataHub CLI",sidebar_label:"CLI",slug:"/cli",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/cli.md"},sidebar:"overviewSidebar",previous:{title:"Design Principles of Java SDK V2",permalink:"/docs/metadata-integration/java/docs/sdk-v2/design-principles"},next:{title:"Dataset Command",permalink:"/docs/cli-commands/dataset"}},u={},p=[{value:"Installation",id:"installation",level:2},{value:"Using pip",id:"using-pip",level:3},{value:"Starter Commands",id:"starter-commands",level:2},{value:"docker",id:"docker",level:3},{value:"version",id:"version",level:3},{value:"ingest",id:"ingest",level:3},{value:"ingest --dry-run",id:"ingest---dry-run",level:4},{value:"ingest list-source-runs",id:"ingest-list-source-runs",level:4},{value:"ingest --preview",id:"ingest---preview",level:4},{value:"ingest --no-default-report",id:"ingest---no-default-report",level:4},{value:"ingest --record (Beta)",id:"ingest---record-beta",level:4},{value:"ingest replay (Beta)",id:"ingest-replay-beta",level:3},{value:"recording (Beta)",id:"recording-beta",level:3},{value:"recording info",id:"recording-info",level:4},{value:"recording extract",id:"recording-extract",level:4},{value:"recording list",id:"recording-list",level:4},{value:"ingest deploy",id:"ingest-deploy",level:3},{value:"Command Options",id:"command-options",level:4},{value:"Examples",id:"examples",level:4},{value:"Remote Executors",id:"remote-executors",level:4},{value:"Using deployment section",id:"using-deployment-section",level:4},{value:"Deployment Configuration Options",id:"deployment-configuration-options",level:4},{value:"Batch Deployment",id:"batch-deployment",level:4},{value:"init",id:"init",level:3},{value:"Interactive Mode",id:"interactive-mode",level:4},{value:"Non-Interactive Mode with Username/Password",id:"non-interactive-mode-with-usernamepassword",level:4},{value:"DataHub Cloud Example",id:"datahub-cloud-example",level:4},{value:"Environment variables supported",id:"environment-variables-supported",level:4},{value:"container",id:"container",level:3},{value:"check",id:"check",level:3},{value:"restore-indices",id:"restore-indices",level:4},{value:"get-kafka-consumer-offsets",id:"get-kafka-consumer-offsets",level:4},{value:"delete",id:"delete",level:3},{value:"exists",id:"exists",level:3},{value:"get",id:"get",level:3},{value:"graphql",id:"graphql",level:3},{value:"put",id:"put",level:3},{value:"put aspect",id:"put-aspect",level:4},{value:"put platform",id:"put-platform",level:4},{value:"timeline",id:"timeline",level:3},{value:"Entity Specific Commands",id:"entity-specific-commands",level:2},{value:"dataset (Dataset Entity)",id:"dataset-dataset-entity",level:3},{value:"user (User Entity)",id:"user-user-entity",level:3},{value:"upsert",id:"upsert",level:4},{value:"add",id:"add",level:4},{value:"group (Group Entity)",id:"group-group-entity",level:3},{value:"dataproduct (Data Product Entity)",id:"dataproduct-data-product-entity",level:3},{value:"upsert",id:"upsert-1",level:4},{value:"update",id:"update",level:4},{value:"diff",id:"diff",level:4},{value:"get",id:"get-1",level:4},{value:"add_asset",id:"add_asset",level:4},{value:"remove_asset",id:"remove_asset",level:4},{value:"add_owner",id:"add_owner",level:4},{value:"remove_owner",id:"remove_owner",level:4},{value:"set_description",id:"set_description",level:4},{value:"delete",id:"delete-1",level:4},{value:"Miscellaneous Admin Commands",id:"miscellaneous-admin-commands",level:2},{value:"lite (experimental)",id:"lite-experimental",level:3},{value:"telemetry",id:"telemetry",level:3},{value:"migrate",id:"migrate",level:3},{value:"dataplatform2instance",id:"dataplatform2instance",level:4},{value:"Dry Run",id:"dry-run",level:5},{value:"Real Migration (with soft-delete)",id:"real-migration-with-soft-delete",level:5},{value:"Alternate Installation Options",id:"alternate-installation-options",level:2},{value:"Using docker",id:"using-docker",level:3},{value:"Install from source",id:"install-from-source",level:3},{value:"Installing Plugins",id:"installing-plugins",level:2},{value:"Sources",id:"sources",level:3},{value:"Debug/Utility Plugins",id:"debugutility-plugins",level:3},{value:"Sinks",id:"sinks",level:3},{value:"Check the active plugins",id:"check-the-active-plugins",level:3},{value:"Release Notes and CLI versions",id:"release-notes-and-cli-versions",level:2}],g={toc:p},c="wrapper";function m(e){var{components:a}=e,t=l(e,["components"]);return(0,n.yg)(c,i(function(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{},n=Object.keys(t);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(t).filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable})))),n.forEach((function(a){r(e,a,t[a])}))}return e}({},g,t),{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"datahub-cli"},"DataHub CLI"),(0,n.yg)("p",null,"DataHub comes with a friendly cli called ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub")," that allows you to perform a lot of common operations using just the command line. ",(0,n.yg)("a",{parentName:"p",href:"https://datahub.com"},"DataHub")," maintains the ",(0,n.yg)("a",{parentName:"p",href:"https://pypi.org/project/acryl-datahub/"},"pypi package")," for ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub"),"."),(0,n.yg)("h2",{id:"installation"},"Installation"),(0,n.yg)("h3",{id:"using-pip"},"Using pip"),(0,n.yg)("p",null,"We recommend Python virtual environments (venv-s) to namespace pip modules. Here's an example setup:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"python3 -m venv venv             # create the environment\nsource venv/bin/activate         # activate the environment\n")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},(0,n.yg)("em",{parentName:"strong"},"NOTE:"))," If you install ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub")," in a virtual environment, that same virtual environment must be re-activated each time a shell window or session is created."),(0,n.yg)("p",null,"Once inside the virtual environment, install ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub")," using the following commands"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'# Requires Python 3.10+\npython3 -m pip install --upgrade pip wheel setuptools\npython3 -m pip install --upgrade acryl-datahub\n# validate that the install was successful\ndatahub version\n# If you see "command not found", try running this instead: python3 -m datahub version\ndatahub init\n# authenticate your datahub CLI with your datahub instance\n')),(0,n.yg)("p",null,"If you run into an error, try checking the ",(0,n.yg)("a",{parentName:"p",href:"/docs/metadata-ingestion/developing#common-setup-issues"},(0,n.yg)("em",{parentName:"a"},"common setup issues")),"."),(0,n.yg)("p",null,"Other installation options such as installation from source and running the cli inside a container are available further below in the guide ",(0,n.yg)("a",{parentName:"p",href:"#alternate-installation-options"},"here"),"."),(0,n.yg)("h2",{id:"starter-commands"},"Starter Commands"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub")," cli allows you to do many things, such as quick-starting a DataHub docker instance locally, ingesting metadata from your sources into a DataHub server or a DataHub lite instance, as well as retrieving, modifying and exploring metadata.\nLike most command line tools, ",(0,n.yg)("inlineCode",{parentName:"p"},"--help")," is your best friend. Use it to discover the capabilities of the cli and the different commands and sub-commands that are supported."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-console"},"datahub --help\nUsage: datahub [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --debug / --no-debug            Enable debug logging.\n  --log-file FILE                 Enable debug logging.\n  --debug-vars / --no-debug-vars  Show variable values in stack traces. Implies --debug. While we try to avoid\n                                  printing sensitive information like passwords, this may still happen.\n  --version                       Show the version and exit.\n  -dl, --detect-memory-leaks      Run memory leak detection.\n  --help                          Show this message and exit.\n\nCommands:\n  actions       <disabled due to missing dependencies>\n  check         Helper commands for checking various aspects of DataHub.\n  container     A group of commands to interact with containers in DataHub.\n  dataproduct   A group of commands to interact with the DataProduct entity in DataHub.\n  dataset       A group of commands to interact with the Dataset entity in DataHub.\n  delete        Delete metadata from DataHub.\n  docker        Helper commands for setting up and interacting with a local DataHub instance using Docker.\n  exists        A group of commands to check existence of entities in DataHub.\n  forms         A group of commands to interact with forms in DataHub.\n  get           A group of commands to get metadata from DataHub.\n  graphql       Execute GraphQL queries and mutations against DataHub.\n  group         A group of commands to interact with the Group entity in DataHub.\n  ingest        Ingest metadata into DataHub.\n  init          Configure which datahub instance to connect to\n  lite          A group of commands to work with a DataHub Lite instance\n  migrate       Helper commands for migrating metadata within DataHub.\n  properties    A group of commands to interact with structured properties in DataHub.\n  put           A group of commands to put metadata in DataHub.\n  state         Managed state stored in DataHub by stateful ingestion.\n  telemetry     Toggle telemetry.\n  timeline      Get timeline for an entity based on certain categories\n  user          A group of commands to interact with the User entity in DataHub.\n  version       Print version number and exit.\n")),(0,n.yg)("p",null,"The following top-level commands listed below are here mainly to give the reader a high-level picture of what are the kinds of things you can accomplish with the cli.\nWe've ordered them roughly in the order we expect you to interact with these commands as you get deeper into the ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub"),"-verse."),(0,n.yg)("h3",{id:"docker"},"docker"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"docker")," command allows you to start up a local DataHub instance using ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub docker quickstart"),". You can also check if the docker cluster is healthy using ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub docker check"),"."),(0,n.yg)("h3",{id:"version"},"version"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"version")," command allows you to get version number of CLI that you are using."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-console"},"datahub version\nDataHub CLI version: 1.2.0.9\nModels: bundled\nPython version: 3.11.11 (main, Mar 17 2025, 21:33:08) [Clang 20.1.0 ]\n")),(0,n.yg)("p",null,"You can pass ",(0,n.yg)("inlineCode",{parentName:"p"},"--include-server")," flag to include server information too. This is helpful to share cli and server version at once for debugging purposes. This does require ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub init")," to be run before that so there is a server that CLI is able to connect to."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-console"},"datahub version --include-server\nDataHub CLI version: 1.2.0.9\nModels: bundled\nPython version: 3.11.11 (main, Mar 17 2025, 21:33:08) [Clang 20.1.0 ]\nServer config: {'models': {}, 'managedIngestion': {'defaultCliVersion': '1.2.0.9', 'enabled': True}, 'timeZone': 'GMT', 'datasetUrnNameCasing': False, 'datahub': {'serverEnv': 'cloud', 'serverType': 'prod'}, 'baseUrl': 'https://xyz.acryl.io', 'patchCapable': True, 'versions': {'acryldata/datahub': {'version': 'v0.3.14rc0', 'commit': '464d94926bb21a596f3d9d81164b272c74872ce7'}}, 'statefulIngestionCapable': True, 'remoteExecutorBackend': {'revision': 2}, 'supportsImpactAnalysis': True, 'telemetry': {'enabledCli': False, 'enabledMixpanel': False, 'enabledServer': True, 'enabledIngestion': True}, 'retention': 'true', 'noCode': 'true'}\n")),(0,n.yg)("h3",{id:"ingest"},"ingest"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"ingest")," command allows you to ingest metadata from your sources using ingestion configuration files, which we call recipes.\nSource specific crawlers are provided by plugins and might sometimes need additional extras to be installed. See ",(0,n.yg)("a",{parentName:"p",href:"#installing-plugins"},"installing plugins")," for more information.\n",(0,n.yg)("a",{parentName:"p",href:"/docs/how/delete-metadata"},"Removing Metadata from DataHub")," contains detailed instructions about how you can use the ingest command to perform operations like rolling-back previously ingested metadata through the ",(0,n.yg)("inlineCode",{parentName:"p"},"rollback")," sub-command and listing all runs that happened through ",(0,n.yg)("inlineCode",{parentName:"p"},"list-runs")," sub-command."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-console"},"Usage: datahub [datahub-options] ingest [command-options]\n\nCommand Options:\n  -c / --config             Config file in .toml or .yaml format\n  -n / --dry-run            Perform a dry run of the ingestion, essentially skipping writing to sink\n  --preview                 Perform limited ingestion from the source to the sink to get a quick preview\n  --preview-workunits       The number of workunits to produce for preview\n  --strict-warnings         If enabled, ingestion runs with warnings will yield a non-zero error code\n  --test-source-connection  When set, ingestion will only test the source connection details from the recipe\n  --no-progress             If enabled, mute intermediate progress ingestion reports\n")),(0,n.yg)("h4",{id:"ingest---dry-run"},"ingest --dry-run"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"--dry-run")," option of the ",(0,n.yg)("inlineCode",{parentName:"p"},"ingest")," command performs all of the ingestion steps, except writing to the sink. This is useful to validate that the\ningestion recipe is producing the desired metadata events before ingesting them into datahub."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"# Dry run\ndatahub ingest -c ./examples/recipes/example_to_datahub_rest.dhub.yaml --dry-run\n# Short-form\ndatahub ingest -c ./examples/recipes/example_to_datahub_rest.dhub.yaml -n\n")),(0,n.yg)("h4",{id:"ingest-list-source-runs"},"ingest list-source-runs"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"list-source-runs")," option of the ",(0,n.yg)("inlineCode",{parentName:"p"},"ingest")," command lists the previous runs, displaying their run ID, source name,\nstart time, status, and source URN. This command allows you to filter results using the --urn option for URN-based\nfiltering or the --source option to filter by source name (partial or complete matches are supported)."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'# List all ingestion runs\ndatahub ingest list-source-runs\n# Filter runs by a source name containing "demo"\ndatahub ingest list-source-runs --source "demo"\n')),(0,n.yg)("h4",{id:"ingest---preview"},"ingest --preview"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"--preview")," option of the ",(0,n.yg)("inlineCode",{parentName:"p"},"ingest")," command performs all of the ingestion steps, but limits the processing to only the first 10 workunits produced by the source.\nThis option helps with quick end-to-end smoke testing of the ingestion recipe."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"# Preview\ndatahub ingest -c ./examples/recipes/example_to_datahub_rest.dhub.yaml --preview\n# Preview with dry-run\ndatahub ingest -c ./examples/recipes/example_to_datahub_rest.dhub.yaml -n --preview\n")),(0,n.yg)("p",null,"By default ",(0,n.yg)("inlineCode",{parentName:"p"},"--preview")," creates 10 workunits. But if you wish to try producing more workunits you can use another option ",(0,n.yg)("inlineCode",{parentName:"p"},"--preview-workunits")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"# Preview 20 workunits without sending anything to sink\ndatahub ingest -c ./examples/recipes/example_to_datahub_rest.dhub.yaml -n --preview --preview-workunits=20\n")),(0,n.yg)("h4",{id:"ingest---no-default-report"},"ingest --no-default-report"),(0,n.yg)("p",null,"By default, the cli sends an ingestion report to DataHub, which allows you to see the result of all cli-based ingestion in the UI. This can be turned off with the ",(0,n.yg)("inlineCode",{parentName:"p"},"--no-default-report")," flag."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"# Running ingestion with reporting to DataHub turned off\ndatahub ingest -c ./examples/recipes/example_to_datahub_rest.dhub.yaml --no-default-report\n")),(0,n.yg)("p",null,"The reports include the recipe that was used for ingestion. This can be turned off by adding an additional section to the ingestion recipe."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},"source:\n  # source configs\n\nsink:\n  # sink configs\n\n# Add configuration for the datahub reporter\nreporting:\n  - type: datahub\n    config:\n      report_recipe: false\n\n# Optional log to put failed JSONs into a file\n# Helpful in case you are trying to debug some issue with specific ingestion failing\nfailure_log:\n  enabled: false\n  log_config:\n    filename: ./path/to/failure.json\n")),(0,n.yg)("h4",{id:"ingest---record-beta"},"ingest --record (Beta)"),(0,n.yg)("admonition",{title:"Beta Feature",type:"note"},(0,n.yg)("p",{parentName:"admonition"},"Recording and replay is currently in beta. The feature is stable for debugging purposes but the archive format may change in future releases.")),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"--record")," option enables recording of all HTTP requests and database queries during ingestion. This creates an encrypted archive that can be replayed offline for debugging."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"# Record an ingestion run with password protection\ndatahub ingest -c ./recipe.yaml --record --record-password mysecret\n\n# Record to a specific local directory\nexport INGESTION_ARTIFACT_DIR=/path/to/recordings\ndatahub ingest -c ./recipe.yaml --record --record-password mysecret --no-s3-upload\n\n# Record and upload directly to S3\ndatahub ingest -c ./recipe.yaml --record --record-password mysecret \\\n    --record-output-path s3://my-bucket/recordings/my-run.zip\n")),(0,n.yg)("p",null,"Recording options:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Option"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--record")),(0,n.yg)("td",{parentName:"tr",align:null},"Enable recording of the ingestion run")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--record-password")),(0,n.yg)("td",{parentName:"tr",align:null},"Password for encrypting the archive. Can also be set via ",(0,n.yg)("inlineCode",{parentName:"td"},"DATAHUB_RECORDING_PASSWORD")," environment variable")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--record-output-path")),(0,n.yg)("td",{parentName:"tr",align:null},"Path to save the recording archive. Use local path or S3 URL (",(0,n.yg)("inlineCode",{parentName:"td"},"s3://bucket/path/file.zip"),")")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--no-s3-upload")),(0,n.yg)("td",{parentName:"tr",align:null},"Disable S3 upload (save locally only)")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--no-secret-redaction")),(0,n.yg)("td",{parentName:"tr",align:null},"Keep actual credentials in recording (use with caution, for local debugging only)")))),(0,n.yg)("p",null,"The recording creates an encrypted ZIP archive containing HTTP cassettes, database query recordings, and a redacted recipe. This archive can be replayed using ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub ingest replay"),"."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Installation:")," Recording requires the ",(0,n.yg)("inlineCode",{parentName:"p"},"debug-recording")," plugin:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"pip install 'acryl-datahub[debug-recording]'\n")),(0,n.yg)("p",null,"\u27a1\ufe0f ",(0,n.yg)("a",{parentName:"p",href:"/docs/how/debug-ingestion-recording"},"Learn more about recording and debugging ingestion")),(0,n.yg)("h3",{id:"ingest-replay-beta"},"ingest replay (Beta)"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"ingest replay")," command replays a recorded ingestion run for debugging. This allows you to reproduce issues in an air-gapped environment without network access."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"# Replay from local file\ndatahub ingest replay ./recording.zip --password mysecret\n\n# Replay from S3\ndatahub ingest replay s3://bucket/recordings/run-id.zip --password mysecret\n\n# Replay with live sink (emit to real DataHub instance)\ndatahub ingest replay ./recording.zip --password mysecret --live-sink --server http://localhost:8080\n")),(0,n.yg)("p",null,"Replay options:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Option"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--password")),(0,n.yg)("td",{parentName:"tr",align:null},"Password for decrypting the archive. Can also be set via ",(0,n.yg)("inlineCode",{parentName:"td"},"DATAHUB_RECORDING_PASSWORD")," environment variable")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--live-sink")),(0,n.yg)("td",{parentName:"tr",align:null},"Emit to real GMS server instead of using recorded responses")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--server")),(0,n.yg)("td",{parentName:"tr",align:null},"GMS server URL when using ",(0,n.yg)("inlineCode",{parentName:"td"},"--live-sink"))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--report-to")),(0,n.yg)("td",{parentName:"tr",align:null},"Path to write the report file")))),(0,n.yg)("h3",{id:"recording-beta"},"recording (Beta)"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"recording")," command group provides utilities for working with recording archives."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"# View archive metadata\ndatahub recording info recording.zip --password mysecret\n\n# Extract archive contents\ndatahub recording extract recording.zip --password mysecret --output-dir ./extracted\n\n# List archive contents\ndatahub recording list recording.zip --password mysecret\n")),(0,n.yg)("h4",{id:"recording-info"},"recording info"),(0,n.yg)("p",null,"Display metadata about a recording archive including run ID, source type, creation time, and whether an exception was captured."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"datahub recording info recording.zip --password mysecret\n")),(0,n.yg)("p",null,"Use ",(0,n.yg)("inlineCode",{parentName:"p"},"--json")," for machine-readable output."),(0,n.yg)("h4",{id:"recording-extract"},"recording extract"),(0,n.yg)("p",null,"Extract a recording archive to inspect its contents:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"datahub recording extract recording.zip --password mysecret --output-dir ./extracted\n")),(0,n.yg)("h4",{id:"recording-list"},"recording list"),(0,n.yg)("p",null,"List the files contained in a recording archive:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"datahub recording list recording.zip --password mysecret\n")),(0,n.yg)("h3",{id:"ingest-deploy"},"ingest deploy"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"ingest deploy")," command instructs the cli to upload an ingestion recipe to DataHub to be run by DataHub's ",(0,n.yg)("a",{parentName:"p",href:"/docs/ui-ingestion"},"UI Ingestion"),".\nThis command can also be used to schedule the ingestion while uploading or even to update existing sources. It will upload to the remote instance the\nCLI is connected to, not the sink of the recipe. Use ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub init")," to set the remote if not already set."),(0,n.yg)("p",null,"This command will automatically create a new recipe if it doesn't exist, or update it if it does.\nNote that this is a complete update, and will remove any options that were previously set.\nI.e: Not specifying a schedule in the cli update command will remove the schedule from the recipe to be updated."),(0,n.yg)("h4",{id:"command-options"},"Command Options"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-console"},'Usage: datahub ingest deploy [OPTIONS]\n\nOptions:\n  -n, --name TEXT     Recipe Name\n  -c, --config FILE   Config file in .toml or .yaml format.  [required]\n  --urn TEXT          Urn of recipe to update. If not specified here or in the recipe\'s pipeline_name,\n                      this will create a new ingestion source.\n  --executor-id TEXT  Executor id to route execution requests to. Do not use this unless you have\n                      configured a custom executor.\n  --cli-version TEXT  Provide a custom CLI version to use for ingestion. By default will use server\n                      default.\n  --schedule TEXT     Cron definition for schedule. If none is provided, ingestion recipe will not be\n                      scheduled\n  --time-zone TEXT    Timezone for the schedule in \'America/New_York\' format. Uses UTC by default.\n  --debug BOOLEAN     Should we debug.\n  --extra-pip TEXT    Extra pip packages. e.g. ["memray"]\n  --extra-env TEXT    Environment variables as comma-separated KEY=VALUE pairs.\n                      e.g. "VAR1=value1,VAR2=value2"\n')),(0,n.yg)("h4",{id:"examples"},"Examples"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Schedule a recipe with default executor:")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'datahub ingest deploy --name "Snowflake Integration" --schedule "0 5 * * *" --time-zone "Europe/London" -c recipe.yaml\n')),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Deploy to a specific remote executor:")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'datahub ingest deploy --name "Remote Snowflake Integration" --executor-id "remote-executor-pool-1" --schedule "0 5 * * *" -c recipe.yaml\n')),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Update an existing recipe:")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'datahub ingest deploy --urn "urn:li:dataHubIngestionSource:deploy-12345678" --schedule "0 6 * * *" -c updated_recipe.yaml\n')),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Deploy with environment variables:")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'datahub ingest deploy --name "Snowflake Integration" --extra-env "SNOWFLAKE_WAREHOUSE=COMPUTE_WH,SNOWFLAKE_ROLE=ANALYST" -c recipe.yaml\n')),(0,n.yg)("p",null,"By default, the ingestion recipe's identifier is generated by hashing the name.\nYou can override the urn generation by passing the ",(0,n.yg)("inlineCode",{parentName:"p"},"--urn")," flag to the CLI."),(0,n.yg)("h4",{id:"remote-executors"},"Remote Executors"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"--executor-id")," option allows you to route ingestion execution to specific executors:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Default executor")," (",(0,n.yg)("inlineCode",{parentName:"li"},'"default"'),"): Uses the managed executor provided by DataHub Cloud or your configured default executor"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Remote executors"),": Route to custom remote executors you've deployed in your environment")),(0,n.yg)("admonition",{type:"note"},(0,n.yg)("p",{parentName:"admonition"},'Use executor IDs other than "default" only if you have configured custom remote executors.')),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Examples with remote executors:")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'# Deploy to default executor (can be configured as remote)\ndatahub ingest deploy --name "My Integration" -c recipe.yaml\n\n# Deploy to specific remote executor pool\ndatahub ingest deploy --name "Private Network Integration" --executor-id "private-network-pool" -c recipe.yaml\n\n# Deploy to region-specific executor\ndatahub ingest deploy --name "EU Region Integration" --executor-id "eu-west-executor" -c recipe.yaml\n')),(0,n.yg)("p",null,"For more information on setting up remote executors, see the ",(0,n.yg)("a",{parentName:"p",href:"/docs/managed-datahub/operator-guide/setting-up-remote-ingestion-executor"},"Remote Executor Setup Guide"),"."),(0,n.yg)("h4",{id:"using-deployment-section"},"Using deployment section"),(0,n.yg)("p",null,"As an alternative to configuring settings from the CLI, they can also be set in the ",(0,n.yg)("inlineCode",{parentName:"p"},"deployment")," field of the recipe."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yml"},'# deployment_recipe.yml\ndeployment:\n  name: "Snowflake Integration"\n  schedule: "0 5 * * *"\n  time_zone: "Europe/London"\n  executor_id: "remote-executor-pool-1" # Optional: specify remote executor\n  cli_version: "0.15.0.1" # Optional: specify CLI version\n  extra_pip: \'["polars==1.35.2"]\'\n  extra_env: "VAR1=value1,VAR2=value2"\n\nsource: ...\n')),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"datahub ingest deploy -c deployment_recipe.yml\n")),(0,n.yg)("p",null,"CLI options will override corresponding values in the deployment section."),(0,n.yg)("h4",{id:"deployment-configuration-options"},"Deployment Configuration Options"),(0,n.yg)("p",null,"These deployment options that can be specified via CLI flags can also be configured in the ",(0,n.yg)("inlineCode",{parentName:"p"},"deployment")," section:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Field"),(0,n.yg)("th",{parentName:"tr",align:null},"CLI Option"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"),(0,n.yg)("th",{parentName:"tr",align:null},"Default"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"name")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--name")),(0,n.yg)("td",{parentName:"tr",align:null},"Recipe name displayed in the UI"),(0,n.yg)("td",{parentName:"tr",align:null},"Required")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"schedule")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--schedule")),(0,n.yg)("td",{parentName:"tr",align:null},"Cron expression for scheduling"),(0,n.yg)("td",{parentName:"tr",align:null},"None (manual only)")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"time_zone")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--time-zone")),(0,n.yg)("td",{parentName:"tr",align:null},"Timezone for scheduled runs"),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},'"UTC"'))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"executor_id")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--executor-id")),(0,n.yg)("td",{parentName:"tr",align:null},"Target executor for ingestion"),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},'"default"'))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"cli_version")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--cli-version")),(0,n.yg)("td",{parentName:"tr",align:null},"CLI version for ingestion"),(0,n.yg)("td",{parentName:"tr",align:null},"Server default")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"extra_pip")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--extra-pip")),(0,n.yg)("td",{parentName:"tr",align:null},"Extra pip packages"),(0,n.yg)("td",{parentName:"tr",align:null},"None")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"extra_env")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"--extra-env")),(0,n.yg)("td",{parentName:"tr",align:null},"Extra environment variables"),(0,n.yg)("td",{parentName:"tr",align:null},"None")))),(0,n.yg)("h4",{id:"batch-deployment"},"Batch Deployment"),(0,n.yg)("p",null,"Deploy multiple recipes from version control:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'# Deploy every yml recipe in a directory\nls recipe_directory/*.yml | xargs -n 1 -I {} datahub ingest deploy -c {}\n\n# Deploy with consistent executor across all recipes\nls recipe_directory/*.yml | xargs -n 1 -I {} datahub ingest deploy --executor-id "production-executor" -c {}\n')),(0,n.yg)("h3",{id:"init"},"init"),(0,n.yg)("p",null,"The init command is used to tell ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub")," about where your DataHub instance is located. The CLI will point to localhost DataHub by default."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},(0,n.yg)("em",{parentName:"strong"},"Note")),": Provide your GMS instance's host when the prompt asks you for the DataHub host."),(0,n.yg)("h4",{id:"interactive-mode"},"Interactive Mode"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"# Interactive mode - prompts for input\ndatahub init\n/Users/user/.datahubenv already exists. Overwrite? [y/N]: y\nConfigure which datahub instance to connect to\nEnter your DataHub host [http://localhost:8080]: http://localhost:8080\nEnter your DataHub access token []:\n")),(0,n.yg)("h4",{id:"non-interactive-mode-with-usernamepassword"},"Non-Interactive Mode with Username/Password"),(0,n.yg)("p",null,"The CLI can automatically generate tokens from your username and password credentials. This is useful for quickstart instances, automation, and CI/CD pipelines."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"# Quickstart (local instance with default credentials)\ndatahub init --username datahub --password datahub\n\n# Custom credentials with longer token duration\ndatahub init --username alice --password secret --token-duration ONE_MONTH\n\n# For long-running jobs or CI/CD\ndatahub init --username alice --password secret --token-duration ONE_WEEK\n")),(0,n.yg)("h4",{id:"datahub-cloud-example"},"DataHub Cloud Example"),(0,n.yg)("p",null,"For DataHub Cloud (Acryl-hosted) instances, you can use an existing token:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"# Interactive\ndatahub init\nEnter your DataHub host [http://localhost:8080]: https://<your-instance-id>.acryl.io/gms\nEnter your DataHub access token []: <token generated from https://<your-instance-id>.acryl.io/settings/tokens>\n\n# Non-interactive\ndatahub init --host https://<your-instance-id>.acryl.io/gms --token <your-token>\n")),(0,n.yg)("h4",{id:"environment-variables-supported"},"Environment variables supported"),(0,n.yg)("p",null,"The environment variables listed below take precedence over the DataHub CLI config created through the ",(0,n.yg)("inlineCode",{parentName:"p"},"init")," command."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_SKIP_CONFIG")," (default ",(0,n.yg)("inlineCode",{parentName:"li"},"false"),") - Set to ",(0,n.yg)("inlineCode",{parentName:"li"},"true")," to skip creating the configuration file."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_GMS_URL")," (default ",(0,n.yg)("inlineCode",{parentName:"li"},"http://localhost:8080"),") - Set to a URL of GMS instance"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_GMS_HOST")," (default ",(0,n.yg)("inlineCode",{parentName:"li"},"localhost"),") - Set to a host of GMS instance. Prefer using ",(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_GMS_URL")," to set the URL."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_GMS_PORT")," (default ",(0,n.yg)("inlineCode",{parentName:"li"},"8080"),") - Set to a port of GMS instance. Prefer using ",(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_GMS_URL")," to set the URL."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_GMS_PROTOCOL")," (default ",(0,n.yg)("inlineCode",{parentName:"li"},"http"),") - Set to a protocol like ",(0,n.yg)("inlineCode",{parentName:"li"},"http")," or ",(0,n.yg)("inlineCode",{parentName:"li"},"https"),". Prefer using ",(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_GMS_URL")," to set the URL."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_GMS_TOKEN")," (default ",(0,n.yg)("inlineCode",{parentName:"li"},"None"),") - Used for communicating with DataHub Cloud."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_USERNAME")," (default ",(0,n.yg)("inlineCode",{parentName:"li"},"None"),") - Username for generating access tokens via ",(0,n.yg)("inlineCode",{parentName:"li"},"datahub init"),". Used with ",(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_PASSWORD")," for non-interactive authentication."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_PASSWORD")," (default ",(0,n.yg)("inlineCode",{parentName:"li"},"None"),") - Password for generating access tokens via ",(0,n.yg)("inlineCode",{parentName:"li"},"datahub init"),". Used with ",(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_USERNAME")," for non-interactive authentication."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_TELEMETRY_ENABLED")," (default ",(0,n.yg)("inlineCode",{parentName:"li"},"true"),") - Set to ",(0,n.yg)("inlineCode",{parentName:"li"},"false")," to disable telemetry. If CLI is being run in an environment with no access to public internet then this should be disabled."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_TELEMETRY_TIMEOUT")," (default ",(0,n.yg)("inlineCode",{parentName:"li"},"10"),") - Set to a custom integer value to specify timeout in secs when sending telemetry."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_DEBUG")," (default ",(0,n.yg)("inlineCode",{parentName:"li"},"false"),") - Set to ",(0,n.yg)("inlineCode",{parentName:"li"},"true")," to enable debug logging for CLI. Can also be achieved through ",(0,n.yg)("inlineCode",{parentName:"li"},"--debug")," option of the CLI. This exposes sensitive information in logs, enabling on production instances should be avoided especially if UI ingestion is in use as logs can be made available for runs through the UI."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_VERSION")," (default ",(0,n.yg)("inlineCode",{parentName:"li"},"head"),") - Set to a specific version to run quickstart with the particular version of docker images."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"ACTIONS_VERSION")," (default ",(0,n.yg)("inlineCode",{parentName:"li"},"head"),") - Set to a specific version to run quickstart with that image tag of ",(0,n.yg)("inlineCode",{parentName:"li"},"datahub-actions")," container."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_ACTIONS_IMAGE")," (default ",(0,n.yg)("inlineCode",{parentName:"li"},"acryldata/datahub-actions"),") - Set to ",(0,n.yg)("inlineCode",{parentName:"li"},"-slim")," to run a slimmer actions container without pyspark/deequ features."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATAHUB_RECORDING_PASSWORD")," - Password for encrypting/decrypting recording archives. Used by ",(0,n.yg)("inlineCode",{parentName:"li"},"--record")," and ",(0,n.yg)("inlineCode",{parentName:"li"},"--replay")," commands."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"INGESTION_ARTIFACT_DIR")," - Directory to save recordings when S3 upload is disabled. If not set, recordings are saved to a temp directory.")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"DATAHUB_SKIP_CONFIG=false\nDATAHUB_GMS_URL=http://localhost:8080\nDATAHUB_GMS_TOKEN=\nDATAHUB_TELEMETRY_ENABLED=true\nDATAHUB_TELEMETRY_TIMEOUT=10\nDATAHUB_DEBUG=false\n")),(0,n.yg)("h3",{id:"container"},"container"),(0,n.yg)("p",null,"A group of commands to interact with containers in DataHub."),(0,n.yg)("p",null,"e.g. You can use this to apply a tag to all datasets recursively in this container."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'datahub container tag --container-urn "urn:li:container:0e9e46bd6d5cf645f33d5a8f0254bc2d" --tag-urn "urn:li:tag:tag1"\ndatahub container domain --container-urn "urn:li:container:3f2effd1fbe154a4d60b597263a41e41" --domain-urn  "urn:li:domain:ajsajo-b832-4ab3-8881-7ed5e991a44c"\ndatahub container owner --container-urn "urn:li:container:3f2effd1fbe154a4d60b597263a41e41" --owner-urn  "urn:li:corpGroup:eng@example.com"\ndatahub container term --container-urn "urn:li:container:3f2effd1fbe154a4d60b597263a41e41" --term-urn  "urn:li:term:PII"\n')),(0,n.yg)("h3",{id:"check"},"check"),(0,n.yg)("p",null,"The datahub package is composed of different plugins that allow you to connect to different metadata sources and ingest metadata from them.\nThe ",(0,n.yg)("inlineCode",{parentName:"p"},"check")," command allows you to check if all plugins are loaded correctly as well as validate an individual MCE-file."),(0,n.yg)("h4",{id:"restore-indices"},"restore-indices"),(0,n.yg)("p",null,"This command allows you to restore indices for one or more ",(0,n.yg)("inlineCode",{parentName:"p"},"urn"),"."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'datahub --debug check restore-indices --urn "URN"\n')),(0,n.yg)("p",null,"It can also take ",(0,n.yg)("inlineCode",{parentName:"p"},"--file")," argument that points to a file that has list of urns like"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"datahub check restore-indices --file ./urn.txt\n")),(0,n.yg)("p",null,"where urn.txt is like this"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-urn.txt"},"urn:li:dataset:(urn:li:dataPlatform:snowflake,test_db.schema1.test_all_nulls,PROD)\nurn:li:dataset:(urn:li:dataPlatform:platform1,test_db.schema2.test_complex_types,PROD)\nurn:li:dataset:(urn:li:dataPlatform:redshift,test_db.schema3.test_few_rows,PROD)\n")),(0,n.yg)("h4",{id:"get-kafka-consumer-offsets"},"get-kafka-consumer-offsets"),(0,n.yg)("p",null,"This required DataHub Cloud ",(0,n.yg)("inlineCode",{parentName:"p"},"0.3.12.x")," or above version."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"datahub check get-kafka-consumer-offsets\n")),(0,n.yg)("p",null,"which can give can print out the details of lag in kafka topics."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"{'mcl': {'generic-mae-consumer-job-client': {'MetadataChangeLog_Versioned_v1': {'metrics': {'avgLag': 36,\n                                                                                            'maxLag': 36,\n                                                                                            'medianLag': 36,\n                                                                                            'totalLag': 36},\n                                                                                'partitions': {'0': {'lag': 36,\n                                                                                                     'offset': 257318}}}}},\n 'mcl-timeseries': {'generic-mae-consumer-job-client': {'MetadataChangeLog_Timeseries_v1': {'metrics': {'avgLag': 0,\n                                                                                                        'maxLag': 0,\n                                                                                                        'medianLag': 0,\n                                                                                                        'totalLag': 0},\n                                                                                            'partitions': {'0': {'lag': 0,\n                                                                                                                 'offset': 113}}}}},\n 'mcp': {'generic-mce-consumer-job-client': {'MetadataChangeProposal_v1': {'metrics': {'avgLag': 7222149,\n                                                                                       'maxLag': 7222149,\n                                                                                       'medianLag': 7222149,\n                                                                                       'totalLag': 7222149},\n                                                                           'partitions': {'0': {'lag': 7222149,\n                                                                                                'offset': 250254}}}}}}\n")),(0,n.yg)("h3",{id:"delete"},"delete"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"delete")," command allows you to delete metadata from DataHub."),(0,n.yg)("p",null,"The ",(0,n.yg)("a",{parentName:"p",href:"/docs/how/delete-metadata"},"metadata deletion guide")," covers the various options for the delete command."),(0,n.yg)("h3",{id:"exists"},"exists"),(0,n.yg)("p",null,"The exists command can be used to check if an entity exists in DataHub."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'> datahub exists --urn "urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)"\ntrue\n> datahub exists --urn "urn:li:dataset:(urn:li:dataPlatform:hive,NonExistentHiveDataset,PROD)"\nfalse\n')),(0,n.yg)("h3",{id:"get"},"get"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"get")," command allows you to easily retrieve metadata from DataHub, by using the REST API. This works for both versioned aspects and timeseries aspects. For timeseries aspects, it fetches the latest value.\nFor example the following command gets the ownership aspect from the dataset ",(0,n.yg)("inlineCode",{parentName:"p"},"urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell-session"},'$ datahub get --urn "urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)" --aspect ownership\n{\n  "ownership": {\n    "lastModified": {\n      "actor": "urn:li:corpuser:jdoe",\n      "time": 1680210917580\n    },\n    "owners": [\n      {\n        "owner": "urn:li:corpuser:jdoe",\n        "source": {\n          "type": "SERVICE"\n        },\n        "type": "NONE"\n      }\n    ]\n  }\n}\n')),(0,n.yg)("h3",{id:"graphql"},"graphql"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"graphql")," command allows you to execute GraphQL queries and mutations against DataHub's GraphQL API. This provides full access to DataHub's metadata through its native GraphQL interface."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'# Execute a GraphQL query\ndatahub graphql --query "query { me { username } }"\n\n# Use named operations from DataHub\'s schema\ndatahub graphql --operation searchAcrossEntities --variables \'{"input": {"query": "users"}}\'\n\n# List available operations\ndatahub graphql --list-operations\n\n# Get help for a specific operation\ndatahub graphql --describe searchAcrossEntities\n\n# Explore types recursively\ndatahub graphql --describe SearchInput --recurse\n\n# Load queries and variables from files\ndatahub graphql --query ./search-tags.graphql --variables ./search-params.json\n\n# Get JSON output for LLM integration\ndatahub graphql --list-operations --format json\n')),(0,n.yg)("p",null,"The GraphQL command supports both raw GraphQL queries/mutations and operation-based execution using DataHub's introspected schema. It automatically detects whether ",(0,n.yg)("inlineCode",{parentName:"p"},"--query")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"--variables")," arguments are file paths or literal content, enabling seamless use of both inline GraphQL and file-based queries."),(0,n.yg)("p",null,"Key features:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Schema discovery"),": List and describe all available operations and types"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"File support"),": Load queries and variables from ",(0,n.yg)("inlineCode",{parentName:"li"},".graphql")," and ",(0,n.yg)("inlineCode",{parentName:"li"},".json")," files"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"LLM-friendly output"),": JSON format with complete type information"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Recursive exploration"),": Deep-dive into complex GraphQL types")),(0,n.yg)("p",null,"\u27a1\ufe0f ",(0,n.yg)("a",{parentName:"p",href:"/docs/cli-commands/graphql"},"Learn more about the GraphQL command")),(0,n.yg)("h3",{id:"put"},"put"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"put")," group of commands allows you to write metadata into DataHub. This is a flexible way for you to issue edits to metadata from the command line."),(0,n.yg)("h4",{id:"put-aspect"},"put aspect"),(0,n.yg)("p",null,"The ",(0,n.yg)("strong",{parentName:"p"},"put aspect")," (also the default ",(0,n.yg)("inlineCode",{parentName:"p"},"put"),") command instructs ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub")," to set a specific aspect for an entity to a specified value.\nFor example, the command shown below sets the ",(0,n.yg)("inlineCode",{parentName:"p"},"ownership")," aspect of the dataset ",(0,n.yg)("inlineCode",{parentName:"p"},"urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)")," to the value in the file ",(0,n.yg)("inlineCode",{parentName:"p"},"ownership.json"),".\nThe JSON in the ",(0,n.yg)("inlineCode",{parentName:"p"},"ownership.json")," file needs to conform to the ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-models/src/main/pegasus/com/linkedin/common/Ownership.pdl"},(0,n.yg)("inlineCode",{parentName:"a"},"Ownership"))," Aspect model as shown below."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "owners": [\n    {\n      "owner": "urn:li:corpuser:jdoe",\n      "type": "DEVELOPER"\n    },\n    {\n      "owner": "urn:li:corpuser:jdub",\n      "type": "DATAOWNER"\n    }\n  ]\n}\n')),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-console"},'datahub --debug put --urn "urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)" --aspect ownership -d ownership.json\n\n[DATE_TIMESTAMP] DEBUG    {datahub.cli.cli_utils:340} - Attempting to emit to DataHub GMS; using curl equivalent to:\ncurl -X POST -H \'User-Agent: python-requests/2.26.0\' -H \'Accept-Encoding: gzip, deflate\' -H \'Accept: */*\' -H \'Connection: keep-alive\' -H \'X-RestLi-Protocol-Version: 2.0.0\' -H \'Content-Type: application/json\' --data \'{"proposal": {"entityType": "dataset", "entityUrn": "urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)", "aspectName": "ownership", "changeType": "UPSERT", "aspect": {"contentType": "application/json", "value": "{\\"owners\\": [{\\"owner\\": \\"urn:li:corpuser:jdoe\\", \\"type\\": \\"DEVELOPER\\"}, {\\"owner\\": \\"urn:li:corpuser:jdub\\", \\"type\\": \\"DATAOWNER\\"}]}"}}}\' \'http://localhost:8080/aspects/?action=ingestProposal\'\nUpdate succeeded with status 200\n')),(0,n.yg)("h4",{id:"put-platform"},"put platform"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"\ud83e\udd1d Version Compatibility:")," ",(0,n.yg)("inlineCode",{parentName:"p"},"acryl-datahub>0.8.44.4")),(0,n.yg)("p",null,"The ",(0,n.yg)("strong",{parentName:"p"},"put platform")," command instructs ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub")," to create or update metadata about a data platform. This is very useful if you are using a custom data platform, to set up its logo and display name for a native UI experience."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'datahub put platform --name longtail_schemas --display_name "Long Tail Schemas" --logo "https://flink.apache.org/img/logo/png/50/color_50.png"\n\u2705 Successfully wrote data platform metadata for urn:li:dataPlatform:longtail_schemas to DataHub (DataHubRestEmitter: configured to talk to https://longtailcompanions.acryl.io/api/gms with token: eyJh**********Cics)\n')),(0,n.yg)("h3",{id:"timeline"},"timeline"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"timeline")," command allows you to view a version history for entities. Currently only supported for Datasets. For example,\nthe following command will show you the modifications to tags for a dataset for the past week. The output includes a computed semantic version,\nrelevant for schema changes only currently, the target of the modification, and a description of the change including a timestamp.\nThe default output is sanitized to be more readable, but the full API output can be obtained by passing the ",(0,n.yg)("inlineCode",{parentName:"p"},"--verbose")," flag and\nto get the raw JSON difference in addition to the API output you can add the ",(0,n.yg)("inlineCode",{parentName:"p"},"--raw")," flag. For more details about the feature please see ",(0,n.yg)("a",{parentName:"p",href:"/docs/dev-guides/timeline"},"the main feature page")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-console"},'datahub timeline --urn "urn:li:dataset:(urn:li:dataPlatform:mysql,User.UserAccount,PROD)" --category TAG --start 7daysago\n2022-02-17 14:03:42 - 0.0.0-computed\n MODIFY TAG dataset:mysql:User.UserAccount : A change in aspect editableSchemaMetadata happened at time 2022-02-17 20:03:42.0\n2022-02-17 14:17:30 - 0.0.0-computed\n MODIFY TAG dataset:mysql:User.UserAccount : A change in aspect editableSchemaMetadata happened at time 2022-02-17 20:17:30.118\n')),(0,n.yg)("h2",{id:"entity-specific-commands"},"Entity Specific Commands"),(0,n.yg)("h3",{id:"dataset-dataset-entity"},"dataset (Dataset Entity)"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"dataset")," command allows you to interact with Dataset entities in DataHub, including creating, updating, retrieving, and validating Dataset metadata."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'# Get a dataset and write to YAML file\ndatahub dataset get --urn "urn:li:dataset:(urn:li:dataPlatform:hive,example_table,PROD)" --to-file dataset.yaml\n\n# Create or update dataset from YAML file\ndatahub dataset upsert -f dataset.yaml\n')),(0,n.yg)("p",null,"\u27a1\ufe0f ",(0,n.yg)("a",{parentName:"p",href:"/docs/cli-commands/dataset"},"Learn more about the dataset command")),(0,n.yg)("h3",{id:"user-user-entity"},"user (User Entity)"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"user")," command allows you to interact with the User entity in DataHub. It supports two main operations:"),(0,n.yg)("h4",{id:"upsert"},"upsert"),(0,n.yg)("p",null,"Create or update users from a YAML file. For detailed information, please refer to ",(0,n.yg)("a",{parentName:"p",href:"/docs/api/tutorials/owners#upsert-users"},"Creating Users and Groups with Datahub CLI"),"."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"datahub user upsert -f users.yaml\n")),(0,n.yg)("p",null,"An example of ",(0,n.yg)("inlineCode",{parentName:"p"},"users.yaml")," would look like as in ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/examples/cli_usage/user/bar.user.dhub.yaml"},"bar.user.dhub.yaml")," file for the complete code."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'- id: bar@acryl.io\n  first_name: The\n  last_name: Bar\n  email: bar@acryl.io\n  slack: "@the_bar_raiser"\n  description: "I like raising the bar higher"\n  groups:\n    - foogroup@acryl.io\n- id: datahub\n  slack: "@datahubproject"\n  phone: "1-800-GOT-META"\n  description: "The DataHub Project"\n  picture_link: "https://raw.githubusercontent.com/datahub-project/datahub/master/datahub-web-react/src/images/datahub-logo-color-stable.svg"\n')),(0,n.yg)("h4",{id:"add"},"add"),(0,n.yg)("p",null,"Create a native DataHub user with email/password authentication. This command creates users who can log in directly to DataHub using their email and password."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'# Create a user with a role\ndatahub user add --email user@example.com --display-name "John Doe" --password --role Admin\n\n# Create a user without a role\ndatahub user add --email user@example.com --display-name "Jane Smith" --password\n')),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Options:")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"--email")," (required): User's email address, which will be used as their login ID"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"--display-name")," (required): User's full display name"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"--password")," (required): Flag to prompt for password input (password will be hidden during entry)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"--role")," (optional): Role to assign to the user. Valid values are ",(0,n.yg)("inlineCode",{parentName:"li"},"Admin"),", ",(0,n.yg)("inlineCode",{parentName:"li"},"Editor"),", or ",(0,n.yg)("inlineCode",{parentName:"li"},"Reader")," (case-insensitive)")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Notes:")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"The command will check if a user with the specified email already exists and exit if found"),(0,n.yg)("li",{parentName:"ul"},"Passwords are entered securely via a hidden prompt and require confirmation"),(0,n.yg)("li",{parentName:"ul"},"If role assignment fails, the user will still be created but without the role"),(0,n.yg)("li",{parentName:"ul"},"Requires admin permissions to execute")),(0,n.yg)("h3",{id:"group-group-entity"},"group (Group Entity)"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"group")," command allows you to interact with the Group entity.\nIt currently supports the ",(0,n.yg)("inlineCode",{parentName:"p"},"upsert")," operation, which can be used to create a new group or update an existing one with embedded Users.\nFor more information, please refer to ",(0,n.yg)("a",{parentName:"p",href:"/docs/api/tutorials/owners#upsert-users"},"Creating Users and Groups with Datahub CLI"),"."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"datahub group upsert -f group.yaml\n")),(0,n.yg)("p",null,"An example of ",(0,n.yg)("inlineCode",{parentName:"p"},"group.yaml")," would look like as in ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/examples/cli_usage/group/foo.group.dhub.yaml"},"foo.group.dhub.yaml")," file for the complete code."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'id: foogroup@acryl.io\ndisplay_name: Foo Group\nadmins:\n  - datahub\nmembers:\n  - bar@acryl.io # refer to a user either by id or by urn\n  - id: joe@acryl.io # inline specification of user\n    slack: "@joe_shmoe"\n    display_name: "Joe\'s Hub"\n')),(0,n.yg)("h3",{id:"dataproduct-data-product-entity"},"dataproduct (Data Product Entity)"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"\ud83e\udd1d Version Compatibility:")," ",(0,n.yg)("inlineCode",{parentName:"p"},"acryl-datahub>=0.10.2.4")),(0,n.yg)("p",null,"The dataproduct group of commands allows you to manage the lifecycle of a DataProduct entity on DataHub.\nSee the ",(0,n.yg)("a",{parentName:"p",href:"/docs/dataproducts"},"Data Products")," page for more details on what a Data Product is and how DataHub represents it."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"datahub dataproduct --help\nCommands:\n  upsert*          Upsert attributes to a Data Product in DataHub\n  update           Create or Update a Data Product in DataHub.\n  add_asset        Add an asset to a Data Product\n  add_owner        Add an owner to a Data Product\n  delete           Delete a Data Product in DataHub.\n  diff             Diff a Data Product file with its twin in DataHub\n  get              Get a Data Product from DataHub\n  remove_asset     Add an asset to a Data Product\n  remove_owner     Remove an owner from a Data Product\n  set_description  Set description for a Data Product in DataHub\n")),(0,n.yg)("p",null,"Here we detail the sub-commands available under the dataproduct group of commands:"),(0,n.yg)("h4",{id:"upsert-1"},"upsert"),(0,n.yg)("p",null,"Use this to upsert a data product yaml file into DataHub. This will create the data product if it doesn't exist already. Remember, this will upsert all the fields that are specified in the yaml file and will not touch the fields that are not specified. For example, if you do not specify the ",(0,n.yg)("inlineCode",{parentName:"p"},"description")," field in the yaml file, then ",(0,n.yg)("inlineCode",{parentName:"p"},"upsert")," will not modify the description field on the Data Product entity in DataHub. To keep this file sync-ed with the metadata on DataHub use the ",(0,n.yg)("a",{parentName:"p",href:"#diff"},"diff")," command. The format of the yaml file is available ",(0,n.yg)("a",{parentName:"p",href:"/docs/dataproducts#creating-a-data-product-yaml--git"},"here"),"."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"# Usage\n> datahub dataproduct upsert -f data_product.yaml\n\n")),(0,n.yg)("h4",{id:"update"},"update"),(0,n.yg)("p",null,"Use this to fully replace a data product's metadata in DataHub from a yaml file. This will create the data product if it doesn't exist already. Remember, this will update all the fields including ones that are not specified in the yaml file. For example, if you do not specify the ",(0,n.yg)("inlineCode",{parentName:"p"},"description")," field in the yaml file, then ",(0,n.yg)("inlineCode",{parentName:"p"},"update")," will set the description field on the Data Product entity in DataHub to empty. To keep this file sync-ed with the metadata on DataHub use the ",(0,n.yg)("a",{parentName:"p",href:"#diff"},"diff")," command. The format of the yaml file is available ",(0,n.yg)("a",{parentName:"p",href:"/docs/dataproducts#creating-a-data-product-yaml--git"},"here"),"."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"# Usage\n> datahub dataproduct upsert -f data_product.yaml\n\n")),(0,n.yg)("admonition",{type:"note"},(0,n.yg)("p",{parentName:"admonition"},"\u2757",(0,n.yg)("strong",{parentName:"p"},"Pro-Tip: upsert versus update")),(0,n.yg)("p",{parentName:"admonition"},"Wondering which command is right for you? Use ",(0,n.yg)("inlineCode",{parentName:"p"},"upsert")," if there are certain elements of metadata that you don't want to manage using the yaml file (e.g. owners, assets or description). Use ",(0,n.yg)("inlineCode",{parentName:"p"},"update")," if you want to manage the entire data product's metadata using the yaml file.")),(0,n.yg)("h4",{id:"diff"},"diff"),(0,n.yg)("p",null,"Use this to keep a data product yaml file updated from its server-side version in DataHub."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"# Usage\n> datahub dataproduct diff -f data_product.yaml --update\n")),(0,n.yg)("h4",{id:"get-1"},"get"),(0,n.yg)("p",null,"Use this to get a data product entity from DataHub and optionally write it to a yaml file"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'# Usage\n> datahub dataproduct get --urn urn:li:dataProduct:pet_of_the_week --to-file pet_of_the_week_dataproduct.yaml\n{\n  "id": "urn:li:dataProduct:pet_of_the_week",\n  "domain": "urn:li:domain:dcadded3-2b70-4679-8b28-02ac9abc92eb",\n  "assets": [\n    "urn:li:dataset:(urn:li:dataPlatform:snowflake,long_tail_companions.analytics.pet_details,PROD)",\n    "urn:li:dashboard:(looker,dashboards.19)",\n    "urn:li:dataFlow:(airflow,snowflake_load,prod)"\n  ],\n  "display_name": "Pet of the Week Campaign",\n  "owners": [\n    {\n      "id": "urn:li:corpuser:jdoe",\n      "type": "BUSINESS_OWNER"\n    }\n  ],\n  "description": "This campaign includes Pet of the Week data.",\n  "tags": [\n    "urn:li:tag:adoption"\n  ],\n  "terms": [\n    "urn:li:glossaryTerm:ClientsAndAccounts.AccountBalance"\n  ],\n  "properties": {\n    "lifecycle": "production",\n    "sla": "7am every day"\n  }\n}\nData Product yaml written to pet_of_the_week_dataproduct.yaml\n')),(0,n.yg)("h4",{id:"add_asset"},"add_asset"),(0,n.yg)("p",null,"Use this to add a data asset to a Data Product."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'# Usage\n> datahub dataproduct add_asset --urn "urn:li:dataProduct:pet_of_the_week"  --asset "urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_deleted,PROD)"\n')),(0,n.yg)("h4",{id:"remove_asset"},"remove_asset"),(0,n.yg)("p",null,"Use this to remove a data asset from a Data Product."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'# Usage\n> datahub dataproduct remove_asset --urn "urn:li:dataProduct:pet_of_the_week"  --asset "urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_deleted,PROD)"\n')),(0,n.yg)("h4",{id:"add_owner"},"add_owner"),(0,n.yg)("p",null,"Use this to add an owner to a Data Product."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'# Usage\n> datahub dataproduct add_owner --urn "urn:li:dataProduct:pet_of_the_week"  --owner "jdoe@longtail.com" --owner-type BUSINESS_OWNER\n')),(0,n.yg)("h4",{id:"remove_owner"},"remove_owner"),(0,n.yg)("p",null,"Use this to remove an owner from a Data Product."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'# Usage\n> datahub dataproduct remove_owner --urn "urn:li:dataProduct:pet_of_the_week"  --owner "urn:li:corpUser:jdoe@longtail.com"\n')),(0,n.yg)("h4",{id:"set_description"},"set_description"),(0,n.yg)("p",null,"Use this to attach rich documentation for a Data Product in DataHub."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'> datahub dataproduct set_description --urn "urn:li:dataProduct:pet_of_the_week" --description "This is the pet dataset"\n# For uploading rich documentation from a markdown file, use the --md-file option\n# > datahub dataproduct set_description --urn "urn:li:dataProduct:pet_of_the_week" --md-file ./pet_of_the_week.md\n')),(0,n.yg)("h4",{id:"delete-1"},"delete"),(0,n.yg)("p",null,"Use this to delete a Data Product from DataHub. Default to ",(0,n.yg)("inlineCode",{parentName:"p"},"--soft")," which preserves metadata, use ",(0,n.yg)("inlineCode",{parentName:"p"},"--hard")," to erase all metadata associated with this Data Product."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},'> datahub dataproduct delete --urn "urn:li:dataProduct:pet_of_the_week"\n# For Hard Delete see below:\n# > datahub dataproduct delete --urn "urn:li:dataProduct:pet_of_the_week" --hard\n')),(0,n.yg)("h2",{id:"miscellaneous-admin-commands"},"Miscellaneous Admin Commands"),(0,n.yg)("h3",{id:"lite-experimental"},"lite (experimental)"),(0,n.yg)("p",null,"The lite group of commands allow you to run an embedded, lightweight DataHub instance for command line exploration of your metadata. This is intended more for developer tool oriented usage rather than as a production server instance for DataHub. See ",(0,n.yg)("a",{parentName:"p",href:"/docs/datahub_lite"},"DataHub Lite")," for more information about how you can ingest metadata into DataHub Lite and explore your metadata easily."),(0,n.yg)("h3",{id:"telemetry"},"telemetry"),(0,n.yg)("p",null,"To help us understand how people are using DataHub, we collect anonymous usage statistics on actions such as command invocations via Mixpanel.\nWe do not collect private information such as IP addresses, contents of ingestions, or credentials.\nThe code responsible for collecting and broadcasting these events is open-source and can be found ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/telemetry/telemetry.py"},"within our GitHub"),"."),(0,n.yg)("p",null,"Telemetry is enabled by default, and the ",(0,n.yg)("inlineCode",{parentName:"p"},"telemetry")," command lets you toggle the sending of these statistics via ",(0,n.yg)("inlineCode",{parentName:"p"},"telemetry enable/disable"),"."),(0,n.yg)("h3",{id:"migrate"},"migrate"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"migrate")," group of commands allows you to perform certain kinds of migrations."),(0,n.yg)("h4",{id:"dataplatform2instance"},"dataplatform2instance"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"dataplatform2instance")," migration command allows you to migrate your entities from an instance-agnostic platform identifier to an instance-specific platform identifier. If you have ingested metadata in the past for this platform and would like to transfer any important metadata over to the new instance-specific entities, then you should use this command. For example, if your users have added documentation or added tags or terms to your datasets, then you should run this command to transfer this metadata over to the new entities. For further context, read the Platform Instance Guide ",(0,n.yg)("a",{parentName:"p",href:"/docs/platform-instances"},"here"),"."),(0,n.yg)("p",null,"A few important options worth calling out:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"--dry-run / -n : Use this to get a report for what will be migrated before running"),(0,n.yg)("li",{parentName:"ul"},"--force / -F : Use this if you know what you are doing and do not want to get a confirmation prompt before migration is started"),(0,n.yg)("li",{parentName:"ul"},"--keep : When enabled, will preserve the old entities and not delete them. Default behavior is to soft-delete old entities."),(0,n.yg)("li",{parentName:"ul"},"--hard : When enabled, will hard-delete the old entities.")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},(0,n.yg)("em",{parentName:"strong"},"Note")),": Timeseries aspects such as Usage Statistics and Dataset Profiles are not migrated over to the new entity instances, you will get new data points created when you re-run ingestion using the ",(0,n.yg)("inlineCode",{parentName:"p"},"usage")," or sources with profiling turned on."),(0,n.yg)("h5",{id:"dry-run"},"Dry Run"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-console"},"datahub migrate dataplatform2instance --platform elasticsearch --instance prod_index --dry-run\nStarting migration: platform:elasticsearch, instance=prod_index, force=False, dry-run=True\n100% (25 of 25) |####################################################################################################################################################################################| Elapsed Time: 0:00:00 Time:  0:00:00\n[Dry Run] Migration Report:\n--------------\n[Dry Run] Migration Run Id: migrate-5710349c-1ec7-4b83-a7d3-47d71b7e972e\n[Dry Run] Num entities created = 25\n[Dry Run] Num entities affected = 0\n[Dry Run] Num entities migrated = 25\n[Dry Run] Details:\n[Dry Run] New Entities Created: {'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.datahubretentionindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.schemafieldindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.system_metadata_service_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.tagindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.dataset_datasetprofileaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.mlmodelindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.mlfeaturetableindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.datajob_datahubingestioncheckpointaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.datahub_usage_event,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.dataset_operationaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.datajobindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.dataprocessindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.glossarytermindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.dataplatformindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.mlmodeldeploymentindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.datajob_datahubingestionrunsummaryaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.graph_service_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.datahubpolicyindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.dataset_datasetusagestatisticsaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.dashboardindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.glossarynodeindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.mlfeatureindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.dataflowindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.mlprimarykeyindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.chartindex_v2,PROD)'}\n[Dry Run] External Entities Affected: None\n[Dry Run] Old Entities Migrated = {'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,dataset_datasetusagestatisticsaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,mlmodelindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,mlmodeldeploymentindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,datajob_datahubingestionrunsummaryaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,datahubretentionindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,datahubpolicyindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,dataset_datasetprofileaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,glossarynodeindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,dataset_operationaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,graph_service_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,datajobindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,mlprimarykeyindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,dashboardindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,datajob_datahubingestioncheckpointaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,tagindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,datahub_usage_event,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,schemafieldindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,mlfeatureindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,dataprocessindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,dataplatformindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,mlfeaturetableindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,glossarytermindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,dataflowindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,chartindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,system_metadata_service_v1,PROD)'}\n")),(0,n.yg)("h5",{id:"real-migration-with-soft-delete"},"Real Migration (with soft-delete)"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"> datahub migrate dataplatform2instance --platform hive --instance\ndatahub migrate dataplatform2instance --platform hive --instance warehouse\nStarting migration: platform:hive, instance=warehouse, force=False, dry-run=False\nWill migrate 4 urns such as ['urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_deleted,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,logging_events,PROD)']\nNew urns will look like ['urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.logging_events,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.fct_users_created,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.SampleHiveDataset,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.fct_users_deleted,PROD)']\n\nOk to proceed? [y/N]:\n...\nMigration Report:\n--------------\nMigration Run Id: migrate-f5ae7201-4548-4bee-aed4-35758bb78c89\nNum entities created = 4\nNum entities affected = 0\nNum entities migrated = 4\nDetails:\nNew Entities Created: {'urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.SampleHiveDataset,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.fct_users_deleted,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.logging_events,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.fct_users_created,PROD)'}\nExternal Entities Affected: None\nOld Entities Migrated = {'urn:li:dataset:(urn:li:dataPlatform:hive,logging_events,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_deleted,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_created,PROD)'}\n")),(0,n.yg)("h2",{id:"alternate-installation-options"},"Alternate Installation Options"),(0,n.yg)("h3",{id:"using-docker"},"Using docker"),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://hub.docker.com/r/acryldata/datahub-ingestion"},(0,n.yg)("img",{parentName:"a",src:"https://img.shields.io/docker/pulls/acryldata/datahub-ingestion?style=plastic",alt:"Docker Hub"}))),(0,n.yg)("p",null,"If you don't want to install locally, you can alternatively run metadata ingestion within a Docker container.\nWe have prebuilt images available on ",(0,n.yg)("a",{parentName:"p",href:"https://hub.docker.com/r/acryldata/datahub-ingestion"},"Docker hub"),". All plugins will be installed and enabled automatically."),(0,n.yg)("p",null,"You can use the ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub-ingestion")," docker image as explained in ",(0,n.yg)("a",{parentName:"p",href:"/docs/docker"},"Docker Images"),". In case you are using Kubernetes you can start a pod with the ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub-ingestion")," docker image, log onto a shell on the pod and you should have the access to datahub CLI in your kubernetes cluster."),(0,n.yg)("p",null,(0,n.yg)("em",{parentName:"p"},"Limitation: the datahub_docker.sh convenience script assumes that the recipe and any input/output files are accessible in the current working directory or its subdirectories. Files outside the current working directory will not be found, and you'll need to invoke the Docker image directly.")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"# Assumes the DataHub repo is cloned locally.\n./metadata-ingestion/scripts/datahub_docker.sh ingest -c ./examples/recipes/example_to_datahub_rest.yml\n")),(0,n.yg)("h3",{id:"install-from-source"},"Install from source"),(0,n.yg)("p",null,"If you'd like to install from source, see the ",(0,n.yg)("a",{parentName:"p",href:"/docs/metadata-ingestion/developing"},"developer guide"),"."),(0,n.yg)("h2",{id:"installing-plugins"},"Installing Plugins"),(0,n.yg)("p",null,"We use a plugin architecture so that you can install only the dependencies you actually need. Click the plugin name to learn more about the specific source recipe and any FAQs!"),(0,n.yg)("h3",{id:"sources"},"Sources"),(0,n.yg)("p",null,"Please see our ",(0,n.yg)("a",{parentName:"p",href:"/integrations"},"Integrations page")," if you want to filter on the features offered by each source."),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Plugin Name"),(0,n.yg)("th",{parentName:"tr",align:null},"Install Command"),(0,n.yg)("th",{parentName:"tr",align:null},"Provides"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/metadata-file"},"metadata-file")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("em",{parentName:"td"},"included by default")),(0,n.yg)("td",{parentName:"tr",align:null},"File source and sink")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/athena"},"athena")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[athena]'")),(0,n.yg)("td",{parentName:"tr",align:null},"AWS Athena source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/bigquery"},"bigquery")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[bigquery]'")),(0,n.yg)("td",{parentName:"tr",align:null},"BigQuery source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/file-based-lineage"},"datahub-lineage-file")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("em",{parentName:"td"},"no additional dependencies")),(0,n.yg)("td",{parentName:"tr",align:null},"Lineage File source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/business-glossary"},"datahub-business-glossary")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("em",{parentName:"td"},"no additional dependencies")),(0,n.yg)("td",{parentName:"tr",align:null},"Business Glossary File source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/dbt"},"dbt")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[dbt]'")),(0,n.yg)("td",{parentName:"tr",align:null},"dbt source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/dremio"},"dremio")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[dremio]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Dremio Source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/druid"},"druid")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[druid]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Druid Source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/feast"},"feast")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[feast]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Feast source (0.26.0)")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/glue"},"glue")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[glue]'")),(0,n.yg)("td",{parentName:"tr",align:null},"AWS Glue source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/hana"},"hana")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[hana]'")),(0,n.yg)("td",{parentName:"tr",align:null},"SAP HANA source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/hive"},"hive")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[hive]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Hive source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/kafka"},"kafka")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[kafka]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Kafka source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/kafka-connect"},"kafka-connect")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[kafka-connect]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Kafka connect source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/ldap"},"ldap")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[ldap]'")," (",(0,n.yg)("a",{parentName:"td",href:"https://www.python-ldap.org/en/python-ldap-3.3.0/installing.html#build-prerequisites"},"extra requirements"),")"),(0,n.yg)("td",{parentName:"tr",align:null},"LDAP source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/looker"},"looker")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[looker]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Looker source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/looker#module-lookml"},"lookml")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[lookml]'")),(0,n.yg)("td",{parentName:"tr",align:null},"LookML source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/metabase"},"metabase")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[metabase]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Metabase source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/mode"},"mode")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[mode]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Mode Analytics source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/mongodb"},"mongodb")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[mongodb]'")),(0,n.yg)("td",{parentName:"tr",align:null},"MongoDB source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/mssql"},"mssql")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[mssql]'")),(0,n.yg)("td",{parentName:"tr",align:null},"SQL Server source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/mysql"},"mysql")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[mysql]'")),(0,n.yg)("td",{parentName:"tr",align:null},"MySQL source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/mariadb"},"mariadb")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[mariadb]'")),(0,n.yg)("td",{parentName:"tr",align:null},"MariaDB source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/openapi"},"openapi")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[openapi]'")),(0,n.yg)("td",{parentName:"tr",align:null},"OpenApi Source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/oracle"},"oracle")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[oracle]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Oracle source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/postgres"},"postgres")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[postgres]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Postgres source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/redash"},"redash")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[redash]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Redash source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/redshift"},"redshift")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[redshift]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Redshift source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/sagemaker"},"sagemaker")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[sagemaker]'")),(0,n.yg)("td",{parentName:"tr",align:null},"AWS SageMaker source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/salesforce"},"salesforce")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[salesforce]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Salesforce source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/snowflake"},"snowflake")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[snowflake]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Snowflake source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/sqlalchemy"},"sqlalchemy")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[sqlalchemy]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Generic SQLAlchemy source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/superset"},"superset")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[superset]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Superset source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/tableau"},"tableau")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[tableau]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Tableau source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/trino"},"trino")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[trino]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Trino source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/trino"},"starburst-trino-usage")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[starburst-trino-usage]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Starburst Trino usage statistics source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/nifi"},"nifi")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[nifi]'")),(0,n.yg)("td",{parentName:"tr",align:null},"NiFi source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/powerbi#module-powerbi"},"powerbi")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[powerbi]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Microsoft Power BI source")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/generated/ingestion/sources/powerbi#module-powerbi-report-server"},"powerbi-report-server")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[powerbi-report-server]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Microsoft Power BI Report Server source")))),(0,n.yg)("h3",{id:"debugutility-plugins"},"Debug/Utility Plugins"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Plugin Name"),(0,n.yg)("th",{parentName:"tr",align:null},"Install Command"),(0,n.yg)("th",{parentName:"tr",align:null},"Provides"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"debug-recording"),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[debug-recording]'")),(0,n.yg)("td",{parentName:"tr",align:null},"Record and replay ingestion runs for debugging (Beta)")))),(0,n.yg)("h3",{id:"sinks"},"Sinks"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Plugin Name"),(0,n.yg)("th",{parentName:"tr",align:null},"Install Command"),(0,n.yg)("th",{parentName:"tr",align:null},"Provides"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/metadata-ingestion/sink_docs/metadata-file"},"metadata-file")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("em",{parentName:"td"},"included by default")),(0,n.yg)("td",{parentName:"tr",align:null},"File source and sink")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/metadata-ingestion/sink_docs/console"},"console")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("em",{parentName:"td"},"included by default")),(0,n.yg)("td",{parentName:"tr",align:null},"Console sink")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/metadata-ingestion/sink_docs/datahub"},"datahub-rest")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[datahub-rest]'")),(0,n.yg)("td",{parentName:"tr",align:null},"DataHub sink over REST API")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/metadata-ingestion/sink_docs/datahub"},"datahub-kafka")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[datahub-kafka]'")),(0,n.yg)("td",{parentName:"tr",align:null},"DataHub sink over Kafka")))),(0,n.yg)("p",null,"These plugins can be mixed and matched as desired. For example:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"pip install 'acryl-datahub[bigquery,datahub-rest]'\n")),(0,n.yg)("h3",{id:"check-the-active-plugins"},"Check the active plugins"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"datahub check plugins\n")),(0,n.yg)("h2",{id:"release-notes-and-cli-versions"},"Release Notes and CLI versions"),(0,n.yg)("p",null,"The server release notes can be found in ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/releases"},"github releases"),". These releases are done approximately every week on a regular cadence unless a blocking issue or regression is discovered."),(0,n.yg)("p",null,"CLI release is made through a different repository and release notes can be found in ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/acryldata/datahub/releases"},"acryldata releases"),". At least one release which is tied to the server release is always made alongwith the server release. Multiple other bigfix releases are made in between based on amount of fixes that are merged between the server release mentioned above."),(0,n.yg)("p",null,"If server with version ",(0,n.yg)("inlineCode",{parentName:"p"},"0.8.28")," is being used then CLI used to connect to it should be ",(0,n.yg)("inlineCode",{parentName:"p"},"0.8.28.x"),". Tests of new CLI are not ran with older server versions so it is not recommended to update the CLI if the server is not updated."))}m.isMDXComponent=!0}}]);