"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[25652],{15680:(e,a,t)=>{t.d(a,{xA:()=>u,yg:()=>c});var n=t(96540);function i(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function l(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?r(Object(t),!0).forEach((function(a){i(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function o(e,a){if(null==e)return{};var t,n,i=function(e,a){if(null==e)return{};var t,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||(i[t]=e[t]);return i}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=n.createContext({}),d=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):l(l({},a),e)),t},u=function(e){var a=d(e.components);return n.createElement(s.Provider,{value:a},e.children)},p="mdxType",g={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},m=n.forwardRef((function(e,a){var t=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),p=d(t),m=i,c=p["".concat(s,".").concat(m)]||p[m]||g[m]||r;return t?n.createElement(c,l(l({ref:a},u),{},{components:t})):n.createElement(c,l({ref:a},u))}));function c(e,a){var t=arguments,i=a&&a.mdxType;if("string"==typeof e||i){var r=t.length,l=new Array(r);l[0]=m;var o={};for(var s in a)hasOwnProperty.call(a,s)&&(o[s]=a[s]);o.originalType=e,o[p]="string"==typeof e?e:i,l[1]=o;for(var d=2;d<r;d++)l[d]=t[d];return n.createElement.apply(null,l)}return n.createElement.apply(null,t)}m.displayName="MDXCreateElement"},91919:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>u,contentTitle:()=>s,default:()=>c,frontMatter:()=>o,metadata:()=>d,toc:()=>p});t(96540);var n=t(15680);function i(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){return a=null!=a?a:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):function(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))})),e}function l(e,a){if(null==e)return{};var t,n,i=function(e,a){if(null==e)return{};var t,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||(i[t]=e[t]);return i}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}const o={title:"DataJob Entity",slug:"/metadata-integration/java/docs/sdk-v2/datajob-entity",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/docs/sdk-v2/datajob-entity.md"},s="DataJob Entity",d={unversionedId:"metadata-integration/java/docs/sdk-v2/datajob-entity",id:"metadata-integration/java/docs/sdk-v2/datajob-entity",title:"DataJob Entity",description:"The DataJob entity represents a unit of work in a data processing pipeline (e.g., an Airflow task, a dbt model, a Spark job). DataJobs belong to DataFlows (pipelines) and can have lineage to datasets and other DataJobs. This guide covers comprehensive DataJob operations in SDK V2.",source:"@site/genDocs/metadata-integration/java/docs/sdk-v2/datajob-entity.md",sourceDirName:"metadata-integration/java/docs/sdk-v2",slug:"/metadata-integration/java/docs/sdk-v2/datajob-entity",permalink:"/docs/metadata-integration/java/docs/sdk-v2/datajob-entity",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/docs/sdk-v2/datajob-entity.md",tags:[],version:"current",frontMatter:{title:"DataJob Entity",slug:"/metadata-integration/java/docs/sdk-v2/datajob-entity",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/docs/sdk-v2/datajob-entity.md"},sidebar:"overviewSidebar",previous:{title:"DataFlow Entity",permalink:"/docs/metadata-integration/java/docs/sdk-v2/dataflow-entity"},next:{title:"MLModel Entity",permalink:"/docs/metadata-integration/java/docs/sdk-v2/mlmodel-entity"}},u={},p=[{value:"Creating a DataJob",id:"creating-a-datajob",level:2},{value:"Minimal DataJob",id:"minimal-datajob",level:3},{value:"With Cluster",id:"with-cluster",level:3},{value:"With Metadata",id:"with-metadata",level:3},{value:"With Custom Properties",id:"with-custom-properties",level:3},{value:"URN Construction",id:"urn-construction",level:2},{value:"Description Operations",id:"description-operations",level:2},{value:"Setting Description",id:"setting-description",level:3},{value:"Reading Description",id:"reading-description",level:3},{value:"Display Name Operations",id:"display-name-operations",level:2},{value:"Setting Name",id:"setting-name",level:3},{value:"Reading Name",id:"reading-name",level:3},{value:"Tags",id:"tags",level:2},{value:"Adding Tags",id:"adding-tags",level:3},{value:"Removing Tags",id:"removing-tags",level:3},{value:"Tag Chaining",id:"tag-chaining",level:3},{value:"Owners",id:"owners",level:2},{value:"Adding Owners",id:"adding-owners",level:3},{value:"Removing Owners",id:"removing-owners",level:3},{value:"Owner Types",id:"owner-types",level:3},{value:"Glossary Terms",id:"glossary-terms",level:2},{value:"Adding Terms",id:"adding-terms",level:3},{value:"Removing Terms",id:"removing-terms",level:3},{value:"Term Chaining",id:"term-chaining",level:3},{value:"Domain",id:"domain",level:2},{value:"Setting Domain",id:"setting-domain",level:3},{value:"Removing Domain",id:"removing-domain",level:3},{value:"Custom Properties",id:"custom-properties",level:2},{value:"Adding Individual Properties",id:"adding-individual-properties",level:3},{value:"Setting All Properties",id:"setting-all-properties",level:3},{value:"Removing Properties",id:"removing-properties",level:3},{value:"Lineage Operations",id:"lineage-operations",level:2},{value:"Understanding Input and Output Datasets",id:"understanding-input-and-output-datasets",level:3},{value:"Input Datasets",id:"input-datasets",level:3},{value:"Adding Single Inlet",id:"adding-single-inlet",level:4},{value:"Adding Multiple Inlets",id:"adding-multiple-inlets",level:4},{value:"Setting All Inlets at Once",id:"setting-all-inlets-at-once",level:4},{value:"Removing Inlets",id:"removing-inlets",level:4},{value:"Reading Inlets",id:"reading-inlets",level:4},{value:"Output Datasets (Outlets)",id:"output-datasets-outlets",level:3},{value:"Adding Single Outlet",id:"adding-single-outlet",level:4},{value:"Adding Multiple Outlets",id:"adding-multiple-outlets",level:4},{value:"Setting All Outlets at Once",id:"setting-all-outlets-at-once",level:4},{value:"Removing Outlets",id:"removing-outlets",level:4},{value:"Reading Outlets",id:"reading-outlets",level:4},{value:"DataJob Dependencies",id:"datajob-dependencies",level:3},{value:"Adding Job Dependencies",id:"adding-job-dependencies",level:4},{value:"Chaining Job Dependencies",id:"chaining-job-dependencies",level:4},{value:"Removing Job Dependencies",id:"removing-job-dependencies",level:4},{value:"Reading Job Dependencies",id:"reading-job-dependencies",level:4},{value:"Example: Airflow Task Dependencies",id:"example-airflow-task-dependencies",level:4},{value:"Field-Level Lineage",id:"field-level-lineage",level:3},{value:"Adding Input Fields",id:"adding-input-fields",level:4},{value:"Adding Output Fields",id:"adding-output-fields",level:4},{value:"Removing Fields",id:"removing-fields",level:4},{value:"Reading Fields",id:"reading-fields",level:4},{value:"Example: Column-Level Tracking",id:"example-column-level-tracking",level:4},{value:"Fine-Grained Lineage",id:"fine-grained-lineage",level:3},{value:"Adding Fine-Grained Lineage",id:"adding-fine-grained-lineage",level:4},{value:"Common Transformation Types",id:"common-transformation-types",level:4},{value:"Removing Fine-Grained Lineage",id:"removing-fine-grained-lineage",level:4},{value:"Reading Fine-Grained Lineage",id:"reading-fine-grained-lineage",level:4},{value:"Example: Complex Aggregation",id:"example-complex-aggregation",level:4},{value:"Example: Multi-Column Derivation",id:"example-multi-column-derivation",level:4},{value:"Confidence Scores",id:"confidence-scores",level:4},{value:"Complete Lineage Example",id:"complete-lineage-example",level:3},{value:"Lineage Flow Visualization",id:"lineage-flow-visualization",level:3},{value:"ETL Pipeline Example",id:"etl-pipeline-example",level:3},{value:"Updating Lineage",id:"updating-lineage",level:3},{value:"Lineage Best Practices",id:"lineage-best-practices",level:3},{value:"Common Patterns",id:"common-patterns",level:3},{value:"Multiple Sources to Single Destination",id:"multiple-sources-to-single-destination",level:4},{value:"Single Source to Multiple Destinations",id:"single-source-to-multiple-destinations",level:4},{value:"Cross-Platform Lineage",id:"cross-platform-lineage",level:4},{value:"Complete Example",id:"complete-example",level:2},{value:"Updating Existing DataJobs",id:"updating-existing-datajobs",level:2},{value:"Load and Modify",id:"load-and-modify",level:3},{value:"Incremental Updates",id:"incremental-updates",level:3},{value:"Builder Options Reference",id:"builder-options-reference",level:2},{value:"Common Patterns",id:"common-patterns-1",level:2},{value:"Creating Multiple DataJobs",id:"creating-multiple-datajobs",level:3},{value:"Batch Metadata Addition",id:"batch-metadata-addition",level:3},{value:"Conditional Metadata",id:"conditional-metadata",level:3},{value:"DataJob vs DataFlow",id:"datajob-vs-dataflow",level:2},{value:"Orchestrator Examples",id:"orchestrator-examples",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Examples",id:"examples",level:2},{value:"Basic DataJob Creation",id:"basic-datajob-creation",level:3},{value:"Comprehensive DataJob with Metadata and Lineage",id:"comprehensive-datajob-with-metadata-and-lineage",level:3},{value:"DataJob Lineage Operations",id:"datajob-lineage-operations",level:3}],g={toc:p},m="wrapper";function c(e){var{components:a}=e,t=l(e,["components"]);return(0,n.yg)(m,r(function(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{},n=Object.keys(t);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(t).filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable})))),n.forEach((function(a){i(e,a,t[a])}))}return e}({},g,t),{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"datajob-entity"},"DataJob Entity"),(0,n.yg)("p",null,"The DataJob entity represents a unit of work in a data processing pipeline (e.g., an Airflow task, a dbt model, a Spark job). DataJobs belong to DataFlows (pipelines) and can have lineage to datasets and other DataJobs. This guide covers comprehensive DataJob operations in SDK V2."),(0,n.yg)("h2",{id:"creating-a-datajob"},"Creating a DataJob"),(0,n.yg)("h3",{id:"minimal-datajob"},"Minimal DataJob"),(0,n.yg)("p",null,"Orchestrator, flowId, and jobId are required:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'DataJob dataJob = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("my_dag")\n    .jobId("my_task")\n    .build();\n')),(0,n.yg)("h3",{id:"with-cluster"},"With Cluster"),(0,n.yg)("p",null,'Specify cluster (default is "prod"):'),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'DataJob dataJob = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("analytics_pipeline")\n    .cluster("staging")\n    .jobId("transform_data")\n    .build();\n// URN: urn:li:dataJob:(urn:li:dataFlow:(airflow,analytics_pipeline,staging),transform_data)\n')),(0,n.yg)("h3",{id:"with-metadata"},"With Metadata"),(0,n.yg)("p",null,"Add description and name at construction (requires both name AND type):"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'DataJob dataJob = DataJob.builder()\n    .orchestrator("dagster")\n    .flowId("customer_etl")\n    .cluster("prod")\n    .jobId("load_customers")\n    .description("Loads customer data from PostgreSQL to Snowflake")\n    .name("Load Customers to DWH")\n    .type("BATCH")\n    .build();\n')),(0,n.yg)("h3",{id:"with-custom-properties"},"With Custom Properties"),(0,n.yg)("p",null,"Include custom properties in builder (requires name and type when using customProperties):"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'Map<String, String> props = new HashMap<>();\nprops.put("schedule", "0 2 * * *");\nprops.put("retries", "3");\nprops.put("timeout", "3600");\n\nDataJob dataJob = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("daily_pipeline")\n    .jobId("my_task")\n    .name("My Daily Task")\n    .type("BATCH")\n    .customProperties(props)\n    .build();\n')),(0,n.yg)("h2",{id:"urn-construction"},"URN Construction"),(0,n.yg)("p",null,"DataJob URNs follow the pattern:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"urn:li:dataJob:(urn:li:dataFlow:({orchestrator},{flowId},{cluster}),{jobId})\n")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Automatic URN creation:")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'DataJob dataJob = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("finance_reporting")\n    .cluster("prod")\n    .jobId("aggregate_transactions")\n    .build();\n\nDataJobUrn urn = dataJob.getDataJobUrn();\n// urn:li:dataJob:(urn:li:dataFlow:(airflow,finance_reporting,prod),aggregate_transactions)\n')),(0,n.yg)("h2",{id:"description-operations"},"Description Operations"),(0,n.yg)("h3",{id:"setting-description"},"Setting Description"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'dataJob.setDescription("Processes daily customer transactions");\n')),(0,n.yg)("h3",{id:"reading-description"},"Reading Description"),(0,n.yg)("p",null,"Get description (lazy-loaded from DataJobInfo):"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"String description = dataJob.getDescription();\n")),(0,n.yg)("h2",{id:"display-name-operations"},"Display Name Operations"),(0,n.yg)("h3",{id:"setting-name"},"Setting Name"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'dataJob.setName("Process Customer Transactions");\n')),(0,n.yg)("h3",{id:"reading-name"},"Reading Name"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"String name = dataJob.getName();\n")),(0,n.yg)("h2",{id:"tags"},"Tags"),(0,n.yg)("h3",{id:"adding-tags"},"Adding Tags"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Simple tag name (auto-prefixed)\ndataJob.addTag("critical");\n// Creates: urn:li:tag:critical\n\n// Full tag URN\ndataJob.addTag("urn:li:tag:etl");\n')),(0,n.yg)("h3",{id:"removing-tags"},"Removing Tags"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'dataJob.removeTag("critical");\ndataJob.removeTag("urn:li:tag:etl");\n')),(0,n.yg)("h3",{id:"tag-chaining"},"Tag Chaining"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'dataJob.addTag("critical")\n       .addTag("pii")\n       .addTag("production");\n')),(0,n.yg)("h2",{id:"owners"},"Owners"),(0,n.yg)("h3",{id:"adding-owners"},"Adding Owners"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'import com.linkedin.common.OwnershipType;\n\n// Technical owner\ndataJob.addOwner(\n    "urn:li:corpuser:data_team",\n    OwnershipType.TECHNICAL_OWNER\n);\n\n// Data steward\ndataJob.addOwner(\n    "urn:li:corpuser:compliance",\n    OwnershipType.DATA_STEWARD\n);\n\n// Business owner\ndataJob.addOwner(\n    "urn:li:corpuser:product_team",\n    OwnershipType.BUSINESS_OWNER\n);\n')),(0,n.yg)("h3",{id:"removing-owners"},"Removing Owners"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'dataJob.removeOwner("urn:li:corpuser:data_team");\n')),(0,n.yg)("h3",{id:"owner-types"},"Owner Types"),(0,n.yg)("p",null,"Available ownership types:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"TECHNICAL_OWNER")," - Maintains the technical implementation"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"BUSINESS_OWNER")," - Business stakeholder"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATA_STEWARD")," - Manages data quality and compliance"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DATAOWNER")," - Generic data owner"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"DEVELOPER")," - Software developer"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"PRODUCER")," - Data producer"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"CONSUMER")," - Data consumer"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"STAKEHOLDER")," - Other stakeholder")),(0,n.yg)("h2",{id:"glossary-terms"},"Glossary Terms"),(0,n.yg)("h3",{id:"adding-terms"},"Adding Terms"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'dataJob.addTerm("urn:li:glossaryTerm:DataProcessing");\ndataJob.addTerm("urn:li:glossaryTerm:ETL");\n')),(0,n.yg)("h3",{id:"removing-terms"},"Removing Terms"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'dataJob.removeTerm("urn:li:glossaryTerm:DataProcessing");\n')),(0,n.yg)("h3",{id:"term-chaining"},"Term Chaining"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'dataJob.addTerm("urn:li:glossaryTerm:DataProcessing")\n       .addTerm("urn:li:glossaryTerm:ETL")\n       .addTerm("urn:li:glossaryTerm:FinancialReporting");\n')),(0,n.yg)("h2",{id:"domain"},"Domain"),(0,n.yg)("h3",{id:"setting-domain"},"Setting Domain"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'dataJob.setDomain("urn:li:domain:Engineering");\n')),(0,n.yg)("h3",{id:"removing-domain"},"Removing Domain"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"dataJob.removeDomain();\n")),(0,n.yg)("h2",{id:"custom-properties"},"Custom Properties"),(0,n.yg)("h3",{id:"adding-individual-properties"},"Adding Individual Properties"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'dataJob.addCustomProperty("schedule", "0 2 * * *");\ndataJob.addCustomProperty("retries", "3");\ndataJob.addCustomProperty("timeout", "3600");\n')),(0,n.yg)("h3",{id:"setting-all-properties"},"Setting All Properties"),(0,n.yg)("p",null,"Replace all custom properties:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'Map<String, String> properties = new HashMap<>();\nproperties.put("schedule", "0 2 * * *");\nproperties.put("retries", "3");\nproperties.put("timeout", "3600");\nproperties.put("priority", "high");\n\ndataJob.setCustomProperties(properties);\n')),(0,n.yg)("h3",{id:"removing-properties"},"Removing Properties"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'dataJob.removeCustomProperty("timeout");\n')),(0,n.yg)("h2",{id:"lineage-operations"},"Lineage Operations"),(0,n.yg)("p",null,"DataJob lineage defines the relationship between data jobs and the datasets they operate on. Lineage enables impact analysis, data provenance tracking, and understanding data flows through your pipelines."),(0,n.yg)("p",null,"The DataJob SDK supports four types of lineage:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Dataset-level lineage")," - Track which datasets a job reads from and writes to"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"DataJob dependencies")," - Track which jobs depend on other jobs (task dependencies)"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Field-level lineage")," - Track specific columns consumed and produced"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Fine-grained lineage")," - Track column-to-column transformations")),(0,n.yg)("h3",{id:"understanding-input-and-output-datasets"},"Understanding Input and Output Datasets"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Input Datasets")," - Datasets that the job reads from:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Represent source data for the job"),(0,n.yg)("li",{parentName:"ul"},"Create upstream lineage: Dataset \u2192 DataJob")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Output Datasets")," - Datasets that the job writes to:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Represent destination data from the job"),(0,n.yg)("li",{parentName:"ul"},"Create downstream lineage: DataJob \u2192 Dataset")),(0,n.yg)("h3",{id:"input-datasets"},"Input Datasets"),(0,n.yg)("h4",{id:"adding-single-inlet"},"Adding Single Inlet"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Using string URN\ndataJob.addInputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.transactions,PROD)");\n\n// Using DatasetUrn object for type safety\nDatasetUrn datasetUrn = DatasetUrn.createFromString(\n    "urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.transactions,PROD)"\n);\ndataJob.addInputDataset(datasetUrn);\n')),(0,n.yg)("h4",{id:"adding-multiple-inlets"},"Adding Multiple Inlets"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Chain multiple calls\ndataJob.addInputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.transactions,PROD)")\n       .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.customers,PROD)")\n       .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:kafka,events.purchases,PROD)");\n')),(0,n.yg)("h4",{id:"setting-all-inlets-at-once"},"Setting All Inlets at Once"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'List<String> inletUrns = Arrays.asList(\n    "urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.orders,PROD)",\n    "urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.customers,PROD)",\n    "urn:li:dataset:(urn:li:dataPlatform:kafka,events.clicks,PROD)"\n);\ndataJob.setInputDatasets(inletUrns);\n')),(0,n.yg)("h4",{id:"removing-inlets"},"Removing Inlets"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Remove single inlet\ndataJob.removeInputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.transactions,PROD)");\n\n// Or using DatasetUrn\nDatasetUrn datasetUrn = DatasetUrn.createFromString(\n    "urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.transactions,PROD)"\n);\ndataJob.removeInputDataset(datasetUrn);\n')),(0,n.yg)("h4",{id:"reading-inlets"},"Reading Inlets"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Get all inlets (lazy-loaded)\nList<DatasetUrn> inlets = dataJob.getInputDatasets();\nfor (DatasetUrn inlet : inlets) {\n    System.out.println("Input: " + inlet);\n}\n')),(0,n.yg)("h3",{id:"output-datasets-outlets"},"Output Datasets (Outlets)"),(0,n.yg)("h4",{id:"adding-single-outlet"},"Adding Single Outlet"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Using string URN\ndataJob.addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.sales_summary,PROD)");\n\n// Using DatasetUrn object\nDatasetUrn datasetUrn = DatasetUrn.createFromString(\n    "urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.sales_summary,PROD)"\n);\ndataJob.addOutputDataset(datasetUrn);\n')),(0,n.yg)("h4",{id:"adding-multiple-outlets"},"Adding Multiple Outlets"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'dataJob.addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.daily_summary,PROD)")\n       .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.monthly_summary,PROD)")\n       .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:s3,reports/summary.parquet,PROD)");\n')),(0,n.yg)("h4",{id:"setting-all-outlets-at-once"},"Setting All Outlets at Once"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'List<String> outletUrns = Arrays.asList(\n    "urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.customer_metrics,PROD)",\n    "urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.product_metrics,PROD)"\n);\ndataJob.setOutputDatasets(outletUrns);\n')),(0,n.yg)("h4",{id:"removing-outlets"},"Removing Outlets"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Remove single outlet\ndataJob.removeOutputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.sales_summary,PROD)");\n\n// Or using DatasetUrn\nDatasetUrn datasetUrn = DatasetUrn.createFromString(\n    "urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.sales_summary,PROD)"\n);\ndataJob.removeOutputDataset(datasetUrn);\n')),(0,n.yg)("h4",{id:"reading-outlets"},"Reading Outlets"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Get all outlets (lazy-loaded)\nList<DatasetUrn> outlets = dataJob.getOutputDatasets();\nfor (DatasetUrn outlet : outlets) {\n    System.out.println("Output: " + outlet);\n}\n')),(0,n.yg)("h3",{id:"datajob-dependencies"},"DataJob Dependencies"),(0,n.yg)("p",null,"DataJob dependencies model task-to-task relationships within workflows. This enables DataHub to track which jobs depend on other jobs completing first."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Use cases:")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Airflow task dependencies (task A \u2192 task B \u2192 task C)"),(0,n.yg)("li",{parentName:"ul"},"Cross-DAG dependencies (jobs in different pipelines)"),(0,n.yg)("li",{parentName:"ul"},"Workflow orchestration visualization")),(0,n.yg)("h4",{id:"adding-job-dependencies"},"Adding Job Dependencies"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Using string URN\ndataJob.addInputDataJob("urn:li:dataJob:(urn:li:dataFlow:(airflow,pipeline,prod),upstream_task)");\n\n// Using DataJobUrn object for type safety\nDataJobUrn upstreamJob = DataJobUrn.createFromString(\n    "urn:li:dataJob:(urn:li:dataFlow:(airflow,pipeline,prod),upstream_task)"\n);\ndataJob.addInputDataJob(upstreamJob);\n')),(0,n.yg)("h4",{id:"chaining-job-dependencies"},"Chaining Job Dependencies"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Multiple dependencies (task runs after all complete)\ndataJob.addInputDataJob("urn:li:dataJob:(urn:li:dataFlow:(airflow,pipeline,prod),task_1)")\n       .addInputDataJob("urn:li:dataJob:(urn:li:dataFlow:(airflow,pipeline,prod),task_2)")\n       .addInputDataJob("urn:li:dataJob:(urn:li:dataFlow:(dagster,other_pipeline,prod),external_task)");\n')),(0,n.yg)("h4",{id:"removing-job-dependencies"},"Removing Job Dependencies"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Remove single dependency\ndataJob.removeInputDataJob("urn:li:dataJob:(urn:li:dataFlow:(airflow,pipeline,prod),task_1)");\n\n// Or using DataJobUrn\nDataJobUrn jobUrn = DataJobUrn.createFromString(\n    "urn:li:dataJob:(urn:li:dataFlow:(airflow,pipeline,prod),task_1)"\n);\ndataJob.removeInputDataJob(jobUrn);\n')),(0,n.yg)("h4",{id:"reading-job-dependencies"},"Reading Job Dependencies"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Get all upstream job dependencies (lazy-loaded)\nList<DataJobUrn> dependencies = dataJob.getInputDataJobs();\nfor (DataJobUrn dependency : dependencies) {\n    System.out.println("Depends on: " + dependency);\n}\n')),(0,n.yg)("h4",{id:"example-airflow-task-dependencies"},"Example: Airflow Task Dependencies"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Model a typical Airflow DAG task chain\nDataJob extractTask = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("etl_pipeline")\n    .jobId("extract_data")\n    .build();\n\nDataJob validateTask = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("etl_pipeline")\n    .jobId("validate_data")\n    .build();\n\n// validate_data depends on extract_data\nvalidateTask.addInputDataJob(extractTask.getUrn().toString());\n\nDataJob transformTask = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("etl_pipeline")\n    .jobId("transform_data")\n    .build();\n\n// transform_data depends on validate_data\ntransformTask.addInputDataJob(validateTask.getUrn().toString());\n\n// Save all tasks\nclient.entities().upsert(extractTask);\nclient.entities().upsert(validateTask);\nclient.entities().upsert(transformTask);\n\n// Result: extract_data \u2192 validate_data \u2192 transform_data\n')),(0,n.yg)("h3",{id:"field-level-lineage"},"Field-Level Lineage"),(0,n.yg)("p",null,"Field-level lineage tracks which specific columns (fields) a job consumes and produces. This provides finer granularity than dataset-level lineage."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Use cases:")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Track which columns are read/written by transformations"),(0,n.yg)("li",{parentName:"ul"},"Understand field-level dependencies"),(0,n.yg)("li",{parentName:"ul"},"Validate that jobs only access necessary columns")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Field URN Format:")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"urn:li:schemaField:(DATASET_URN,COLUMN_NAME)\n")),(0,n.yg)("h4",{id:"adding-input-fields"},"Adding Input Fields"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Track which columns the job reads\ndataJob.addInputField("urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,db.orders,PROD),order_id)");\ndataJob.addInputField("urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,db.orders,PROD),customer_id)");\ndataJob.addInputField("urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,db.orders,PROD),total_amount)");\n')),(0,n.yg)("h4",{id:"adding-output-fields"},"Adding Output Fields"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Track which columns the job writes\ndataJob.addOutputField("urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.sales,PROD),order_id)");\ndataJob.addOutputField("urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.sales,PROD),customer_id)");\ndataJob.addOutputField("urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.sales,PROD),revenue)");\n')),(0,n.yg)("h4",{id:"removing-fields"},"Removing Fields"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Remove field lineage\ndataJob.removeInputField("urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,db.orders,PROD),order_id)");\ndataJob.removeOutputField("urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.sales,PROD),revenue)");\n')),(0,n.yg)("h4",{id:"reading-fields"},"Reading Fields"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Get all input fields (lazy-loaded)\nList<Urn> inputFields = dataJob.getInputFields();\nfor (Urn field : inputFields) {\n    System.out.println("Reads field: " + field);\n}\n\n// Get all output fields (lazy-loaded)\nList<Urn> outputFields = dataJob.getOutputFields();\nfor (Urn field : outputFields) {\n    System.out.println("Writes field: " + field);\n}\n')),(0,n.yg)("h4",{id:"example-column-level-tracking"},"Example: Column-Level Tracking"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'DataJob aggregateJob = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("analytics")\n    .jobId("aggregate_sales")\n    .description("Aggregates sales data by customer")\n    .name("Aggregate Sales by Customer")\n    .type("BATCH")\n    .build();\n\n// Dataset-level lineage\naggregateJob.addInputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.transactions,PROD)");\naggregateJob.addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.customer_sales,PROD)");\n\n// Field-level lineage - specify exact columns used\naggregateJob.addInputField("urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.transactions,PROD),customer_id)");\naggregateJob.addInputField("urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.transactions,PROD),amount)");\naggregateJob.addInputField("urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.transactions,PROD),transaction_date)");\n\naggregateJob.addOutputField("urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.customer_sales,PROD),customer_id)");\naggregateJob.addOutputField("urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.customer_sales,PROD),total_sales)");\naggregateJob.addOutputField("urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.customer_sales,PROD),transaction_count)");\n\nclient.entities().upsert(aggregateJob);\n')),(0,n.yg)("h3",{id:"fine-grained-lineage"},"Fine-Grained Lineage"),(0,n.yg)("p",null,"Fine-grained lineage captures column-to-column transformations, showing exactly which input columns produce which output columns and how they're transformed."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Use cases:")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},'Document transformation logic (e.g., "SUM(amount)")'),(0,n.yg)("li",{parentName:"ul"},"Track column-level impact analysis"),(0,n.yg)("li",{parentName:"ul"},"Understand data derivations"),(0,n.yg)("li",{parentName:"ul"},"Compliance and audit trails")),(0,n.yg)("h4",{id:"adding-fine-grained-lineage"},"Adding Fine-Grained Lineage"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Basic transformation (no confidence score)\ndataJob.addFineGrainedLineage(\n    "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.orders,PROD),customer_id)",\n    "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.sales,PROD),customer_id)",\n    "IDENTITY",\n    null\n);\n\n// Transformation with confidence score (0.0 to 1.0)\ndataJob.addFineGrainedLineage(\n    "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.orders,PROD),amount)",\n    "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.sales,PROD),revenue)",\n    "SUM",\n    1.0f  // High confidence\n);\n')),(0,n.yg)("h4",{id:"common-transformation-types"},"Common Transformation Types"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// IDENTITY - direct copy\ndataJob.addFineGrainedLineage(upstream, downstream, "IDENTITY", 1.0f);\n\n// Aggregations\ndataJob.addFineGrainedLineage(upstream, downstream, "SUM", 1.0f);\ndataJob.addFineGrainedLineage(upstream, downstream, "COUNT", 1.0f);\ndataJob.addFineGrainedLineage(upstream, downstream, "AVG", 1.0f);\ndataJob.addFineGrainedLineage(upstream, downstream, "MAX", 1.0f);\ndataJob.addFineGrainedLineage(upstream, downstream, "MIN", 1.0f);\n\n// String operations\ndataJob.addFineGrainedLineage(upstream, downstream, "CONCAT", 0.9f);\ndataJob.addFineGrainedLineage(upstream, downstream, "UPPER", 1.0f);\ndataJob.addFineGrainedLineage(upstream, downstream, "SUBSTRING", 0.95f);\n\n// Date operations\ndataJob.addFineGrainedLineage(upstream, downstream, "DATE_TRUNC", 1.0f);\ndataJob.addFineGrainedLineage(upstream, downstream, "EXTRACT", 1.0f);\n\n// Custom transformations\ndataJob.addFineGrainedLineage(upstream, downstream, "CUSTOM_FUNCTION", 0.8f);\n')),(0,n.yg)("h4",{id:"removing-fine-grained-lineage"},"Removing Fine-Grained Lineage"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Remove specific transformation\ndataJob.removeFineGrainedLineage(\n    "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.orders,PROD),amount)",\n    "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.sales,PROD),revenue)",\n    "SUM",\n    null  // queryUrn (optional)\n);\n')),(0,n.yg)("h4",{id:"reading-fine-grained-lineage"},"Reading Fine-Grained Lineage"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Get all fine-grained lineage (lazy-loaded)\nList<FineGrainedLineage> lineages = dataJob.getFineGrainedLineages();\nfor (FineGrainedLineage lineage : lineages) {\n    System.out.println("Upstreams: " + lineage.getUpstreams());\n    System.out.println("Downstreams: " + lineage.getDownstreams());\n    System.out.println("Transformation: " + lineage.getTransformOperation());\n    System.out.println("Confidence: " + lineage.getConfidenceScore());\n}\n')),(0,n.yg)("h4",{id:"example-complex-aggregation"},"Example: Complex Aggregation"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'DataJob salesAggregation = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("analytics")\n    .jobId("daily_sales_summary")\n    .name("Daily Sales Summary")\n    .type("BATCH")\n    .build();\n\n// Dataset-level lineage\nsalesAggregation.addInputDataset("urn:li:dataset:(urn:li:dataPlatform:postgres,sales.transactions,PROD)");\nsalesAggregation.addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.daily_summary,PROD)");\n\n// Fine-grained transformations\nString inputDataset = "urn:li:dataset:(urn:li:dataPlatform:postgres,sales.transactions,PROD)";\nString outputDataset = "urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.daily_summary,PROD)";\n\n// Date is copied directly\nsalesAggregation.addFineGrainedLineage(\n    "urn:li:schemaField:(" + inputDataset + ",transaction_date)",\n    "urn:li:schemaField:(" + outputDataset + ",date)",\n    "IDENTITY",\n    1.0f\n);\n\n// Revenue is SUM of amounts\nsalesAggregation.addFineGrainedLineage(\n    "urn:li:schemaField:(" + inputDataset + ",amount)",\n    "urn:li:schemaField:(" + outputDataset + ",total_revenue)",\n    "SUM",\n    1.0f\n);\n\n// Transaction count\nsalesAggregation.addFineGrainedLineage(\n    "urn:li:schemaField:(" + inputDataset + ",transaction_id)",\n    "urn:li:schemaField:(" + outputDataset + ",transaction_count)",\n    "COUNT",\n    1.0f\n);\n\n// Average order value\nsalesAggregation.addFineGrainedLineage(\n    "urn:li:schemaField:(" + inputDataset + ",amount)",\n    "urn:li:schemaField:(" + outputDataset + ",avg_order_value)",\n    "AVG",\n    1.0f\n);\n\nclient.entities().upsert(salesAggregation);\n')),(0,n.yg)("h4",{id:"example-multi-column-derivation"},"Example: Multi-Column Derivation"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Model a transformation where output depends on multiple input columns\nDataJob enrichmentJob = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("enrichment")\n    .jobId("enrich_customer_data")\n    .build();\n\nString inputDataset = "urn:li:dataset:(urn:li:dataPlatform:postgres,crm.customers,PROD)";\nString outputDataset = "urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.customers_enriched,PROD)";\n\n// full_name = CONCAT(first_name, \' \', last_name)\n// Both first_name and last_name contribute to full_name\nenrichmentJob.addFineGrainedLineage(\n    "urn:li:schemaField:(" + inputDataset + ",first_name)",\n    "urn:li:schemaField:(" + outputDataset + ",full_name)",\n    "CONCAT",\n    1.0f\n);\n\nenrichmentJob.addFineGrainedLineage(\n    "urn:li:schemaField:(" + inputDataset + ",last_name)",\n    "urn:li:schemaField:(" + outputDataset + ",full_name)",\n    "CONCAT",\n    1.0f\n);\n\n// email_domain = SUBSTRING(email, POSITION(\'@\', email) + 1)\nenrichmentJob.addFineGrainedLineage(\n    "urn:li:schemaField:(" + inputDataset + ",email)",\n    "urn:li:schemaField:(" + outputDataset + ",email_domain)",\n    "SUBSTRING",\n    1.0f\n);\n\nclient.entities().upsert(enrichmentJob);\n')),(0,n.yg)("h4",{id:"confidence-scores"},"Confidence Scores"),(0,n.yg)("p",null,"Confidence scores (0.0 to 1.0) indicate how certain you are about the transformation:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"1.0")," - Exact, deterministic transformation (e.g., IDENTITY, SUM)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"0.9-0.99")," - High confidence (e.g., simple string operations)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"0.7-0.89")," - Medium confidence (e.g., complex transformations with some uncertainty)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"0.5-0.69")," - Low confidence (e.g., ML-derived lineage, heuristic-based)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"< 0.5")," - Very uncertain (generally not recommended)")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// High confidence - exact transformation known\ndataJob.addFineGrainedLineage(source, target, "UPPER", 1.0f);\n\n// Medium confidence - inferred from SQL parsing\ndataJob.addFineGrainedLineage(source, target, "CASE_WHEN", 0.85f);\n\n// Low confidence - ML-predicted transformation\ndataJob.addFineGrainedLineage(source, target, "INFERRED", 0.6f);\n')),(0,n.yg)("h3",{id:"complete-lineage-example"},"Complete Lineage Example"),(0,n.yg)("p",null,"This example demonstrates all four types of lineage working together:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Create upstream validation job\nDataJob validateJob = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("analytics_pipeline")\n    .cluster("prod")\n    .jobId("validate_transactions")\n    .name("Validate Transaction Data")\n    .type("BATCH")\n    .build();\n\nvalidateJob.addInputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.transactions,PROD)")\n           .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,validated.transactions,PROD)");\n\nclient.entities().upsert(validateJob);\n\n// Create main transformation job with comprehensive lineage\nDataJob transformJob = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("analytics_pipeline")\n    .cluster("prod")\n    .jobId("aggregate_sales")\n    .description("Aggregates daily sales data from multiple validated sources")\n    .name("Aggregate Daily Sales")\n    .type("BATCH")\n    .build();\n\n// 1. Dataset-level lineage - Which tables are read/written\ntransformJob.addInputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,validated.transactions,PROD)")\n            .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.customers,PROD)")\n            .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.daily_sales,PROD)");\n\n// 2. DataJob dependencies - This job depends on the validation job\ntransformJob.addInputDataJob(validateJob.getUrn().toString());\n\n// 3. Field-level lineage - Which specific columns are accessed\nString transactionsDataset = "urn:li:dataset:(urn:li:dataPlatform:snowflake,validated.transactions,PROD)";\nString customersDataset = "urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.customers,PROD)";\nString outputDataset = "urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.daily_sales,PROD)";\n\n// Input fields\ntransformJob.addInputField("urn:li:schemaField:(" + transactionsDataset + ",transaction_id)")\n            .addInputField("urn:li:schemaField:(" + transactionsDataset + ",customer_id)")\n            .addInputField("urn:li:schemaField:(" + transactionsDataset + ",amount)")\n            .addInputField("urn:li:schemaField:(" + transactionsDataset + ",transaction_date)")\n            .addInputField("urn:li:schemaField:(" + customersDataset + ",customer_id)")\n            .addInputField("urn:li:schemaField:(" + customersDataset + ",customer_name)");\n\n// Output fields\ntransformJob.addOutputField("urn:li:schemaField:(" + outputDataset + ",date)")\n            .addOutputField("urn:li:schemaField:(" + outputDataset + ",customer_name)")\n            .addOutputField("urn:li:schemaField:(" + outputDataset + ",total_revenue)")\n            .addOutputField("urn:li:schemaField:(" + outputDataset + ",transaction_count)");\n\n// 4. Fine-grained lineage - Specific column-to-column transformations\n// Date column (identity transformation)\ntransformJob.addFineGrainedLineage(\n    "urn:li:schemaField:(" + transactionsDataset + ",transaction_date)",\n    "urn:li:schemaField:(" + outputDataset + ",date)",\n    "IDENTITY",\n    1.0f\n);\n\n// Customer name (join + identity)\ntransformJob.addFineGrainedLineage(\n    "urn:li:schemaField:(" + customersDataset + ",customer_name)",\n    "urn:li:schemaField:(" + outputDataset + ",customer_name)",\n    "IDENTITY",\n    1.0f\n);\n\n// Total revenue (aggregation)\ntransformJob.addFineGrainedLineage(\n    "urn:li:schemaField:(" + transactionsDataset + ",amount)",\n    "urn:li:schemaField:(" + outputDataset + ",total_revenue)",\n    "SUM",\n    1.0f\n);\n\n// Transaction count (aggregation)\ntransformJob.addFineGrainedLineage(\n    "urn:li:schemaField:(" + transactionsDataset + ",transaction_id)",\n    "urn:li:schemaField:(" + outputDataset + ",transaction_count)",\n    "COUNT",\n    1.0f\n);\n\n// Add other metadata\ntransformJob.addTag("critical")\n            .addOwner("urn:li:corpuser:data_team", OwnershipType.TECHNICAL_OWNER);\n\n// Save to DataHub\nclient.entities().upsert(transformJob);\n\n// Result: Creates comprehensive lineage showing:\n// - Job dependency: validate_transactions \u2192 aggregate_sales\n// - Dataset flow: raw.transactions \u2192 validated.transactions \u2192 analytics.daily_sales\n//                 raw.customers \u2192 analytics.daily_sales\n// - Column-level: transaction_date \u2192 date (IDENTITY)\n//                 amount \u2192 total_revenue (SUM)\n//                 transaction_id \u2192 transaction_count (COUNT)\n//                 customer_name \u2192 customer_name (IDENTITY via JOIN)\n')),(0,n.yg)("h3",{id:"lineage-flow-visualization"},"Lineage Flow Visualization"),(0,n.yg)("p",null,"The comprehensive lineage example above creates this multi-level lineage graph:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"Job-to-Job Level:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Validate Transactions  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192\u2502  Aggregate Sales Job \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nDataset Level:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 raw.transactions    \u2502\u2500\u2500\u2500\u2192\u2502 validated.transactions  \u2502\u2500\u2500\u2500\u2192\u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502  analytics.daily_sales  \u2502\n                                                           \u2502                         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502                         \u2502\n\u2502 raw.customers       \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192\u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nColumn Level (Fine-Grained):\nvalidated.transactions.transaction_date \u2500\u2500[IDENTITY]\u2500\u2500\u2192 daily_sales.date\nvalidated.transactions.amount           \u2500\u2500[SUM]\u2500\u2500\u2500\u2500\u2500\u2500\u2192 daily_sales.total_revenue\nvalidated.transactions.transaction_id   \u2500\u2500[COUNT]\u2500\u2500\u2500\u2500\u2192 daily_sales.transaction_count\nraw.customers.customer_name             \u2500\u2500[IDENTITY]\u2500\u2500\u2192 daily_sales.customer_name\n")),(0,n.yg)("h3",{id:"etl-pipeline-example"},"ETL Pipeline Example"),(0,n.yg)("p",null,"Model a complete Extract-Transform-Load pipeline:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Extract job\nDataJob extractJob = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("etl_pipeline")\n    .jobId("extract")\n    .build();\n\nextractJob.addInputDataset("urn:li:dataset:(urn:li:dataPlatform:mysql,prod.orders,PROD)")\n          .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:s3,staging/orders_raw,PROD)");\n\nclient.entities().upsert(extractJob);\n\n// Transform job\nDataJob transformJob = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("etl_pipeline")\n    .jobId("transform")\n    .build();\n\ntransformJob.addInputDataset("urn:li:dataset:(urn:li:dataPlatform:s3,staging/orders_raw,PROD)")\n            .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:s3,staging/orders_clean,PROD)");\n\nclient.entities().upsert(transformJob);\n\n// Load job\nDataJob loadJob = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("etl_pipeline")\n    .jobId("load")\n    .build();\n\nloadJob.addInputDataset("urn:li:dataset:(urn:li:dataPlatform:s3,staging/orders_clean,PROD)")\n       .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.orders,PROD)");\n\nclient.entities().upsert(loadJob);\n\n// Creates end-to-end lineage:\n// mysql.orders \u2192 [Extract] \u2192 s3.raw \u2192 [Transform] \u2192 s3.clean \u2192 [Load] \u2192 snowflake.analytics\n')),(0,n.yg)("h3",{id:"updating-lineage"},"Updating Lineage"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Load existing job\nDataJobUrn urn = DataJobUrn.createFromString(\n    "urn:li:dataJob:(urn:li:dataFlow:(airflow,my_pipeline,prod),my_task)"\n);\nDataJob dataJob = client.entities().get(urn);\n\n// Add new inlet (e.g., requirements changed)\ndataJob.addInputDataset("urn:li:dataset:(urn:li:dataPlatform:kafka,events.new_source,PROD)");\n\n// Remove old outlet (e.g., deprecated table)\ndataJob.removeOutputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,old.deprecated_table,PROD)");\n\n// Apply changes\nclient.entities().update(dataJob);\n')),(0,n.yg)("h3",{id:"lineage-best-practices"},"Lineage Best Practices"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Be Complete")," - Define both inputs and outputs for accurate lineage"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Use Correct URNs")," - Ensure dataset URNs match existing datasets in DataHub"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Update When Changed")," - Keep lineage current as pipelines evolve"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Document Transformations")," - Use descriptions to explain what the job does"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Model All Jobs")," - Include every step in your pipeline for complete lineage"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Use Typed URNs")," - Prefer DatasetUrn/DataJobUrn objects over strings for compile-time safety"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Layer Your Lineage")," - Start with dataset-level, add field-level and fine-grained as needed"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Track Dependencies")," - Use DataJob dependencies to model task orchestration"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Be Precise with Transformations")," - Use accurate transformation types in fine-grained lineage"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Set Confidence Scores")," - Use appropriate confidence scores to indicate lineage quality")),(0,n.yg)("h3",{id:"common-patterns"},"Common Patterns"),(0,n.yg)("h4",{id:"multiple-sources-to-single-destination"},"Multiple Sources to Single Destination"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Data aggregation job\ndataJob.addInputDataset("urn:li:dataset:(urn:li:dataPlatform:postgres,sales.orders,PROD)")\n       .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:postgres,sales.customers,PROD)")\n       .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:postgres,sales.products,PROD)")\n       .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.sales_summary,PROD)");\n')),(0,n.yg)("h4",{id:"single-source-to-multiple-destinations"},"Single Source to Multiple Destinations"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Data fanout job\ndataJob.addInputDataset("urn:li:dataset:(urn:li:dataPlatform:kafka,events.raw,PROD)")\n       .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:s3,archive/events,PROD)")\n       .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,events.processed,PROD)")\n       .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:elasticsearch,events.searchable,PROD)");\n')),(0,n.yg)("h4",{id:"cross-platform-lineage"},"Cross-Platform Lineage"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// ETL across different platforms\ndataJob.addInputDataset("urn:li:dataset:(urn:li:dataPlatform:mysql,production.transactions,PROD)")\n       .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:kafka,events.user_activity,PROD)")\n       .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:s3,raw/reference_data,PROD)")\n       .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.customer_360,PROD)")\n       .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:bigquery,reporting.customer_metrics,PROD)");\n')),(0,n.yg)("h2",{id:"complete-example"},"Complete Example"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'import datahub.client.v2.DataHubClientV2;\nimport datahub.client.v2.entity.DataJob;\nimport com.linkedin.common.OwnershipType;\nimport java.io.IOException;\nimport java.util.concurrent.ExecutionException;\n\npublic class DataJobExample {\n    public static void main(String[] args) {\n        // Create client\n        DataHubClientV2 client = DataHubClientV2.builder()\n            .server("http://localhost:8080")\n            .build();\n\n        try {\n            // Build data job with all metadata\n            DataJob dataJob = DataJob.builder()\n                .orchestrator("airflow")\n                .flowId("customer_analytics")\n                .cluster("prod")\n                .jobId("process_events")\n                .description("Processes customer events from Kafka to warehouse")\n                .name("Process Customer Events")\n                .type("BATCH")\n                .build();\n\n            // Add tags\n            dataJob.addTag("critical")\n                   .addTag("etl")\n                   .addTag("pii");\n\n            // Add owners\n            dataJob.addOwner("urn:li:corpuser:data_team", OwnershipType.TECHNICAL_OWNER)\n                   .addOwner("urn:li:corpuser:product_team", OwnershipType.BUSINESS_OWNER);\n\n            // Add glossary terms\n            dataJob.addTerm("urn:li:glossaryTerm:DataProcessing")\n                   .addTerm("urn:li:glossaryTerm:CustomerData");\n\n            // Set domain\n            dataJob.setDomain("urn:li:domain:Analytics");\n\n            // Add custom properties\n            dataJob.addCustomProperty("schedule", "0 2 * * *")\n                   .addCustomProperty("retries", "3")\n                   .addCustomProperty("timeout", "7200");\n\n            // Upsert to DataHub\n            client.entities().upsert(dataJob);\n\n            System.out.println("Successfully created data job: " + dataJob.getUrn());\n\n        } catch (IOException | ExecutionException | InterruptedException e) {\n            e.printStackTrace();\n        } finally {\n            try {\n                client.close();\n            } catch (IOException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n}\n')),(0,n.yg)("h2",{id:"updating-existing-datajobs"},"Updating Existing DataJobs"),(0,n.yg)("h3",{id:"load-and-modify"},"Load and Modify"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Load existing data job\nDataJobUrn urn = DataJobUrn.createFromString(\n    "urn:li:dataJob:(urn:li:dataFlow:(airflow,my_dag,prod),my_task)"\n);\nDataJob dataJob = client.entities().get(urn);\n\n// Add new metadata (creates patches)\ndataJob.addTag("new-tag")\n       .addOwner("urn:li:corpuser:new_owner", OwnershipType.TECHNICAL_OWNER);\n\n// Apply patches\nclient.entities().update(dataJob);\n')),(0,n.yg)("h3",{id:"incremental-updates"},"Incremental Updates"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// Just add what you need\ndataJob.addTag("critical");\nclient.entities().update(dataJob);\n\n// Later, add more\ndataJob.addCustomProperty("priority", "high");\nclient.entities().update(dataJob);\n')),(0,n.yg)("h2",{id:"builder-options-reference"},"Builder Options Reference"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Method"),(0,n.yg)("th",{parentName:"tr",align:null},"Required"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"orchestrator(String)")),(0,n.yg)("td",{parentName:"tr",align:null},"\u2705 Yes"),(0,n.yg)("td",{parentName:"tr",align:null},'Orchestrator (e.g., "airflow", "dagster")')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"flowId(String)")),(0,n.yg)("td",{parentName:"tr",align:null},"\u2705 Yes"),(0,n.yg)("td",{parentName:"tr",align:null},"Flow/DAG identifier")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"jobId(String)")),(0,n.yg)("td",{parentName:"tr",align:null},"\u2705 Yes"),(0,n.yg)("td",{parentName:"tr",align:null},"Job/task identifier")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"cluster(String)")),(0,n.yg)("td",{parentName:"tr",align:null},"No"),(0,n.yg)("td",{parentName:"tr",align:null},'Cluster name (e.g., "prod", "dev"). Default: "prod"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"description(String)")),(0,n.yg)("td",{parentName:"tr",align:null},"No"),(0,n.yg)("td",{parentName:"tr",align:null},"Job description. ",(0,n.yg)("strong",{parentName:"td"},"Requires both ",(0,n.yg)("inlineCode",{parentName:"strong"},"name()")," and ",(0,n.yg)("inlineCode",{parentName:"strong"},"type()")," to be set"))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"name(String)")),(0,n.yg)("td",{parentName:"tr",align:null},"No"),(0,n.yg)("td",{parentName:"tr",align:null},"Display name shown in UI. ",(0,n.yg)("strong",{parentName:"td"},"Required if using ",(0,n.yg)("inlineCode",{parentName:"strong"},"description()"),", ",(0,n.yg)("inlineCode",{parentName:"strong"},"type()"),", or ",(0,n.yg)("inlineCode",{parentName:"strong"},"customProperties()")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"type(String)")),(0,n.yg)("td",{parentName:"tr",align:null},"No"),(0,n.yg)("td",{parentName:"tr",align:null},'Job type (e.g., "BATCH", "STREAMING"). ',(0,n.yg)("strong",{parentName:"td"},"Required if using ",(0,n.yg)("inlineCode",{parentName:"strong"},"description()"),", ",(0,n.yg)("inlineCode",{parentName:"strong"},"name()"),", or ",(0,n.yg)("inlineCode",{parentName:"strong"},"customProperties()")))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"customProperties(Map)")),(0,n.yg)("td",{parentName:"tr",align:null},"No"),(0,n.yg)("td",{parentName:"tr",align:null},"Map of custom key-value properties. ",(0,n.yg)("strong",{parentName:"td"},"Requires both ",(0,n.yg)("inlineCode",{parentName:"strong"},"name()")," and ",(0,n.yg)("inlineCode",{parentName:"strong"},"type()")," to be set"))))),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Important:")," The DataJobInfo aspect requires both ",(0,n.yg)("inlineCode",{parentName:"p"},"name")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"type")," fields. If you provide any of ",(0,n.yg)("inlineCode",{parentName:"p"},"description"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"name"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"type"),", or ",(0,n.yg)("inlineCode",{parentName:"p"},"customProperties")," in the builder, you must provide both ",(0,n.yg)("inlineCode",{parentName:"p"},"name")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"type"),". Otherwise, you'll get an ",(0,n.yg)("inlineCode",{parentName:"p"},"IllegalArgumentException")," at build time."),(0,n.yg)("h2",{id:"common-patterns-1"},"Common Patterns"),(0,n.yg)("h3",{id:"creating-multiple-datajobs"},"Creating Multiple DataJobs"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'String[] tasks = {"extract", "transform", "load"};\nfor (String taskName : tasks) {\n    DataJob dataJob = DataJob.builder()\n        .orchestrator("airflow")\n        .flowId("etl_pipeline")\n        .cluster("prod")\n        .jobId(taskName)\n        .build();\n\n    dataJob.addTag("etl")\n           .addCustomProperty("team", "data-engineering");\n\n    client.entities().upsert(dataJob);\n}\n')),(0,n.yg)("h3",{id:"batch-metadata-addition"},"Batch Metadata Addition"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'DataJob dataJob = DataJob.builder()\n    .orchestrator("airflow")\n    .flowId("my_dag")\n    .jobId("my_task")\n    .build();\n\nList<String> tags = Arrays.asList("critical", "production", "etl");\ntags.forEach(dataJob::addTag);\n\nclient.entities().upsert(dataJob);  // Emits all tags in one call\n')),(0,n.yg)("h3",{id:"conditional-metadata"},"Conditional Metadata"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'if (isCritical(dataJob)) {\n    dataJob.addTag("critical")\n           .addTerm("urn:li:glossaryTerm:BusinessCritical");\n}\n\nif (processesFinancialData(dataJob)) {\n    dataJob.addTag("financial")\n           .addOwner("urn:li:corpuser:compliance_team", OwnershipType.DATA_STEWARD);\n}\n')),(0,n.yg)("h2",{id:"datajob-vs-dataflow"},"DataJob vs DataFlow"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"DataFlow")," represents a pipeline or DAG (e.g., an Airflow DAG):"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"URN: ",(0,n.yg)("inlineCode",{parentName:"li"},"urn:li:dataFlow:(orchestrator,flowId,cluster)")),(0,n.yg)("li",{parentName:"ul"},"Contains multiple DataJobs")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"DataJob")," represents a task within a pipeline:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"URN: ",(0,n.yg)("inlineCode",{parentName:"li"},"urn:li:dataJob:(flowUrn,jobId)")),(0,n.yg)("li",{parentName:"ul"},"Belongs to one DataFlow"),(0,n.yg)("li",{parentName:"ul"},"Can have lineage to datasets and other DataJobs")),(0,n.yg)("p",null,"Example hierarchy:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"DataFlow: urn:li:dataFlow:(airflow,customer_pipeline,prod)\n\u251c\u2500\u2500 DataJob: urn:li:dataJob:(urn:li:dataFlow:(airflow,customer_pipeline,prod),extract)\n\u251c\u2500\u2500 DataJob: urn:li:dataJob:(urn:li:dataFlow:(airflow,customer_pipeline,prod),transform)\n\u2514\u2500\u2500 DataJob: urn:li:dataJob:(urn:li:dataFlow:(airflow,customer_pipeline,prod),load)\n")),(0,n.yg)("h2",{id:"orchestrator-examples"},"Orchestrator Examples"),(0,n.yg)("p",null,"Common orchestrator values:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"airflow")," - Apache Airflow"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"dagster")," - Dagster"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"prefect")," - Prefect"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"dbt")," - dbt (data build tool)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"spark")," - Apache Spark"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"glue")," - AWS Glue"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"dataflow")," - Google Cloud Dataflow"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"azkaban")," - Azkaban"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"luigi")," - Luigi")),(0,n.yg)("h2",{id:"next-steps"},"Next Steps"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("a",{parentName:"strong",href:"/docs/metadata-integration/java/docs/sdk-v2/dataset-entity"},"Dataset Entity"))," - Working with dataset entities"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("a",{parentName:"strong",href:"/docs/metadata-integration/java/docs/sdk-v2/patch-operations"},"Patch Operations"))," - Deep dive into patches"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("a",{parentName:"strong",href:"/docs/metadata-integration/java/docs/sdk-v2/migration-from-v1"},"Migration Guide"))," - Upgrading from V1")),(0,n.yg)("h2",{id:"examples"},"Examples"),(0,n.yg)("h3",{id:"basic-datajob-creation"},"Basic DataJob Creation"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'# Inlined from /metadata-integration/java/examples/src/main/java/io/datahubproject/examples/v2/DataJobCreateExample.java\npackage io.datahubproject.examples.v2;\n\nimport com.linkedin.common.OwnershipType;\nimport datahub.client.v2.DataHubClientV2;\nimport datahub.client.v2.entity.DataJob;\nimport java.io.IOException;\nimport java.util.concurrent.ExecutionException;\n\n/**\n * Example demonstrating how to create a DataJob using Java SDK V2.\n *\n * <p>This example shows:\n *\n * <ul>\n *   <li>Creating a DataHubClientV2\n *   <li>Building a DataJob with fluent builder\n *   <li>Adding tags, owners, and custom properties\n *   <li>Upserting to DataHub\n * </ul>\n */\npublic class DataJobCreateExample {\n\n  public static void main(String[] args)\n      throws IOException, ExecutionException, InterruptedException {\n    // Create client (use environment variables or pass explicit values)\n    DataHubClientV2 client =\n        DataHubClientV2.builder()\n            .server(System.getenv().getOrDefault("DATAHUB_SERVER", "http://localhost:8080"))\n            .token(System.getenv("DATAHUB_TOKEN")) // Optional\n            .build();\n\n    try {\n      // Test connection\n      if (!client.testConnection()) {\n        System.err.println("Failed to connect to DataHub server");\n        return;\n      }\n      System.out.println("\u2713 Connected to DataHub");\n\n      // Build data job with metadata\n      DataJob dataJob =\n          DataJob.builder()\n              .orchestrator("airflow")\n              .flowId("user_analytics_dag")\n              .cluster("prod")\n              .jobId("process_user_events")\n              .description("Processes user interaction events and loads into data warehouse")\n              .name("Process User Events")\n              .type("BATCH")\n              .build();\n\n      System.out.println("\u2713 Built data job with URN: " + dataJob.getUrn());\n\n      // Add tags\n      dataJob.addTag("critical").addTag("etl").addTag("user-data");\n\n      System.out.println("\u2713 Added 3 tags");\n\n      // Add owners\n      dataJob\n          .addOwner("urn:li:corpuser:datahub", OwnershipType.TECHNICAL_OWNER)\n          .addOwner("urn:li:corpuser:data_team", OwnershipType.DATA_STEWARD);\n\n      System.out.println("\u2713 Added 2 owners");\n\n      // Add custom properties\n      dataJob\n          .addCustomProperty("team", "data-engineering")\n          .addCustomProperty("schedule", "0 2 * * *")\n          .addCustomProperty("retries", "3")\n          .addCustomProperty("timeout", "3600");\n\n      System.out.println("\u2713 Added 4 custom properties");\n\n      // Upsert to DataHub\n      client.entities().upsert(dataJob);\n\n      System.out.println("\u2713 Successfully created data job in DataHub!");\n      System.out.println("\\n  URN: " + dataJob.getUrn());\n      System.out.println(\n          "  View in DataHub: " + client.getConfig().getServer() + "/dataJob/" + dataJob.getUrn());\n\n    } finally {\n      client.close();\n    }\n  }\n}\n\n')),(0,n.yg)("h3",{id:"comprehensive-datajob-with-metadata-and-lineage"},"Comprehensive DataJob with Metadata and Lineage"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'# Inlined from /metadata-integration/java/examples/src/main/java/io/datahubproject/examples/v2/DataJobFullExample.java\npackage io.datahubproject.examples.v2;\n\nimport com.linkedin.common.OwnershipType;\nimport datahub.client.v2.DataHubClientV2;\nimport datahub.client.v2.entity.DataJob;\nimport java.io.IOException;\nimport java.util.concurrent.ExecutionException;\n\n/**\n * Comprehensive example demonstrating all DataJob metadata operations using Java SDK V2.\n *\n * <p>This example shows:\n *\n * <ul>\n *   <li>Creating a data job with complete metadata\n *   <li>Adding tags, owners, glossary terms\n *   <li>Setting domain and custom properties\n *   <li>Defining lineage with input datasets (inlets) and output datasets (outlets)\n *   <li>Combining all operations in single entity\n * </ul>\n */\npublic class DataJobFullExample {\n\n  public static void main(String[] args)\n      throws IOException, ExecutionException, InterruptedException {\n    // Create client\n    DataHubClientV2 client =\n        DataHubClientV2.builder()\n            .server(System.getenv().getOrDefault("DATAHUB_SERVER", "http://localhost:8080"))\n            .token(System.getenv("DATAHUB_TOKEN"))\n            .build();\n\n    try {\n      // Test connection\n      if (!client.testConnection()) {\n        System.err.println("Failed to connect to DataHub server");\n        return;\n      }\n      System.out.println("\u2713 Connected to DataHub");\n\n      // Build comprehensive data job with all metadata types\n      DataJob dataJob =\n          DataJob.builder()\n              .orchestrator("airflow")\n              .flowId("financial_reporting_pipeline")\n              .cluster("prod")\n              .jobId("aggregate_customer_transactions")\n              .description(\n                  "Critical ETL job that aggregates daily customer transaction data from multiple sources "\n                      + "and loads into the enterprise data warehouse. Includes data quality checks, "\n                      + "PII tokenization, and regulatory compliance validation.")\n              .name("Aggregate Customer Transactions")\n              .type("BATCH")\n              .build();\n\n      System.out.println("\u2713 Built data job with URN: " + dataJob.getUrn());\n\n      // Add multiple tags for categorization\n      dataJob\n          .addTag("critical")\n          .addTag("pii")\n          .addTag("financial")\n          .addTag("etl")\n          .addTag("production");\n\n      System.out.println("\u2713 Added 5 tags");\n\n      // Add multiple owners with different roles\n      dataJob\n          .addOwner("urn:li:corpuser:data_engineering", OwnershipType.TECHNICAL_OWNER)\n          .addOwner("urn:li:corpuser:finance_team", OwnershipType.BUSINESS_OWNER)\n          .addOwner("urn:li:corpuser:compliance_team", OwnershipType.DATA_STEWARD);\n\n      System.out.println("\u2713 Added 3 owners");\n\n      // Add glossary terms for business context\n      dataJob\n          .addTerm("urn:li:glossaryTerm:DataProcessing")\n          .addTerm("urn:li:glossaryTerm:ETL")\n          .addTerm("urn:li:glossaryTerm:FinancialReporting");\n\n      System.out.println("\u2713 Added 3 glossary terms");\n\n      // Set domain for organizational structure\n      dataJob.setDomain("urn:li:domain:Finance");\n\n      System.out.println("\u2713 Set domain");\n\n      // Add comprehensive custom properties\n      dataJob\n          .addCustomProperty("team", "data-platform")\n          .addCustomProperty("schedule", "0 2 * * *") // Daily at 2 AM\n          .addCustomProperty("retries", "3")\n          .addCustomProperty("timeout", "7200") // 2 hours\n          .addCustomProperty("sla_hours", "4")\n          .addCustomProperty("priority", "high")\n          .addCustomProperty("notification_channel", "#data-alerts")\n          .addCustomProperty("requires_manual_approval", "false")\n          .addCustomProperty("data_classification", "highly-confidential")\n          .addCustomProperty("compliance_level", "PCI-DSS");\n\n      System.out.println("\u2713 Added 10 custom properties");\n\n      // Add lineage: Define input datasets (inlets) that this job reads from\n      dataJob\n          .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.transactions,PROD)")\n          .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.customers,PROD)")\n          .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:kafka,events.user_activity,PROD)");\n\n      System.out.println("\u2713 Added 3 input datasets (inlets)");\n\n      // Add lineage: Define output datasets (outlets) that this job writes to\n      dataJob\n          .addOutputDataset(\n              "urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.customer_transactions,PROD)")\n          .addOutputDataset(\n              "urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.daily_summary,PROD)");\n\n      System.out.println("\u2713 Added 2 output datasets (outlets)");\n\n      // Count accumulated patches\n      System.out.println("\\nAccumulated " + dataJob.getPendingPatches().size() + " patches");\n\n      // Upsert to DataHub - all metadata in single operation\n      client.entities().upsert(dataJob);\n\n      System.out.println("\\n\u2713 Successfully created comprehensive data job in DataHub!");\n      System.out.println("\\nSummary:");\n      System.out.println("  URN: " + dataJob.getUrn());\n      System.out.println("  Orchestrator: airflow");\n      System.out.println("  Flow: financial_reporting_pipeline");\n      System.out.println("  Job: aggregate_customer_transactions");\n      System.out.println("  Tags: 5");\n      System.out.println("  Owners: 3");\n      System.out.println("  Glossary Terms: 3");\n      System.out.println("  Domain: Finance");\n      System.out.println("  Custom Properties: 10");\n      System.out.println("  Input Datasets (Inlets): 3");\n      System.out.println("  Output Datasets (Outlets): 2");\n      System.out.println(\n          "\\n  View in DataHub: "\n              + client.getConfig().getServer()\n              + "/dataJob/"\n              + dataJob.getUrn());\n      System.out.println("\\nLineage Flow:");\n      System.out.println(\n          "  raw.transactions, raw.customers, events.user_activity \u2192 [Job] \u2192 customer_transactions, daily_summary");\n\n    } finally {\n      client.close();\n    }\n  }\n}\n\n')),(0,n.yg)("h3",{id:"datajob-lineage-operations"},"DataJob Lineage Operations"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'# Inlined from /metadata-integration/java/examples/src/main/java/io/datahubproject/examples/v2/DataJobLineageExample.java\npackage io.datahubproject.examples.v2;\n\nimport com.linkedin.common.urn.DatasetUrn;\nimport datahub.client.v2.DataHubClientV2;\nimport datahub.client.v2.entity.DataJob;\nimport java.io.IOException;\nimport java.net.URISyntaxException;\nimport java.util.Arrays;\nimport java.util.concurrent.ExecutionException;\n\n/**\n * Focused example demonstrating DataJob lineage operations using Java SDK V2.\n *\n * <p>This example shows:\n *\n * <ul>\n *   <li>Creating a DataJob with input datasets and output datasets\n *   <li>Adding and removing individual inputs and outputs\n *   <li>Setting multiple inputs/outputs at once\n *   <li>Building a complete ETL pipeline with lineage\n *   <li>Understanding the relationship between DataJob and DataFlow\n * </ul>\n *\n * <p>Lineage Concepts:\n *\n * <ul>\n *   <li><b>Input Datasets</b> - Datasets that the job reads from\n *   <li><b>Output Datasets</b> - Datasets that the job writes to\n *   <li>Lineage flows: Dataset (input) \u2192 DataJob \u2192 Dataset (output)\n * </ul>\n */\npublic class DataJobLineageExample {\n\n  public static void main(String[] args)\n      throws IOException, ExecutionException, InterruptedException, URISyntaxException {\n    // Create client\n    DataHubClientV2 client =\n        DataHubClientV2.builder()\n            .server(System.getenv().getOrDefault("DATAHUB_SERVER", "http://localhost:8080"))\n            .token(System.getenv("DATAHUB_TOKEN"))\n            .build();\n\n    try {\n      // Test connection\n      if (!client.testConnection()) {\n        System.err.println("Failed to connect to DataHub server");\n        return;\n      }\n      System.out.println("\u2713 Connected to DataHub\\n");\n\n      // ==================== Example 1: Simple ETL Job with Lineage ====================\n      System.out.println("Example 1: Creating ETL job with lineage");\n      System.out.println("==========================================");\n\n      DataJob etlJob =\n          DataJob.builder()\n              .orchestrator("airflow")\n              .flowId("customer_etl_pipeline")\n              .cluster("prod")\n              .jobId("load_customers")\n              .description("Extracts customer data from PostgreSQL and loads into Snowflake")\n              .name("Load Customers")\n              .type("BATCH")\n              .build();\n\n      // Add input dataset (source)\n      etlJob.addInputDataset("urn:li:dataset:(urn:li:dataPlatform:postgres,public.customers,PROD)");\n\n      // Add output dataset (destination)\n      etlJob.addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,dwh.customers,PROD)");\n\n      client.entities().upsert(etlJob);\n\n      System.out.println("\u2713 Created ETL job: " + etlJob.getUrn());\n      System.out.println(\n          "  Lineage: postgres.public.customers \u2192 [Load Customers] \u2192 snowflake.dwh.customers\\n");\n\n      // ==================== Example 2: Complex Aggregation Job ====================\n      System.out.println("Example 2: Complex aggregation with multiple inputs/outputs");\n      System.out.println("============================================================");\n\n      DataJob aggregationJob =\n          DataJob.builder()\n              .orchestrator("airflow")\n              .flowId("analytics_pipeline")\n              .cluster("prod")\n              .jobId("aggregate_sales_metrics")\n              .description("Aggregates sales data from multiple sources into summary tables")\n              .name("Aggregate Sales Metrics")\n              .type("BATCH")\n              .build();\n\n      // Add multiple input datasets (different sources)\n      aggregationJob\n          .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.transactions,PROD)")\n          .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.customers,PROD)")\n          .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,raw.products,PROD)")\n          .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:kafka,events.purchases,PROD)");\n\n      // Add multiple output datasets (different aggregation levels)\n      aggregationJob\n          .addOutputDataset(\n              "urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.daily_sales,PROD)")\n          .addOutputDataset(\n              "urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.monthly_sales,PROD)")\n          .addOutputDataset(\n              "urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.customer_summary,PROD)");\n\n      client.entities().upsert(aggregationJob);\n\n      System.out.println("\u2713 Created aggregation job: " + aggregationJob.getUrn());\n      System.out.println("  Input datasets: 4 (Snowflake tables + Kafka topic)");\n      System.out.println("  Output datasets: 3 (daily, monthly, customer summaries)\\n");\n\n      // ==================== Example 3: Using setInputDatasets/setOutputDatasets\n      // ====================\n      System.out.println("Example 3: Setting multiple inlets/outlets at once");\n      System.out.println("===================================================");\n\n      DataJob batchJob =\n          DataJob.builder()\n              .orchestrator("dagster")\n              .flowId("data_quality_pipeline")\n              .cluster("prod")\n              .jobId("validate_warehouse_tables")\n              .description("Runs data quality checks on warehouse tables")\n              .name("Validate Warehouse Tables")\n              .type("BATCH")\n              .build();\n\n      // Set all inlets at once\n      batchJob.setInputDatasets(\n          Arrays.asList(\n              "urn:li:dataset:(urn:li:dataPlatform:snowflake,dwh.orders,PROD)",\n              "urn:li:dataset:(urn:li:dataPlatform:snowflake,dwh.customers,PROD)",\n              "urn:li:dataset:(urn:li:dataPlatform:snowflake,dwh.products,PROD)"));\n\n      // Set all outlets at once\n      batchJob.setOutputDatasets(\n          Arrays.asList(\n              "urn:li:dataset:(urn:li:dataPlatform:snowflake,quality.validation_results,PROD)",\n              "urn:li:dataset:(urn:li:dataPlatform:snowflake,quality.data_quality_metrics,PROD)"));\n\n      client.entities().upsert(batchJob);\n\n      System.out.println("\u2713 Created validation job: " + batchJob.getUrn());\n      System.out.println("  Set 3 inlets and 2 outlets in batch operations\\n");\n\n      // ==================== Example 4: Updating Lineage ====================\n      System.out.println("Example 4: Updating existing job lineage");\n      System.out.println("=========================================");\n\n      // Create a simple job first\n      DataJob updateJob =\n          DataJob.builder()\n              .orchestrator("airflow")\n              .flowId("data_processing_pipeline")\n              .cluster("prod")\n              .jobId("process_events")\n              .description("Processes event data")\n              .name("Process Events")\n              .type("BATCH")\n              .build();\n\n      // Initial lineage\n      updateJob.addInputDataset("urn:li:dataset:(urn:li:dataPlatform:kafka,events.raw,PROD)");\n      updateJob.addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:s3,processed/events,PROD)");\n\n      client.entities().upsert(updateJob);\n      System.out.println("\u2713 Created job with initial lineage");\n\n      // Add another input source (e.g., requirement changed)\n      updateJob.addInputDataset(\n          "urn:li:dataset:(urn:li:dataPlatform:kafka,events.enrichment,PROD)");\n\n      // Add another output destination\n      updateJob.addOutputDataset(\n          "urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.events,PROD)");\n\n      client.entities().upsert(updateJob);\n      System.out.println("\u2713 Updated job with additional inlet and outlet\\n");\n\n      // ==================== Example 5: Using DatasetUrn directly ====================\n      System.out.println("Example 5: Using DatasetUrn objects for type safety");\n      System.out.println("====================================================");\n\n      DataJob typedJob =\n          DataJob.builder()\n              .orchestrator("spark")\n              .flowId("ml_feature_pipeline")\n              .cluster("prod")\n              .jobId("generate_features")\n              .description("Generates ML features from raw data")\n              .name("Generate ML Features")\n              .type("BATCH")\n              .build();\n\n      // Create DatasetUrn objects for type safety\n      DatasetUrn sourceUrn =\n          DatasetUrn.createFromString("urn:li:dataset:(urn:li:dataPlatform:hive,user_events,PROD)");\n      DatasetUrn featureUrn =\n          DatasetUrn.createFromString("urn:li:dataset:(urn:li:dataPlatform:hive,ml_features,PROD)");\n\n      // Use typed URN objects\n      typedJob.addInputDataset(sourceUrn).addOutputDataset(featureUrn);\n\n      client.entities().upsert(typedJob);\n\n      System.out.println("\u2713 Created job using DatasetUrn objects");\n      System.out.println("  Type-safe URN creation prevents errors\\n");\n\n      // ==================== Example 6: Complete Data Pipeline ====================\n      System.out.println("Example 6: Modeling a complete ETL pipeline");\n      System.out.println("============================================");\n      System.out.println("Pipeline: Extract \u2192 Transform \u2192 Load");\n      System.out.println();\n\n      // Extract job\n      DataJob extractJob =\n          DataJob.builder()\n              .orchestrator("airflow")\n              .flowId("complete_etl_pipeline")\n              .cluster("prod")\n              .jobId("extract_from_source")\n              .description("Extracts data from operational database")\n              .name("Extract from Source DB")\n              .type("BATCH")\n              .build();\n\n      extractJob\n          .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:mysql,production.orders,PROD)")\n          .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:s3,staging/orders_raw,PROD)");\n\n      client.entities().upsert(extractJob);\n      System.out.println("\u2713 Extract: mysql.orders \u2192 s3.staging/orders_raw");\n\n      // Transform job\n      DataJob transformJob =\n          DataJob.builder()\n              .orchestrator("airflow")\n              .flowId("complete_etl_pipeline")\n              .cluster("prod")\n              .jobId("transform_data")\n              .description("Cleanses and transforms extracted data")\n              .name("Transform Data")\n              .type("BATCH")\n              .build();\n\n      transformJob\n          .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:s3,staging/orders_raw,PROD)")\n          .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:s3,staging/orders_clean,PROD)");\n\n      client.entities().upsert(transformJob);\n      System.out.println("\u2713 Transform: s3.staging/orders_raw \u2192 s3.staging/orders_clean");\n\n      // Load job\n      DataJob loadJob =\n          DataJob.builder()\n              .orchestrator("airflow")\n              .flowId("complete_etl_pipeline")\n              .cluster("prod")\n              .jobId("load_to_warehouse")\n              .description("Loads transformed data into data warehouse")\n              .name("Load to Warehouse")\n              .type("BATCH")\n              .build();\n\n      loadJob\n          .addInputDataset("urn:li:dataset:(urn:li:dataPlatform:s3,staging/orders_clean,PROD)")\n          .addOutputDataset("urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.orders,PROD)");\n\n      client.entities().upsert(loadJob);\n      System.out.println("\u2713 Load: s3.staging/orders_clean \u2192 snowflake.analytics.orders");\n      System.out.println();\n      System.out.println("Complete lineage created:");\n      System.out.println(\n          "  mysql.orders \u2192 [Extract] \u2192 s3.raw \u2192 [Transform] \u2192 s3.clean \u2192 [Load] \u2192 snowflake.analytics\\n");\n\n      // ==================== Summary ====================\n      System.out.println("==========================================");\n      System.out.println("Summary: Successfully created 9 data jobs with comprehensive lineage");\n      System.out.println("==========================================");\n      System.out.println("\\nKey Takeaways:");\n      System.out.println("  \u2022 Inlets represent input datasets (sources)");\n      System.out.println("  \u2022 Outlets represent output datasets (destinations)");\n      System.out.println("  \u2022 Use addInputDataset/addOutputDataset for single operations");\n      System.out.println("  \u2022 Use setInputDatasets/setOutputDatasets for batch operations");\n      System.out.println("  \u2022 DatasetUrn objects provide type safety");\n      System.out.println("  \u2022 All jobs belong to a DataFlow (pipeline/DAG)");\n      System.out.println("\\nView your lineage graphs in DataHub UI!");\n\n    } finally {\n      client.close();\n    }\n  }\n}\n\n')))}c.isMDXComponent=!0}}]);