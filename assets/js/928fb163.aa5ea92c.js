"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[23426],{7653:(e,t,a)=>{a.d(t,{A:()=>n});const n={icon:{tag:"svg",attrs:{"fill-rule":"evenodd",viewBox:"64 64 896 896",focusable:"false"},children:[{tag:"path",attrs:{d:"M512 64c247.4 0 448 200.6 448 448S759.4 960 512 960 64 759.4 64 512 264.6 64 512 64zm127.98 274.82h-.04l-.08.06L512 466.75 384.14 338.88c-.04-.05-.06-.06-.08-.06a.12.12 0 00-.07 0c-.03 0-.05.01-.09.05l-45.02 45.02a.2.2 0 00-.05.09.12.12 0 000 .07v.02a.27.27 0 00.06.06L466.75 512 338.88 639.86c-.05.04-.06.06-.06.08a.12.12 0 000 .07c0 .03.01.05.05.09l45.02 45.02a.2.2 0 00.09.05.12.12 0 00.07 0c.02 0 .04-.01.08-.05L512 557.25l127.86 127.87c.04.04.06.05.08.05a.12.12 0 00.07 0c.03 0 .05-.01.09-.05l45.02-45.02a.2.2 0 00.05-.09.12.12 0 000-.07v-.02a.27.27 0 00-.05-.06L557.25 512l127.87-127.86c.04-.04.05-.06.05-.08a.12.12 0 000-.07c0-.03-.01-.05-.05-.09l-45.02-45.02a.2.2 0 00-.09-.05.12.12 0 00-.07 0z"}}]},name:"close-circle",theme:"filled"}},4732:(e,t,a)=>{a.d(t,{A:()=>l});var n=a(89379),s=a(96540),i=a(7653),o=a(89990),r=function(e,t){return s.createElement(o.A,(0,n.A)((0,n.A)({},e),{},{ref:t,icon:i.A}))};const l=s.forwardRef(r)},15680:(e,t,a)=>{a.d(t,{xA:()=>p,yg:()=>m});var n=a(96540);function s(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){s(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function r(e,t){if(null==e)return{};var a,n,s=function(e,t){if(null==e)return{};var a,n,s={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(s[a]=e[a]);return s}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},g=n.forwardRef((function(e,t){var a=e.components,s=e.mdxType,i=e.originalType,l=e.parentName,p=r(e,["components","mdxType","originalType","parentName"]),d=c(a),g=s,m=d["".concat(l,".").concat(g)]||d[g]||u[g]||i;return a?n.createElement(m,o(o({ref:t},p),{},{components:a})):n.createElement(m,o({ref:t},p))}));function m(e,t){var a=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var i=a.length,o=new Array(i);o[0]=g;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r[d]="string"==typeof e?e:s,o[1]=r;for(var c=2;c<i;c++)o[c]=a[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}g.displayName="MDXCreateElement"},43655:(e,t,a)=>{a.d(t,{A:()=>b});var n=a(96540),s=a(20053);const i="availabilityCard_P5od",o="managedIcon_AxXO",r="platform_wqXv",l="platformAvailable_Y8lN";var c=a(4732),p=a(89379);const d={icon:{tag:"svg",attrs:{viewBox:"64 64 896 896",focusable:"false"},children:[{tag:"path",attrs:{d:"M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64zm193.5 301.7l-210.6 292a31.8 31.8 0 01-51.7 0L318.5 484.9c-3.8-5.3 0-12.7 6.5-12.7h46.9c10.2 0 19.9 4.9 25.9 13.3l71.2 98.8 157.2-218c6-8.3 15.6-13.3 25.9-13.3H699c6.5 0 10.3 7.4 6.5 12.7z"}}]},name:"check-circle",theme:"filled"};var u=a(89990),g=function(e,t){return n.createElement(u.A,(0,p.A)((0,p.A)({},e),{},{ref:t,icon:d}))};const m=n.forwardRef(g);const y={icon:{tag:"svg",attrs:{viewBox:"64 64 896 896",focusable:"false"},children:[{tag:"path",attrs:{d:"M811.4 418.7C765.6 297.9 648.9 212 512.2 212S258.8 297.8 213 418.6C127.3 441.1 64 519.1 64 612c0 110.5 89.5 200 199.9 200h496.2C870.5 812 960 722.5 960 612c0-92.7-63.1-170.7-148.6-193.3zm36.3 281a123.07 123.07 0 01-87.6 36.3H263.9c-33.1 0-64.2-12.9-87.6-36.3A123.3 123.3 0 01140 612c0-28 9.1-54.3 26.2-76.3a125.7 125.7 0 0166.1-43.7l37.9-9.9 13.9-36.6c8.6-22.8 20.6-44.1 35.7-63.4a245.6 245.6 0 0152.4-49.9c41.1-28.9 89.5-44.2 140-44.2s98.9 15.3 140 44.2c19.9 14 37.5 30.8 52.4 49.9 15.1 19.3 27.1 40.7 35.7 63.4l13.8 36.5 37.8 10c54.3 14.5 92.1 63.8 92.1 120 0 33.1-12.9 64.3-36.3 87.7z"}}]},name:"cloud",theme:"outlined"};var h=function(e,t){return n.createElement(u.A,(0,p.A)((0,p.A)({},e),{},{ref:t,icon:y}))};const f=n.forwardRef(h),b=({saasOnly:e,ossOnly:t})=>n.createElement("div",{className:(0,s.A)(i,"card")},n.createElement("strong",null,"Feature Availability"),n.createElement("div",null,n.createElement("span",{className:(0,s.A)(r,!e&&l)},"Self-Hosted DataHub ",e?n.createElement(c.A,null):n.createElement(m,null))),n.createElement("div",null,n.createElement(f,{className:o}),n.createElement("span",{className:(0,s.A)(r,!t&&l)},"DataHub Cloud ",t?n.createElement(c.A,null):n.createElement(m,null))))},72799:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>c,default:()=>y,frontMatter:()=>l,metadata:()=>p,toc:()=>u});a(96540);var n=a(15680),s=a(43655);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){return t=null!=t?t:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):function(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))})),e}function r(e,t){if(null==e)return{};var a,n,s=function(e,t){if(null==e)return{};var a,n,s={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(s[a]=e[a]);return s}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}const l={title:"Metadata Tests",slug:"/tests/metadata-tests",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/tests/metadata-tests.md"},c="Metadata Tests",p={unversionedId:"docs/tests/metadata-tests",id:"docs/tests/metadata-tests",title:"Metadata Tests",description:"DataHub includes a highly configurable, no-code framework that allows you to configure broad-spanning monitors & continuous actions",source:"@site/genDocs/docs/tests/metadata-tests.md",sourceDirName:"docs/tests",slug:"/tests/metadata-tests",permalink:"/docs/tests/metadata-tests",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/tests/metadata-tests.md",tags:[],version:"current",frontMatter:{title:"Metadata Tests",slug:"/tests/metadata-tests",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/tests/metadata-tests.md"},sidebar:"overviewSidebar",previous:{title:"Centralized Management",permalink:"/docs/features/feature-guides/logical-models/centralized-management"},next:{title:"MCP Server",permalink:"/docs/features/feature-guides/mcp"}},d={},u=[{value:"Automated Asset Classification",id:"automated-asset-classification",level:3},{value:"Automated Data Governance Monitoring",id:"automated-data-governance-monitoring",level:3},{value:"Metadata Tests Setup, Prerequisites, and Permissions",id:"metadata-tests-setup-prerequisites-and-permissions",level:2},{value:"Using Metadata Tests",id:"using-metadata-tests",level:2},{value:"Creating a Metadata Test",id:"creating-a-metadata-test",level:3},{value:"Step 1. Defining Selection Criteria (Scope)",id:"step-1-defining-selection-criteria-scope",level:4},{value:"Selecting Asset Types",id:"selecting-asset-types",level:5},{value:"Building Conditions",id:"building-conditions",level:5},{value:"Step 2: Defining Rules",id:"step-2-defining-rules",level:4},{value:"Validating Test Conditions",id:"validating-test-conditions",level:5},{value:"Step 3: Defining Actions (Optional)",id:"step-3-defining-actions-optional",level:4},{value:"Step 4: Name, Category, Description",id:"step-4-name-category-description",level:4},{value:"Viewing Test Results",id:"viewing-test-results",level:3},{value:"Updating an Existing Test",id:"updating-an-existing-test",level:3},{value:"Removing a Test",id:"removing-a-test",level:3},{value:"GraphQL",id:"graphql",level:3},{value:"Performance Tuning for Batch Evaluation",id:"performance-tuning-for-batch-evaluation",level:2},{value:"Batch Processing Architecture",id:"batch-processing-architecture",level:3},{value:"When ElasticSearch Executor is Used",id:"when-elasticsearch-executor-is-used",level:3},{value:"Key Configuration Variable",id:"key-configuration-variable",level:3},{value:"Scaling Recommendations",id:"scaling-recommendations",level:3},{value:"FAQ and Troubleshooting",id:"faq-and-troubleshooting",level:2}],g={toc:u},m="wrapper";function y(e){var{components:t}=e,a=r(e,["components"]);return(0,n.yg)(m,o(function(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{},n=Object.keys(a);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(a).filter((function(e){return Object.getOwnPropertyDescriptor(a,e).enumerable})))),n.forEach((function(t){i(e,t,a[t])}))}return e}({},g,a),{components:t,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"metadata-tests"},"Metadata Tests"),(0,n.yg)(s.A,{saasOnly:!0,mdxType:"FeatureAvailability"}),(0,n.yg)("p",null,"DataHub includes a highly configurable, no-code framework that allows you to configure broad-spanning monitors & continuous actions\nfor the data assets - datasets, dashboards, charts, pipelines - that make up your enterprise Metadata Graph.\nAt the center of this framework is the concept of a Metadata Test."),(0,n.yg)("p",null,"There are two powerful use cases that are uniquely enabled by the Metadata Tests framework:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Automated Asset Classification"),(0,n.yg)("li",{parentName:"ol"},"Automated Metadata Completion Monitoring")),(0,n.yg)("h3",{id:"automated-asset-classification"},"Automated Asset Classification"),(0,n.yg)("p",null,"Metadata Tests allows you to define conditions for selecting a subset of data assets (e.g. datasets, dashboards, etc),\nalong with a set of actions to take for entities that are selected. After the test is defined, the actions\nwill be applied continuously over time, as the selection set evolves & changes with your data ecosystem."),(0,n.yg)("p",null,'When defining selection criteria, you\'ll be able to choose from a range of useful technical signals (e.g. usage, size) that are automatically\nextracted by DataHub (which vary by integration). This makes automatically classifying the "important" assets in your organization quite easy, which\nis in turn critical for running effective Data Governance initiatives within your organization.'),(0,n.yg)("p",null,'For example, we can define a Metadata Test which selects all Snowflake Tables which are in the top 10% of "most queried"\nfor the past 30 days, and then assign those Tables to a special "Tier 1" group using DataHub Tags, Glossary Terms, or Domains.'),(0,n.yg)("h3",{id:"automated-data-governance-monitoring"},"Automated Data Governance Monitoring"),(0,n.yg)("p",null,"Metadata Tests allow you to define & monitor a set of rules that apply to assets in your data ecosystem (e.g. datasets, dashboards, etc). This is particularly useful when attempting to govern\nyour data, as it allows for the (1) definition and (2) measurement of centralized metadata standards, which are key for both bootstrapping\nand maintaining a well-governed data ecosystem."),(0,n.yg)("p",null,'For example, we can define a Metadata Test which requires that all "Tier 1" data assets (e.g. those marked with a special Tag or Glossary Term),\nmust have the following metadata:'),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"At least 1 explicit owner ",(0,n.yg)("em",{parentName:"li"},"and")),(0,n.yg)("li",{parentName:"ol"},"High-level, human-authored documentation ",(0,n.yg)("em",{parentName:"li"},"and")),(0,n.yg)("li",{parentName:"ol"},'At least 1 Glossary Term from the "Classification" Term Group')),(0,n.yg)("p",null,"Then, we can closely monitor which assets are passing and failing these rules as we work to improve things over time.\nWe can easily identify assets that are ",(0,n.yg)("em",{parentName:"p"},"in")," and ",(0,n.yg)("em",{parentName:"p"},"out of")," compliance with a set of centrally-defined standards."),(0,n.yg)("p",null,"By applying automation, Metadata Tests\ncan enable the full lifecycle of complex Data Governance initiatives - from scoping to execution to monitoring."),(0,n.yg)("h2",{id:"metadata-tests-setup-prerequisites-and-permissions"},"Metadata Tests Setup, Prerequisites, and Permissions"),(0,n.yg)("p",null,"What you need to manage Metadata Tests on DataHub:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Manage Tests")," Privilege")),(0,n.yg)("p",null,"This Platform Privilege allows users to create, edit, and remove all Metadata Tests on DataHub. Therefore, it should only be\ngiven to those users who will be serving as metadata Admins of the platform. The default ",(0,n.yg)("inlineCode",{parentName:"p"},"Admin")," role has this Privilege."),(0,n.yg)("blockquote",null,(0,n.yg)("p",{parentName:"blockquote"},"Note that the Metadata Tests feature is currently limited in support for the following DataHub Asset Types:"),(0,n.yg)("ul",{parentName:"blockquote"},(0,n.yg)("li",{parentName:"ul"},"Dataset"),(0,n.yg)("li",{parentName:"ul"},"Dashboard"),(0,n.yg)("li",{parentName:"ul"},"Chart"),(0,n.yg)("li",{parentName:"ul"},"Data Flow (e.g. Pipeline)"),(0,n.yg)("li",{parentName:"ul"},"Data Job (e.g. Task)"),(0,n.yg)("li",{parentName:"ul"},"Container (Database, Schema, Project)")),(0,n.yg)("p",{parentName:"blockquote"},"If you'd like to see Metadata Tests for other asset types, please let your DataHub Cloud CustomerSuccess partner know!")),(0,n.yg)("h2",{id:"using-metadata-tests"},"Using Metadata Tests"),(0,n.yg)("p",null,"Metadata Tests can be created by first navigating to ",(0,n.yg)("strong",{parentName:"p"},"Govern > Tests"),"."),(0,n.yg)("p",null,"To begin building a new Metadata, click ",(0,n.yg)("strong",{parentName:"p"},"Create new Test"),"."),(0,n.yg)("p",{align:"center"},(0,n.yg)("img",{width:"80%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/manage-tests.png"})),(0,n.yg)("h3",{id:"creating-a-metadata-test"},"Creating a Metadata Test"),(0,n.yg)("p",null,"Inside the Metadata Test builder, we'll need to construct the 3 parts of a Metadata Test:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Selection Criteria")," - Select assets that are in the scope of the test"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Rules")," - Define rules that selected assets can either pass or fail"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Actions (Optional)")," - Define automated actions to be taken assets that are passing\nor failing the test")),(0,n.yg)("p",{align:"center"},(0,n.yg)("img",{width:"80%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/metadata-test-create.png"})),(0,n.yg)("h4",{id:"step-1-defining-selection-criteria-scope"},"Step 1. Defining Selection Criteria (Scope)"),(0,n.yg)("p",null,'In the first step, we define a set of conditions that are used to select a subset of the assets in our Metadata Graph\nthat will be "in the scope" of the new test. Assets that ',(0,n.yg)("strong",{parentName:"p"},"match")," the selection conditions will be considered in scope, while those which do not are simply not applicable for the test.\nOnce the test is created, the test will be evaluated for any assets which fall in scope on a continuous basis (when an asset changes on DataHub\nor once every day)."),(0,n.yg)("h5",{id:"selecting-asset-types"},"Selecting Asset Types"),(0,n.yg)("p",null,"You must select at least one asset ",(0,n.yg)("em",{parentName:"p"},"type")," from a set that includes Datasets, Dashboards, Charts, Data Flows (Pipelines), Data Jobs (Tasks),\nand Containers."),(0,n.yg)("p",{align:"center"},(0,n.yg)("img",{width:"50%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/metadata-test-select-type.png"})),(0,n.yg)("p",null,"Entities will the selected types will be considered in scope, while those of other types will be considered out of scope and\nthus omitted from evaluation of the test."),(0,n.yg)("h5",{id:"building-conditions"},"Building Conditions"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Property")," conditions are the basic unit of comparison used for selecting data assets. Each ",(0,n.yg)("strong",{parentName:"p"},"Property")," condition consists of a target ",(0,n.yg)("em",{parentName:"p"},"property"),",\nan ",(0,n.yg)("em",{parentName:"p"},"operator"),", and an optional ",(0,n.yg)("em",{parentName:"p"},"value"),"."),(0,n.yg)("p",null,"A ",(0,n.yg)("em",{parentName:"p"},"property")," is an attribute of a data asset. It can either be a technical signal (e.g. ",(0,n.yg)("strong",{parentName:"p"},"metric")," such as usage, storage size) or a",(0,n.yg)("br",{parentName:"p"}),"\n","metadata signal (e.g. owners, domain, glossary terms, tags, and more), depending on the asset type and applicability of the signal.\nThe full set of supported ",(0,n.yg)("em",{parentName:"p"},"properties")," can be found in the table below."),(0,n.yg)("p",null,"An ",(0,n.yg)("em",{parentName:"p"},"operator")," is the type of predicate that will be applied to the selected ",(0,n.yg)("em",{parentName:"p"},"property")," when evaluating the test for an asset. The types\nof operators that are applicable depend on the selected property. Some examples of operators include ",(0,n.yg)("inlineCode",{parentName:"p"},"Equals"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"Exists"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"Matches Regex"),",\nand ",(0,n.yg)("inlineCode",{parentName:"p"},"Contains"),"."),(0,n.yg)("p",null,"A ",(0,n.yg)("em",{parentName:"p"},"value")," defines the right-hand side of the condition, or a pre-configured value to evaluate the property and operator against. The type of the value\nis dependent on the selected ",(0,n.yg)("em",{parentName:"p"},"property")," and ",(0,n.yg)("em",{parentName:"p"},"operator. For example, if the selected "),"operator","*"," is ",(0,n.yg)("inlineCode",{parentName:"p"},"Matches Regex"),", the type of the\nvalue would be a string."),(0,n.yg)("p",null,"By selecting a property, operator, and value, we can create a single condition (or predicate) used for\nselecting a data asset to be tested. For example, we can build property conditions that match:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"All datasets in the top 25% of query usage in the past 30 days"),(0,n.yg)("li",{parentName:"ul"},'All assets that have the "Tier 1" Glossary Term attached'),(0,n.yg)("li",{parentName:"ul"},'All assets in the "Marketing" Domain'),(0,n.yg)("li",{parentName:"ul"},"All assets without owners"),(0,n.yg)("li",{parentName:"ul"},"All assets without a description")),(0,n.yg)("p",null,"To create a ",(0,n.yg)("strong",{parentName:"p"},"Property")," condition, simply click ",(0,n.yg)("strong",{parentName:"p"},"Add Condition")," then select ",(0,n.yg)("strong",{parentName:"p"},"Property")," condition."),(0,n.yg)("p",{align:"center"},(0,n.yg)("img",{width:"80%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/metadata-test-create-property-condition.png"})),(0,n.yg)("p",null,"We can combine ",(0,n.yg)("strong",{parentName:"p"},"Property")," conditions using boolean operators including ",(0,n.yg)("inlineCode",{parentName:"p"},"AND"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"OR"),", and ",(0,n.yg)("inlineCode",{parentName:"p"},"NOT"),", by\ncreating ",(0,n.yg)("strong",{parentName:"p"},"Logical")," conditions. To create a ",(0,n.yg)("strong",{parentName:"p"},"Logical")," condition, simply click ",(0,n.yg)("strong",{parentName:"p"},"Add Condition")," then select an\n",(0,n.yg)("strong",{parentName:"p"},"And"),", ",(0,n.yg)("strong",{parentName:"p"},"Or"),", or ",(0,n.yg)("strong",{parentName:"p"},"Not")," condition."),(0,n.yg)("p",{align:"center"},(0,n.yg)("img",{width:"80%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/metadata-test-create-logical-condition.png"})),(0,n.yg)("p",null,"Logical conditions allow us to accommodate complex real-world selection requirements:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"All Snowflake Tables that are in the Top 25% of most queried AND do not have a Domain"),(0,n.yg)("li",{parentName:"ul"},"All Looker Dashboards that do not have a description authored in Looker OR in DataHub")),(0,n.yg)("h4",{id:"step-2-defining-rules"},"Step 2: Defining Rules"),(0,n.yg)("p",null,'In the second step, we can define a set of conditions that selected assets must match in order to be "passing" the test.\nTo do so, we can construct another set of ',(0,n.yg)("strong",{parentName:"p"},"Property")," conditions (as described above)."),(0,n.yg)("blockquote",null,(0,n.yg)("p",{parentName:"blockquote"},(0,n.yg)("strong",{parentName:"p"},"Pro-Tip"),': If no rules are supplied, then all assets that are selected by the criteria defined in Step 1 will be considered "passing".\nIf you need to apply an automated Action to the selected assets, you can leave the Rules blank and continue to the next step.')),(0,n.yg)("p",{align:"center"},(0,n.yg)("img",{width:"50%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/metadata-test-create-rules.png"})),(0,n.yg)("p",null,"When combined with the selection criteria, Rules allow us to define complex, highly custom ",(0,n.yg)("strong",{parentName:"p"},"Data Governance")," policies such as:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"All datasets in the top 25% of query usage in the past 30 days ",(0,n.yg)("strong",{parentName:"li"},"must have an owner"),"."),(0,n.yg)("li",{parentName:"ul"},'All assets in the "Marketing" Domain ',(0,n.yg)("strong",{parentName:"li"},"must have a description")),(0,n.yg)("li",{parentName:"ul"},"All Snowflake Tables that are in the Top 25% of most queried AND do not have a Domain ",(0,n.yg)("strong",{parentName:"li"},"must have\na Glossary Term from the Classification Term Group"))),(0,n.yg)("h5",{id:"validating-test-conditions"},"Validating Test Conditions"),(0,n.yg)("p",null,"During Step 2, we can quickly verify that the Selection Criteria & Rules we've authored\nmatch our expectations by testing them against some existing assets indexed by DataHub."),(0,n.yg)("p",null,"To verify your Test conditions, simply click ",(0,n.yg)("strong",{parentName:"p"},"Try it out"),", find an asset to test against by searching & filtering down your assets,\nand finally click ",(0,n.yg)("strong",{parentName:"p"},"Run Test")," to see whether the asset is passes or fails the provided conditions."),(0,n.yg)("p",{align:"center"},(0,n.yg)("img",{width:"80%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/metadata-test-validate-conditions.png"})),(0,n.yg)("h4",{id:"step-3-defining-actions-optional"},"Step 3: Defining Actions (Optional)"),(0,n.yg)("blockquote",null,(0,n.yg)("p",{parentName:"blockquote"},"If you don't wish to take any actions for assets that pass or fail the test, simply click 'Skip'.")),(0,n.yg)("p",null,"In the third step, we can define a set of Actions that will be automatically applied to each selected asset which passes or fails the Rules conditions."),(0,n.yg)("p",null,"For example, we may wish to mark ",(0,n.yg)("strong",{parentName:"p"},"passing"),' assets with a special DataHub Tag or Glossary Term (e.g. "Tier 1"), or remove these special marking for those which are failing.\nThis allows us to automatically control classifications of data assets as they move in and out of compliance with the Rules defined in Step 2.'),(0,n.yg)("p",null,"A few of the supported Action types include:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Adding or removing specific Tags"),(0,n.yg)("li",{parentName:"ul"},"Adding or removing specific Glossary Terms"),(0,n.yg)("li",{parentName:"ul"},"Adding or removing specific Owners"),(0,n.yg)("li",{parentName:"ul"},"Adding or removing to a specific Domain")),(0,n.yg)("p",{align:"center"},(0,n.yg)("img",{width:"80%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/metadata-test-define-actions.png"})),(0,n.yg)("h4",{id:"step-4-name-category-description"},"Step 4: Name, Category, Description"),(0,n.yg)("p",null,"In the final step, we can add a freeform name, category, and description for our new Metadata Test."),(0,n.yg)("h3",{id:"viewing-test-results"},"Viewing Test Results"),(0,n.yg)("p",null,"Metadata Test results can be viewed in 2 places:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"On an asset profile page (e.g. Dataset profile page), inside the ",(0,n.yg)("strong",{parentName:"li"},"Validation")," tab."),(0,n.yg)("li",{parentName:"ol"},"On the Metadata Tests management page. To view all assets passing or failing a particular test,\nsimply click on the labels which showing the number of passing or failing assets.")),(0,n.yg)("p",{align:"center"},(0,n.yg)("img",{width:"50%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/metadata-test-view-results.png"})),(0,n.yg)("h3",{id:"updating-an-existing-test"},"Updating an Existing Test"),(0,n.yg)("p",null,"To update an existing Test, simply click ",(0,n.yg)("strong",{parentName:"p"},"Edit")," on the test you wish to change."),(0,n.yg)("p",null,"Then, make the changes required and click ",(0,n.yg)("strong",{parentName:"p"},"Save"),". When you save a Test, it may take up to 2 minutes for changes\nto be reflected across DataHub."),(0,n.yg)("h3",{id:"removing-a-test"},"Removing a Test"),(0,n.yg)("p",null,"To remove a Test, simply click on the trashcan icon located on the Tests list. This will remove the Test and\ndeactivate it so that it no is evaluated."),(0,n.yg)("p",null,"When you delete a Test, it may take up to 2 minutes for changes to be reflected."),(0,n.yg)("h3",{id:"graphql"},"GraphQL"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"/docs/graphql/queries#listtests"},"listTests")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"/docs/graphql/mutations#createtest"},"createTest")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"/docs/graphql/mutations#deletetest"},"deleteTest"))),(0,n.yg)("h2",{id:"performance-tuning-for-batch-evaluation"},"Performance Tuning for Batch Evaluation"),(0,n.yg)("p",null,"This section covers performance considerations for scheduled batch evaluation of Metadata Tests."),(0,n.yg)("h3",{id:"batch-processing-architecture"},"Batch Processing Architecture"),(0,n.yg)("p",null,"The batch evaluation job runs in the ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub-upgrade")," container with direct local access to ",(0,n.yg)("inlineCode",{parentName:"p"},"EntityService")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"EntitySearchService"),". Test results and actions are written asynchronously to Kafka, then processed by the MCE Consumer (embedded in GMS by default) which writes to the database."),(0,n.yg)("h3",{id:"when-elasticsearch-executor-is-used"},"When ElasticSearch Executor is Used"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Enabled by default (",(0,n.yg)("inlineCode",{parentName:"li"},"METADATA_TESTS_ELASTICSEARCH_EXECUTOR_ENABLED=true"),")"),(0,n.yg)("li",{parentName:"ul"},"Used when test selection criteria and rules can be expressed as ElasticSearch queries"),(0,n.yg)("li",{parentName:"ul"},"Provides faster evaluation for large entity counts"),(0,n.yg)("li",{parentName:"ul"},"Falls back to SQL queries via local EntityService for unsupported predicates")),(0,n.yg)("h3",{id:"key-configuration-variable"},"Key Configuration Variable"),(0,n.yg)("p",null,"The primary performance tuning parameter is ",(0,n.yg)("strong",{parentName:"p"},(0,n.yg)("inlineCode",{parentName:"strong"},"METADATA_TESTS_ACTIONS_CONCURRENCY"))," (default: ",(0,n.yg)("inlineCode",{parentName:"p"},"2"),"), which controls the number of threads generating action MCPs. Increasing this value allows more actions to be produced in parallel, improving throughput when processing large numbers of test results."),(0,n.yg)("h3",{id:"scaling-recommendations"},"Scaling Recommendations"),(0,n.yg)("p",null,"Since test evaluation uses local services, the bottleneck is MCE Consumer throughput processing async writes from Kafka. To scale effectively:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Adequate Kafka partition count on ",(0,n.yg)("inlineCode",{parentName:"li"},"MetadataChangeProposal_v1")," topic"),(0,n.yg)("li",{parentName:"ul"},"Sufficient MCE consumer capacity (embedded or standalone)")),(0,n.yg)("p",null,"For standalone MCE/MAE consumer deployment and Kafka configuration, see ",(0,n.yg)("a",{parentName:"p",href:"/docs/how/kafka-config"},"Configuring Kafka"),". For complete environment variable reference, see ",(0,n.yg)("a",{parentName:"p",href:"/docs/deploy/environment-vars#component-configuration"},"Environment Variables"),"."),(0,n.yg)("h2",{id:"faq-and-troubleshooting"},"FAQ and Troubleshooting"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"When are Metadata Tests evaluated?")),(0,n.yg)("p",null,"Metadata Tests are evaluated in two scenarios:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Real-time evaluation"),": When an individual asset changes in DataHub, all tests that include it in scope are evaluated. This feature is typically disabled by default. It can be enabled on demand, subject to disussion.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Scheduled evaluation"),": A dedicated Metadata Test evaluator runs on a recurring schedule (typically every 24 hours) and evaluates all tests against the entire Metadata Graph. The cadence can be adjusted. It can be made more frequent in limited cases where metadata is small, subject to discussion."))),(0,n.yg)("p",null,"Both scenarios have increased server load concerns which may require additional server resources and consultation with your Acryl representative for the associated cost."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Can I configure a custom evaluation schedule for my Metadata Test?")),(0,n.yg)("p",null,"No, you cannot. Currently, the internal evaluator will ensure that tests are run continuously for\neach asset, regardless of whether it is being changed on DataHub."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"How is a Metadata Test different from an Assertion?")),(0,n.yg)("p",null,"An Assertion is a specific test, similar to a unit test, that is defined for a single data asset. Typically,\nit will include domain-specific knowledge about the asset and test against physical attributes of it. For example, an Assertion\nmay verify that the number of rows for a specific table in Snowflake falls into a well-defined range."),(0,n.yg)("p",null,"A Metadata Test is a broad spanning predicate which applies to a subset of the Metadata Graph (e.g. across multiple\ndata assets). Typically, it is defined against ",(0,n.yg)("em",{parentName:"p"},"metadata")," attributes, as opposed to the physical data itself. For example,\na Metadata Test may verify that ALL tables in Snowflake have at least 1 assigned owner, and a human-authored description.\nMetadata Tests allow you to manage broad policies across your entire data ecosystem driven by metadata, for example to\naugment a larger scale Data Governance initiative."))}y.isMDXComponent=!0}}]);