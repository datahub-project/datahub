"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[43130],{15680:(e,a,t)=>{t.d(a,{xA:()=>g,yg:()=>y});var n=t(96540);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function l(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?l(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):l(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},l=Object.keys(e);for(n=0;n<l.length;n++)t=l[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)t=l[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var p=n.createContext({}),o=function(e){var a=n.useContext(p),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},g=function(e){var a=o(e.components);return n.createElement(p.Provider,{value:a},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},m=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,l=e.originalType,p=e.parentName,g=s(e,["components","mdxType","originalType","parentName"]),u=o(t),m=r,y=u["".concat(p,".").concat(m)]||u[m]||d[m]||l;return t?n.createElement(y,i(i({ref:a},g),{},{components:t})):n.createElement(y,i({ref:a},g))}));function y(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var l=t.length,i=new Array(l);i[0]=m;var s={};for(var p in a)hasOwnProperty.call(a,p)&&(s[p]=a[p]);s.originalType=e,s[u]="string"==typeof e?e:r,i[1]=s;for(var o=2;o<l;o++)i[o]=t[o];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}m.displayName="MDXCreateElement"},29116:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>g,contentTitle:()=>p,default:()=>y,frontMatter:()=>s,metadata:()=>o,toc:()=>u});t(96540);var n=t(15680);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function l(e,a){return a=null!=a?a:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):function(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))})),e}function i(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},l=Object.keys(e);for(n=0;n<l.length;n++)t=l[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)t=l[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}const s={title:"Spark",slug:"/metadata-integration/java/acryl-spark-lineage",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/acryl-spark-lineage/README.md"},p="Spark",o={unversionedId:"metadata-integration/java/acryl-spark-lineage/README",id:"version-1.1.0/metadata-integration/java/acryl-spark-lineage/README",title:"Spark",description:"To integrate Spark with DataHub, we provide a lightweight Java agent that listens for Spark application and job events",source:"@site/versioned_docs/version-1.1.0/metadata-integration/java/acryl-spark-lineage/README.md",sourceDirName:"metadata-integration/java/acryl-spark-lineage",slug:"/metadata-integration/java/acryl-spark-lineage",permalink:"/docs/1.1.0/metadata-integration/java/acryl-spark-lineage",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/acryl-spark-lineage/README.md",tags:[],version:"1.1.0",frontMatter:{title:"Spark",slug:"/metadata-integration/java/acryl-spark-lineage",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/acryl-spark-lineage/README.md"},sidebar:"overviewSidebar",previous:{title:"Prefect Integration with DataHub",permalink:"/docs/1.1.0/lineage/prefect"},next:{title:"Great Expectations",permalink:"/docs/1.1.0/metadata-ingestion/integration_docs/great-expectations"}},g={},u=[{value:"Configuring Spark agent",id:"configuring-spark-agent",level:2},{value:"Before you begin: Versions and Release Notes",id:"before-you-begin-versions-and-release-notes",level:3},{value:"Configuration Instructions: spark-submit",id:"configuration-instructions-spark-submit",level:3},{value:"spark-submit command line",id:"spark-submit-command-line",level:2},{value:"Configuration Instructions: Amazon EMR",id:"configuration-instructions-amazon-emr",level:3},{value:"Configuration Instructions: Notebooks",id:"configuration-instructions-notebooks",level:3},{value:"Configuration Instructions: Standalone Java Applications",id:"configuration-instructions-standalone-java-applications",level:3},{value:"Configuration Instructions: Databricks",id:"configuration-instructions-databricks",level:3},{value:"Configuration Options",id:"configuration-options",level:2},{value:"What to Expect: The Metadata Model",id:"what-to-expect-the-metadata-model",level:2},{value:"Custom properties &amp; relating to Spark UI",id:"custom-properties--relating-to-spark-ui",level:3},{value:"Spark versions supported",id:"spark-versions-supported",level:3},{value:"Environments tested with",id:"environments-tested-with",level:3},{value:"Configuring Hdfs based dataset URNs",id:"configuring-hdfs-based-dataset-urns",level:3},{value:"Important notes on usage",id:"important-notes-on-usage",level:3},{value:"Debugging",id:"debugging",level:3},{value:"How to build",id:"how-to-build",level:2},{value:"Known limitations",id:"known-limitations",level:2},{value:"Changelog",id:"changelog",level:2},{value:"Version 0.2.17",id:"version-0217",level:3},{value:"Version 0.2.16",id:"version-0216",level:3},{value:"Version 0.2.15",id:"version-0215",level:3},{value:"Version 0.2.14",id:"version-0214",level:3},{value:"Version 0.2.13",id:"version-0213",level:3},{value:"Version 0.2.12",id:"version-0212",level:3},{value:"Version 0.2.11",id:"version-0211",level:3}],d={toc:u},m="wrapper";function y(e){var{components:a}=e,t=i(e,["components"]);return(0,n.yg)(m,l(function(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{},n=Object.keys(t);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(t).filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable})))),n.forEach((function(a){r(e,a,t[a])}))}return e}({},d,t),{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"spark"},"Spark"),(0,n.yg)("p",null,"To integrate Spark with DataHub, we provide a lightweight Java agent that listens for Spark application and job events\nand pushes metadata out to DataHub in real-time. The agent listens to events such application start/end, and\nSQLExecution start/end to create pipelines (i.e. DataJob) and tasks (i.e. DataFlow) in Datahub along with lineage to\ndatasets that are being read from and written to. Read on to learn how to configure this for different Spark scenarios."),(0,n.yg)("h2",{id:"configuring-spark-agent"},"Configuring Spark agent"),(0,n.yg)("p",null,"The Spark agent can be configured using a config file or while creating a Spark Session. If you are using Spark on\nDatabricks, refer ",(0,n.yg)("a",{parentName:"p",href:"#configuration-instructions--databricks"},"Configuration Instructions for Databricks"),"."),(0,n.yg)("h3",{id:"before-you-begin-versions-and-release-notes"},"Before you begin: Versions and Release Notes"),(0,n.yg)("p",null,"Versioning of the jar artifact will follow the semantic versioning of the\nmain ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub"},"DataHub repo")," and release notes will be\navailable ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/releases"},"here"),".\nAlways check ",(0,n.yg)("a",{parentName:"p",href:"https://search.maven.org/search?q=a:acryl-spark-lineage"},"the Maven central repository")," for the latest\nreleased version."),(0,n.yg)("h3",{id:"configuration-instructions-spark-submit"},"Configuration Instructions: spark-submit"),(0,n.yg)("p",null,"When running jobs using spark-submit, the agent needs to be configured in the config file."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-text"},"#Configuring DataHub spark agent jar\nspark.jars.packages                          io.acryl:acryl-spark-lineage:0.2.17\nspark.extraListeners                         datahub.spark.DatahubSparkListener\nspark.datahub.rest.server                    http://localhost:8080\n")),(0,n.yg)("h2",{id:"spark-submit-command-line"},"spark-submit command line"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-sh"},'spark-submit --packages io.acryl:acryl-spark-lineage:0.2.17 --conf "spark.extraListeners=datahub.spark.DatahubSparkListener" my_spark_job_to_run.py\n')),(0,n.yg)("h3",{id:"configuration-instructions-amazon-emr"},"Configuration Instructions: Amazon EMR"),(0,n.yg)("p",null,"Set the following spark-defaults configuration properties as it\nstated ",(0,n.yg)("a",{parentName:"p",href:"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html"},"here")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-text"},"spark.jars.packages                          io.acryl:acryl-spark-lineage:0.2.17\nspark.extraListeners                         datahub.spark.DatahubSparkListener\nspark.datahub.rest.server                    https://your_datahub_host/gms\n#If you have authentication set up then you also need to specify the Datahub access token\nspark.datahub.rest.token                     yourtoken\n")),(0,n.yg)("h3",{id:"configuration-instructions-notebooks"},"Configuration Instructions: Notebooks"),(0,n.yg)("p",null,"When running interactive jobs from a notebook, the listener can be configured while building the Spark Session."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'spark = SparkSession.builder\n.master("spark://spark-master:7077")\n.appName("test-application")\n.config("spark.jars.packages", "io.acryl:acryl-spark-lineage:0.2.17")\n.config("spark.extraListeners", "datahub.spark.DatahubSparkListener")\n.config("spark.datahub.rest.server", "http://localhost:8080")\n.enableHiveSupport()\n.getOrCreate()\n')),(0,n.yg)("h3",{id:"configuration-instructions-standalone-java-applications"},"Configuration Instructions: Standalone Java Applications"),(0,n.yg)("p",null,"The configuration for standalone Java apps is very similar."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'spark =SparkSession.\n\nbuilder()\n        .\n\nappName("test-application")\n        .\n\nconfig("spark.master","spark://spark-master:7077")\n        .\n\nconfig("spark.jars.packages","io.acryl:acryl-spark-lineage:0.2.17")\n        .\n\nconfig("spark.extraListeners","datahub.spark.DatahubSparkListener")\n        .\n\nconfig("spark.datahub.rest.server","http://localhost:8080")\n        .\n\nenableHiveSupport()\n        .\n\ngetOrCreate();\n')),(0,n.yg)("h3",{id:"configuration-instructions-databricks"},"Configuration Instructions: Databricks"),(0,n.yg)("p",null,"The Spark agent can be configured using Databricks\nCluster ",(0,n.yg)("a",{parentName:"p",href:"https://docs.databricks.com/clusters/configure.html#spark-configuration"},"Spark configuration"),"\nand ",(0,n.yg)("a",{parentName:"p",href:"https://docs.databricks.com/clusters/configure.html#init-scripts"},"Init script"),"."),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://docs.databricks.com/security/secrets/secrets.html"},"Databricks Secrets")," can be leveraged to store sensitive\ninformation like tokens."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Download ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub-spark-lineage")," jar\nfrom ",(0,n.yg)("a",{parentName:"p",href:"https://s01.oss.sonatype.org/content/groups/public/io/acryl/acryl-spark-lineage/"},"the Maven central repository"),".")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Create ",(0,n.yg)("inlineCode",{parentName:"p"},"init.sh")," with below content"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-sh"},"#!/bin/bash\ncp /dbfs/datahub/datahub-spark-lineage*.jar /databricks/jars\n"))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Install and configure ",(0,n.yg)("a",{parentName:"p",href:"https://docs.databricks.com/dev-tools/cli/index.html"},"Databricks CLI"),".")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Copy jar and init script to Databricks File System(DBFS) using Databricks CLI."),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-sh"},"databricks fs mkdirs dbfs:/datahub\ndatabricks fs cp --overwrite datahub-spark-lineage*.jar dbfs:/datahub\ndatabricks fs cp --overwrite init.sh dbfs:/datahub\n"))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Open Databricks Cluster configuration page. Click the ",(0,n.yg)("strong",{parentName:"p"},"Advanced Options")," toggle. Click the ",(0,n.yg)("strong",{parentName:"p"},"Spark")," tab. Add below\nconfigurations under ",(0,n.yg)("inlineCode",{parentName:"p"},"Spark Config"),"."),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-text"},"spark.extraListeners                    datahub.spark.DatahubSparkListener\nspark.datahub.rest.server               http://localhost:8080\nspark.datahub.stage_metadata_coalescing true\nspark.datahub.databricks.cluster        cluster-name<any preferred cluster identifier>\n"))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Click the ",(0,n.yg)("strong",{parentName:"p"},"Init Scripts")," tab. Set cluster init script as ",(0,n.yg)("inlineCode",{parentName:"p"},"dbfs:/datahub/init.sh"),".")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Configuring DataHub authentication token"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Add below config in cluster spark config."),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-text"},"spark.datahub.rest.token <token>\n"))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Alternatively, Databricks secrets can be used to secure token."),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Create secret using Databricks CLI."),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-sh"},"databricks secrets create-scope --scope datahub --initial-manage-principal users\ndatabricks secrets put --scope datahub --key rest-token\ndatabricks secrets list --scope datahub &lt;&lt;Edit prompted file with token value&gt;&gt;\n"))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Add in spark config"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-text"},"spark.datahub.rest.token {{secrets/datahub/rest-token}}\n")))))))),(0,n.yg)("h2",{id:"configuration-options"},"Configuration Options"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Field"),(0,n.yg)("th",{parentName:"tr",align:null},"Required"),(0,n.yg)("th",{parentName:"tr",align:null},"Default"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"),(0,n.yg)("th",{parentName:"tr",align:null}))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.jars.packages"),(0,n.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Set with latest/required version io.acryl:acryl-spark-lineage:0.2.15"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.extraListeners"),(0,n.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"datahub.spark.DatahubSparkListener"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.emitter"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"rest"),(0,n.yg)("td",{parentName:"tr",align:null},"Specify the ways to emit metadata. By default it sends to DataHub using REST emitter. Valid options are rest, kafka or file"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.rest.server"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"http://localhost:8080"),(0,n.yg)("td",{parentName:"tr",align:null},"Datahub server url eg:",(0,n.yg)("a",{parentName:"td",href:"http://localhost:8080"},"http://localhost:8080")),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.rest.token"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Authentication token."),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.rest.disable_ssl_verification"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"false"),(0,n.yg)("td",{parentName:"tr",align:null},"Disable SSL certificate validation. Caution: Only use this if you know what you are doing!"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.rest.disable_chunked_encoding"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"false"),(0,n.yg)("td",{parentName:"tr",align:null},"Disable Chunked Transfer Encoding. In some environment chunked encoding causes issues. With this config option it can be disabled."),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.rest.max_retries"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"0"),(0,n.yg)("td",{parentName:"tr",align:null},"Number of times a request retried if failed"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.rest.retry_interval"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"10"),(0,n.yg)("td",{parentName:"tr",align:null},"Number of seconds to wait between retries"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.file.filename"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"The file where metadata will be written if file emitter is set"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.kafka.bootstrap"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"The Kafka bootstrap server url to use if the Kafka emitter is set"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.kafka.schema_registry_url"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"The Schema registry url to use if the Kafka emitter is set"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.kafka.schema_registry_config."),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Additional config to pass in to the Schema Registry Client"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.kafka.producer_config."),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Additional config to pass in to the Kafka producer. For example: ",(0,n.yg)("inlineCode",{parentName:"td"},'--conf "spark.datahub.kafka.producer_config.client.id=my_client_id"')),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.metadata.pipeline.platformInstance"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Pipeline level platform instance"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.metadata.dataset.platformInstance"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"dataset level platform instance (it is usefult to set if you have it in your glue ingestion)"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.metadata.dataset.env"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"PROD"),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("a",{parentName:"td",href:"/docs/graphql/enums#fabrictype"},"Supported values"),". In all other cases, will fallback to PROD"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.metadata.dataset.hivePlatformAlias"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"hive"),(0,n.yg)("td",{parentName:"tr",align:null},"By default, datahub assigns Hive-like tables to the Hive platform. If you are using Glue as your Hive metastore, set this config flag to ",(0,n.yg)("inlineCode",{parentName:"td"},"glue")),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.metadata.include_scheme"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"true"),(0,n.yg)("td",{parentName:"tr",align:null},"Include scheme from the path URI (e.g. hdfs://, s3://) in the dataset URN. We recommend setting this value to false, it is set to true for backwards compatibility with previous versions"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.metadata.remove_partition_pattern"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Remove partition pattern. (e.g. /partition=\\d+) It change database/table/partition=123 to database/table"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.coalesce_jobs"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"true"),(0,n.yg)("td",{parentName:"tr",align:null},"Only one datajob(task) will be emitted containing all input and output datasets for the spark application"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.parent.datajob_urn"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Specified dataset will be set as upstream dataset for datajob created. Effective only when spark.datahub.coalesce_jobs is set to true"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.metadata.dataset.materialize"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"false"),(0,n.yg)("td",{parentName:"tr",align:null},"Materialize Datasets in DataHub"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.platform.s3.path_spec_list"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"List of pathspec per platform"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.metadata.dataset.include_schema_metadata"),(0,n.yg)("td",{parentName:"tr",align:null},"false"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Emit dataset schema metadata based on the spark execution. It is recommended to get schema information from platform specific DataHub sources as this is less reliable"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.flow_name"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"If it is set it will be used as the DataFlow name otherwise it uses spark app name as flow_name"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.file_partition_regexp"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Strip partition part from the path if path end matches with the specified regexp. Example ",(0,n.yg)("inlineCode",{parentName:"td"},"year=.*/month=.*/day=.*")),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.tags"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Comma separated list of tags to attach to the DataFlow"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.domains"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Comma separated list of domain urns to attach to the DataFlow"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.stage_metadata_coalescing"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Normally it coalesces and sends metadata at the onApplicationEnd event which is never called on Databricks or on Glue. You should enable this on Databricks if you want coalesced run."),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.patch.enabled"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"false"),(0,n.yg)("td",{parentName:"tr",align:null},"Set this to true to send lineage as a patch, which appends rather than overwrites existing Dataset lineage edges. By default, it is disabled."),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.metadata.dataset.lowerCaseUrns"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"false"),(0,n.yg)("td",{parentName:"tr",align:null},"Set this to true to lowercase dataset urns. By default, it is disabled."),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.disableSymlinkResolution"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"false"),(0,n.yg)("td",{parentName:"tr",align:null},"Set this to true if you prefer using the s3 location instead of the Hive table. By default, it is disabled."),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.s3.bucket"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"The name of the bucket where metadata will be written if s3 emitter is set"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.s3.prefix"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"The prefix for the file where metadata will be written on s3 if s3 emitter is set"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.s3.filename"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"The name of the file where metadata will be written if it is not set random filename will be used on s3 if s3 emitter is set"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.s3.filename"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"The name of the file where metadata will be written if it is not set random filename will be used on s3 if s3 emitter is set"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.log.mcps"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"true"),(0,n.yg)("td",{parentName:"tr",align:null},"Set this to true to log MCPS to the log. By default, it is enabled."),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"spark.datahub.legacyLineageCleanup.enabled"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"false"),(0,n.yg)("td",{parentName:"tr",align:null},"Set this to true to remove legacy lineages from older Spark Plugin runs. This will remove those lineages from the Datasets which it adds to DataJob. By default, it is disabled."),(0,n.yg)("td",{parentName:"tr",align:null})))),(0,n.yg)("h2",{id:"what-to-expect-the-metadata-model"},"What to Expect: The Metadata Model"),(0,n.yg)("p",null,"As of current writing, the Spark agent produces metadata related to the Spark job, tasks and lineage edges to datasets."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"A pipeline is created per Spark <master, appName>."),(0,n.yg)("li",{parentName:"ul"},"A task is created per unique Spark query execution within an app.")),(0,n.yg)("p",null,"For Spark on Databricks,"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"A pipeline is created per",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"cluster_identifier: specified with spark.datahub.databricks.cluster"),(0,n.yg)("li",{parentName:"ul"},"applicationID: on every restart of the cluster new spark applicationID will be created."))),(0,n.yg)("li",{parentName:"ul"},"A task is created per unique Spark query execution.")),(0,n.yg)("h3",{id:"custom-properties--relating-to-spark-ui"},"Custom properties & relating to Spark UI"),(0,n.yg)("p",null,"The following custom properties in pipelines and tasks relate to the Spark UI:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"appName and appId in a pipeline can be used to determine the Spark application"),(0,n.yg)("li",{parentName:"ul"},"Other custom properties of pipelines and tasks capture the start and end times of execution etc.")),(0,n.yg)("p",null,"For Spark on Databricks, pipeline start time is the cluster start time."),(0,n.yg)("h3",{id:"spark-versions-supported"},"Spark versions supported"),(0,n.yg)("p",null,"Supports Spark 3.x series."),(0,n.yg)("h3",{id:"environments-tested-with"},"Environments tested with"),(0,n.yg)("p",null,"This initial release has been tested with the following environments:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"spark-submit of Python/Java applications to local and remote servers"),(0,n.yg)("li",{parentName:"ul"},"Standalone Java applications"),(0,n.yg)("li",{parentName:"ul"},"Databricks Standalone Cluster"),(0,n.yg)("li",{parentName:"ul"},"EMR")),(0,n.yg)("p",null,"Testing with Databricks Standard and High-concurrency Cluster is not done yet."),(0,n.yg)("h3",{id:"configuring-hdfs-based-dataset-urns"},"Configuring Hdfs based dataset URNs"),(0,n.yg)("p",null,"Spark emits lineage between datasets. It has its own logic for generating urns. Python sources emit metadata of\ndatasets. To link these 2 things, urns generated by both have to match.\nThis section will help you to match urns to that of other ingestion sources.\nBy default, URNs are created using\ntemplate ",(0,n.yg)("inlineCode",{parentName:"p"},"urn:li:dataset:(urn:li:dataPlatform:<$platform>,<platformInstance>.<name>,<env>)"),". We can configure these 4\nthings to generate the desired urn."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Platform"),":\nHdfs-based platforms supported explicitly:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"AWS S3 (s3)"),(0,n.yg)("li",{parentName:"ul"},"Google Cloud Storage (gcs)"),(0,n.yg)("li",{parentName:"ul"},'local ( local file system) (local)\nAll other platforms will have "hdfs" as a platform.')),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Name"),":\nBy default, the name is the complete path. For Hdfs base datasets, tables can be at different levels in the path than\nthat of the actual file read due to various reasons like partitioning, and sharding. 'path_spec' is used to alter the\nname.\n{table} marker is used to specify the table level. Below are a few examples. One can specify multiple path_specs for\ndifferent paths specified in the ",(0,n.yg)("inlineCode",{parentName:"p"},"path_spec_list"),". Each actual path is matched against all path_spes present in the\nlist. First, one to match will be used to generate urn."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"path_spec Examples")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},'spark.datahub.platform.s3.path_spec_list=s3://my-bucket/foo/{table}/year=*/month=*/day=*/*,s3://my-other-bucket/foo/{table}/year=*/month=*/day=*/*"\n')),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Absolute path"),(0,n.yg)("th",{parentName:"tr",align:null},"path_spec"),(0,n.yg)("th",{parentName:"tr",align:null},"Urn"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"s3://my-bucket/foo/tests/bar.avro"),(0,n.yg)("td",{parentName:"tr",align:null},"Not provided"),(0,n.yg)("td",{parentName:"tr",align:null},"urn:li:dataset:(urn:li:dataPlatform:s3,my-bucket/foo/tests/bar.avro,PROD)")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"s3://my-bucket/foo/tests/bar.avro"),(0,n.yg)("td",{parentName:"tr",align:null},"s3://my-bucket/foo/{table}/","*"),(0,n.yg)("td",{parentName:"tr",align:null},"urn:li:dataset:(urn:li:dataPlatform:s3,my-bucket/foo/tests,PROD)")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"s3://my-bucket/foo/tests/bar.avro"),(0,n.yg)("td",{parentName:"tr",align:null},"s3://my-bucket/foo/tests/{table}"),(0,n.yg)("td",{parentName:"tr",align:null},"urn:li:dataset:(urn:li:dataPlatform:s3,my-bucket/foo/tests/bar.avro,PROD)")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"gs://my-bucket/foo/tests/bar.avro"),(0,n.yg)("td",{parentName:"tr",align:null},"gs://my-bucket/{table}/",(0,n.yg)("em",{parentName:"td"},"/")),(0,n.yg)("td",{parentName:"tr",align:null},"urn:li:dataset:(urn:li:dataPlatform:gcs,my-bucket/foo,PROD)")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"gs://my-bucket/foo/tests/bar.avro"),(0,n.yg)("td",{parentName:"tr",align:null},"gs://my-bucket/{table}"),(0,n.yg)("td",{parentName:"tr",align:null},"urn:li:dataset:(urn:li:dataPlatform:gcs,my-bucket/foo,PROD)")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"file:///my-bucket/foo/tests/bar.avro"),(0,n.yg)("td",{parentName:"tr",align:null},"file:///my-bucket/",(0,n.yg)("em",{parentName:"td"},"/"),"/{table}"),(0,n.yg)("td",{parentName:"tr",align:null},"urn:li:dataset:(urn:li:dataPlatform:local,my-bucket/foo/tests/bar.avro,PROD)")))),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"platform instance and env:")),(0,n.yg)("p",null,"The default value for env is 'PROD' and the platform instance is None. env and platform instances can be set for all\ndatasets using configurations 'spark.datahub.metadata.dataset.env' and 'spark.datahub.metadata.dataset.platformInstace'.\nIf spark is processing data that belongs to a different env or platform instance, then 'path_alias' can be used to\nspecify ",(0,n.yg)("inlineCode",{parentName:"p"},"path_spec")," specific values of these. 'path_alias' groups the 'path_spec_list', its env, and platform instance\ntogether."),(0,n.yg)("p",null,"path_alias_list Example:"),(0,n.yg)("p",null,'The below example explains the configuration of the case, where files from 2 buckets are being processed in a single\nspark application and files from my-bucket are supposed to have "instance1" as platform instance and "PROD" as env, and\nfiles from bucket2 should have env "DEV" in their dataset URNs.'),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"spark.datahub.platform.s3.path_alias_list :  path1,path2\nspark.datahub.platform.s3.path1.env : PROD\nspark.datahub.platform.s3.path1.path_spec_list: s3://my-bucket/*/*/{table}\nspark.datahub.platform.s3.path1.platform_instance : instance-1\nspark.datahub.platform.s3.path2.env: DEV\nspark.datahub.platform.s3.path2.path_spec_list: s3://bucket2/*/{table}\n")),(0,n.yg)("h3",{id:"important-notes-on-usage"},"Important notes on usage"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"It is advisable to ensure appName is used appropriately to ensure you can trace lineage from a pipeline back to your\nsource code."),(0,n.yg)("li",{parentName:"ul"},"If multiple apps with the same appName run concurrently, dataset-lineage will be captured correctly but the\ncustom-properties e.g. app-id, SQLQueryId would be unreliable. We expect this to be quite rare."),(0,n.yg)("li",{parentName:"ul"},"If spark execution fails, then an empty pipeline would still get created, but it may not have any tasks."),(0,n.yg)("li",{parentName:"ul"},"For HDFS sources, the folder (name) is regarded as the dataset (name) to align with typical storage of parquet/csv\nformats.")),(0,n.yg)("h3",{id:"debugging"},"Debugging"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Following info logs are generated")),(0,n.yg)("p",null,"On Spark context startup"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-text"},"YY/MM/DD HH:mm:ss INFO DatahubSparkListener: DatahubSparkListener initialised.\nYY/MM/DD HH:mm:ss INFO SparkContext: Registered listener datahub.spark.DatahubSparkListener\n")),(0,n.yg)("p",null,"On application start"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-text"},"YY/MM/DD HH:mm:ss INFO DatahubSparkListener: Application started: SparkListenerApplicationStart(AppName,Some(local-1644489736794),1644489735772,user,None,None)\nYY/MM/DD HH:mm:ss INFO McpEmitter: REST Emitter Configuration: GMS url <rest.server>\nYY/MM/DD HH:mm:ss INFO McpEmitter: REST Emitter Configuration: Token XXXXX\n")),(0,n.yg)("p",null,"On pushing data to server"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-text"},'YY/MM/DD HH:mm:ss INFO McpEmitter: MetadataWriteResponse(success=true, responseContent={"value":"<URN>"}, underlyingResponse=HTTP/1.1 200 OK [Date: day, DD month year HH:mm:ss GMT, Content-Type: application/json, X-RestLi-Protocol-Version: 2.0.0, Content-Length: 97, Server: Jetty(9.4.46.v20220331)] [Content-Length: 97,Chunked: false])\n')),(0,n.yg)("p",null,"On application end"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-text"},"YY/MM/DD HH:mm:ss INFO DatahubSparkListener: Application ended : AppName AppID\n")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"To enable debugging logs, add below configuration in log4j.properties file")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-properties"},"log4j.logger.datahub.spark=DEBUG\nlog4j.logger.datahub.client.rest=DEBUG\n")),(0,n.yg)("h2",{id:"how-to-build"},"How to build"),(0,n.yg)("p",null,"Use Java 8 to build the project. The project uses Gradle as the build tool. To build the project, run the following command:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"./gradlew -PjavaClassVersionDefault=8 :metadata-integration:java:acryl-spark-lineage:shadowJar\n")),(0,n.yg)("h2",{id:"known-limitations"},"Known limitations"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"})),(0,n.yg)("h2",{id:"changelog"},"Changelog"),(0,n.yg)("h3",{id:"version-0217"},"Version 0.2.17"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("em",{parentName:"p"},"Major changes"),":"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Finegrained lineage is emitted on the DataJob and not on the emitted Datasets. This is the correct behaviour which was not correct earlier. This causes earlier emitted finegrained lineages won't be overwritten by the new ones.\nYou can remove the old lineages by setting ",(0,n.yg)("inlineCode",{parentName:"li"},"spark.datahub.legacyLineageCleanup.enabled=true"),". Make sure you have the latest server if you enable with patch support. (this was introduced since 0.2.17-rc5)"))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("em",{parentName:"p"},"Changes"),":"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"OpenLineage 1.25.0 upgrade"),(0,n.yg)("li",{parentName:"ul"},"Add option to disable chunked encoding in the datahub rest sink -> ",(0,n.yg)("inlineCode",{parentName:"li"},"spark.datahub.rest.disable_chunked_encoding")),(0,n.yg)("li",{parentName:"ul"},"Add option to specify the mcp kafka topic for the datahub kafka sink -> ",(0,n.yg)("inlineCode",{parentName:"li"},"spark.datahub.kafka.mcp_topic")),(0,n.yg)("li",{parentName:"ul"},"Add option to remove legacy lineages from older Spark Plugin runs. This will remove those lineages from the Datasets which it adds to DataJob -> ",(0,n.yg)("inlineCode",{parentName:"li"},"spark.datahub.legacyLineageCleanup.enabled")))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("em",{parentName:"p"},"Fixes"),":"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Fix handling map transformation in the lineage. Earlier it generated wrong lineage for map transformation.")))),(0,n.yg)("h3",{id:"version-0216"},"Version 0.2.16"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Remove logging DataHub config into logs")),(0,n.yg)("h3",{id:"version-0215"},"Version 0.2.15"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Add Kafka emitter to emit lineage to kafka"),(0,n.yg)("li",{parentName:"ul"},"Add File emitter to emit lineage to file"),(0,n.yg)("li",{parentName:"ul"},"Add S3 emitter to save mcps to s3"),(0,n.yg)("li",{parentName:"ul"},"Upgrading OpenLineage to 1.19.0"),(0,n.yg)("li",{parentName:"ul"},"Renaming project to acryl-datahub-spark-lineage"),(0,n.yg)("li",{parentName:"ul"},"Supporting OpenLineage 1.17+ glue identifier changes"),(0,n.yg)("li",{parentName:"ul"},"Fix handling OpenLineage input/output where wasn't any facet attached")),(0,n.yg)("h3",{id:"version-0214"},"Version 0.2.14"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Fix warning about MeterFilter warning from Micrometer")),(0,n.yg)("h3",{id:"version-0213"},"Version 0.2.13"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Add kafka emitter to emit lineage to kafka")),(0,n.yg)("h3",{id:"version-0212"},"Version 0.2.12"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Silencing some chatty warnings in RddPathUtils")),(0,n.yg)("h3",{id:"version-0211"},"Version 0.2.11"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Add option to lowercase dataset URNs"),(0,n.yg)("li",{parentName:"ul"},"Add option to set platform instance and/or env per platform with ",(0,n.yg)("inlineCode",{parentName:"li"},"spark.datahub.platform.<platform_name>.env")," and ",(0,n.yg)("inlineCode",{parentName:"li"},"spark.datahub.platform.<platform_name>.platform_instance")," config parameter"),(0,n.yg)("li",{parentName:"ul"},"Fixing platform instance setting for datasets when ",(0,n.yg)("inlineCode",{parentName:"li"},"spark.datahub.metadata.dataset.platformInstance")," is set"),(0,n.yg)("li",{parentName:"ul"},"Fixing column level lineage support when patch is enabled")))}y.isMDXComponent=!0}}]);