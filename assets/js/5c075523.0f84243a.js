"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[72971],{15680:(e,t,a)=>{a.d(t,{xA:()=>c,yg:()=>y});var n=a(96540);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),u=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},c=function(e){var t=u(e.components);return n.createElement(s.Provider,{value:t},e.children)},g="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),g=u(a),p=r,y=g["".concat(s,".").concat(p)]||g[p]||m[p]||i;return a?n.createElement(y,l(l({ref:t},c),{},{components:a})):n.createElement(y,l({ref:t},c))}));function y(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,l=new Array(i);l[0]=p;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[g]="string"==typeof e?e:r,l[1]=o;for(var u=2;u<i;u++)l[u]=a[u];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},96467:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>y,frontMatter:()=>o,metadata:()=>u,toc:()=>g});a(96540);var n=a(15680);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){return t=null!=t?t:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):function(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))})),e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}const o={title:"Monitoring DataHub",slug:"/advanced/monitoring",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/advanced/monitoring.md"},s="Monitoring DataHub",u={unversionedId:"docs/advanced/monitoring",id:"docs/advanced/monitoring",title:"Monitoring DataHub",description:"Overview",source:"@site/genDocs/docs/advanced/monitoring.md",sourceDirName:"docs/advanced",slug:"/advanced/monitoring",permalink:"/docs/advanced/monitoring",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/advanced/monitoring.md",tags:[],version:"current",frontMatter:{title:"Monitoring DataHub",slug:"/advanced/monitoring",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/advanced/monitoring.md"},sidebar:"overviewSidebar",previous:{title:"Configuring Database Retention",permalink:"/docs/advanced/db-retention"},next:{title:"Telemetry",permalink:"/docs/deploy/telemetry"}},c={},g=[{value:"Overview",id:"overview",level:2},{value:"Why Monitor DataHub?",id:"why-monitor-datahub",level:2},{value:"Observability Components",id:"observability-components",level:2},{value:"GraphQL Instrumentation (Micrometer)",id:"graphql-instrumentation-micrometer",level:2},{value:"Overview",id:"overview-1",level:3},{value:"Why Path-Level GraphQL Instrumentation Matters",id:"why-path-level-graphql-instrumentation-matters",level:3},{value:"Real-World Example",id:"real-world-example",level:3},{value:"What Path-Level Instrumentation Reveals",id:"what-path-level-instrumentation-reveals",level:3},{value:"Key Benefits",id:"key-benefits",level:3},{value:"1. <strong>Surgical Optimization</strong>",id:"1-surgical-optimization",level:4},{value:"2. <strong>Smart Query Patterns</strong>",id:"2-smart-query-patterns",level:4},{value:"3. <strong>Client-Specific Debugging</strong>",id:"3-client-specific-debugging",level:4},{value:"4. <strong>N+1 Query Detection</strong>",id:"4-n1-query-detection",level:4},{value:"Configuration Strategy",id:"configuration-strategy",level:3},{value:"Architecture",id:"architecture",level:3},{value:"Metrics Collected",id:"metrics-collected",level:3},{value:"Request-Level Metrics",id:"request-level-metrics",level:4},{value:"Field-Level Metrics",id:"field-level-metrics",level:4},{value:"Configuration Guide",id:"configuration-guide",level:3},{value:"Master Controls",id:"master-controls",level:4},{value:"Selective Field Instrumentation",id:"selective-field-instrumentation",level:4},{value:"1. <strong>Operation-Based Filtering</strong>",id:"1-operation-based-filtering",level:5},{value:"2. <strong>Path-Based Filtering</strong>",id:"2-path-based-filtering",level:5},{value:"3. <strong>Combined Filtering</strong>",id:"3-combined-filtering",level:5},{value:"Advanced Options",id:"advanced-options",level:4},{value:"Filtering Modes Explained",id:"filtering-modes-explained",level:3},{value:"Performance Considerations",id:"performance-considerations",level:3},{value:"Impact Assessment",id:"impact-assessment",level:4},{value:"Best Practices",id:"best-practices",level:4},{value:"Example Configurations",id:"example-configurations",level:3},{value:"Development Environment (Full Visibility)",id:"development-environment-full-visibility",level:4},{value:"Production - Targeted Monitoring",id:"production---targeted-monitoring",level:4},{value:"Production - Minimal Overhead",id:"production---minimal-overhead",level:4},{value:"Debugging Slow Queries",id:"debugging-slow-queries",level:3},{value:"Integration with Monitoring Stack",id:"integration-with-monitoring-stack",level:3},{value:"Kafka Consumer Instrumentation (Micrometer)",id:"kafka-consumer-instrumentation-micrometer",level:2},{value:"Overview",id:"overview-2",level:3},{value:"Why Kafka Queue Time Monitoring Matters",id:"why-kafka-queue-time-monitoring-matters",level:3},{value:"Real-World Impact",id:"real-world-impact",level:4},{value:"Architecture",id:"architecture-1",level:3},{value:"Metrics Collected",id:"metrics-collected-1",level:3},{value:"Core Metric",id:"core-metric",level:4},{value:"Statistical Distribution",id:"statistical-distribution",level:4},{value:"Configuration Guide",id:"configuration-guide-1",level:4},{value:"Key Monitoring Patterns",id:"key-monitoring-patterns",level:4},{value:"Performance Considerations",id:"performance-considerations-1",level:4},{value:"Migration from Legacy Metrics",id:"migration-from-legacy-metrics",level:4},{value:"DataHub Request Hook Latency Instrumentation (Micrometer)",id:"datahub-request-hook-latency-instrumentation-micrometer",level:2},{value:"Overview",id:"overview-3",level:3},{value:"Why Hook Latency Monitoring Matters",id:"why-hook-latency-monitoring-matters",level:3},{value:"Configuration",id:"configuration",level:3},{value:"Metrics Collected",id:"metrics-collected-2",level:3},{value:"Core Metric",id:"core-metric-1",level:4},{value:"Key Monitoring Patterns",id:"key-monitoring-patterns-1",level:4},{value:"Implementation Details",id:"implementation-details",level:4},{value:"Performance Considerations",id:"performance-considerations-2",level:4},{value:"Relationship to Kafka Queue Time Metrics",id:"relationship-to-kafka-queue-time-metrics",level:4},{value:"Cache Monitoring (Micrometer)",id:"cache-monitoring-micrometer",level:2},{value:"Overview",id:"overview-4",level:3},{value:"Automatic Cache Metrics",id:"automatic-cache-metrics",level:3},{value:"Core Metrics",id:"core-metrics",level:4},{value:"Derived Metrics",id:"derived-metrics",level:4},{value:"DataHub Cache Configuration",id:"datahub-cache-configuration",level:3},{value:"1. Entity Client Cache",id:"1-entity-client-cache",level:4},{value:"2. Usage Statistics Cache",id:"2-usage-statistics-cache",level:4},{value:"3. Search &amp; Lineage Cache",id:"3-search--lineage-cache",level:4},{value:"Monitoring Best Practices",id:"monitoring-best-practices",level:3},{value:"Key Indicators to Watch",id:"key-indicators-to-watch",level:4},{value:"Thread Pool Executor Monitoring (Micrometer)",id:"thread-pool-executor-monitoring-micrometer",level:2},{value:"Overview",id:"overview-5",level:3},{value:"Automatic Executor Metrics",id:"automatic-executor-metrics",level:3},{value:"Pool State Metrics",id:"pool-state-metrics",level:4},{value:"Queue Metrics",id:"queue-metrics",level:4},{value:"Performance Metrics",id:"performance-metrics",level:4},{value:"DataHub Executor Configurations",id:"datahub-executor-configurations",level:3},{value:"1. GraphQL Query Executor",id:"1-graphql-query-executor",level:4},{value:"2. Batch Processing Executors",id:"2-batch-processing-executors",level:4},{value:"3. Search &amp; Analytics Executors",id:"3-search--analytics-executors",level:4},{value:"Critical Monitoring Patterns",id:"critical-monitoring-patterns",level:3},{value:"Saturation Detection",id:"saturation-detection",level:4},{value:"Rejection &amp; Starvation",id:"rejection--starvation",level:4},{value:"Performance Analysis",id:"performance-analysis",level:4},{value:"Tuning Guidelines",id:"tuning-guidelines",level:3},{value:"Symptoms &amp; Solutions",id:"symptoms--solutions",level:4},{value:"Capacity Planning",id:"capacity-planning",level:4},{value:"Distributed Tracing",id:"distributed-tracing",level:2},{value:"Configuration Note",id:"configuration-note",level:3},{value:"Micrometer",id:"micrometer",level:2},{value:"Why Micrometer?",id:"why-micrometer",level:3},{value:"Micrometer Transition Plan",id:"micrometer-transition-plan",level:2},{value:"Current State",id:"current-state",level:3},{value:"Transition State",id:"transition-state",level:3},{value:"Future State",id:"future-state",level:3},{value:"DropWizard &amp; JMX",id:"dropwizard--jmx",level:2},{value:"Enable monitoring through docker-compose",id:"enable-monitoring-through-docker-compose",level:4},{value:"Health check endpoint",id:"health-check-endpoint",level:2}],m={toc:g},p="wrapper";function y(e){var{components:t}=e,a=l(e,["components"]);return(0,n.yg)(p,i(function(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{},n=Object.keys(a);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(a).filter((function(e){return Object.getOwnPropertyDescriptor(a,e).enumerable})))),n.forEach((function(t){r(e,t,a[t])}))}return e}({},m,a),{components:t,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"monitoring-datahub"},"Monitoring DataHub"),(0,n.yg)("h2",{id:"overview"},"Overview"),(0,n.yg)("p",null,"Monitoring DataHub's system components is essential for maintaining operational excellence, troubleshooting performance issues,\nand ensuring system reliability. This comprehensive guide covers how to implement observability in DataHub through tracing and metrics,\nand how to extract valuable insights from your running instances."),(0,n.yg)("h2",{id:"why-monitor-datahub"},"Why Monitor DataHub?"),(0,n.yg)("p",null,"Effective monitoring enables you to:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Identify Performance Bottlenecks: Pinpoint slow queries or API endpoints"),(0,n.yg)("li",{parentName:"ul"},"Debug Issues Faster: Trace requests across distributed components to locate failures"),(0,n.yg)("li",{parentName:"ul"},"Meet SLAs: Track and alert on key performance indicators")),(0,n.yg)("h2",{id:"observability-components"},"Observability Components"),(0,n.yg)("p",null,"DataHub's observability strategy consists of two complementary approaches:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Metrics Collection"),(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Purpose:")," Aggregate statistical data about system behavior over time\n",(0,n.yg)("strong",{parentName:"p"},"Technology:")," Transitioning from DropWizard/JMX to Micrometer"),(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Current State:")," DropWizard metrics exposed via JMX, collected by Prometheus\n",(0,n.yg)("strong",{parentName:"p"},"Future Direction:")," Native Micrometer integration for Spring-based metrics\n",(0,n.yg)("strong",{parentName:"p"},"Compatibility:")," Prometheus-compatible format with support for other metrics backends"),(0,n.yg)("p",{parentName:"li"},"Key Metrics Categories:"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Performance Metrics: Request latency, throughput, error rates"),(0,n.yg)("li",{parentName:"ul"},"Resource Metrics: CPU, memory utilization"),(0,n.yg)("li",{parentName:"ul"},"Application Metrics: Cache hit rates, queue depths, processing times"),(0,n.yg)("li",{parentName:"ul"},"Business Metrics: Entity counts, ingestion rates, search performance"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Distributed Tracing"),(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Purpose:")," Track individual requests as they flow through multiple services and components\n",(0,n.yg)("strong",{parentName:"p"},"Technology:")," OpenTelemetry-based instrumentation"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Provides end-to-end visibility of request lifecycles"),(0,n.yg)("li",{parentName:"ul"},"Automatically instruments popular libraries (Kafka, JDBC, Elasticsearch)"),(0,n.yg)("li",{parentName:"ul"},"Supports multiple backend systems (Jaeger, Zipkin, etc.)"),(0,n.yg)("li",{parentName:"ul"},"Enables custom span creation with minimal code changes")),(0,n.yg)("p",{parentName:"li"},"Key Benefits:"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Visualize request flow across microservices"),(0,n.yg)("li",{parentName:"ul"},"Identify latency hotspots"),(0,n.yg)("li",{parentName:"ul"},"Understand service dependencies"),(0,n.yg)("li",{parentName:"ul"},"Debug complex distributed transactions")))),(0,n.yg)("h2",{id:"graphql-instrumentation-micrometer"},"GraphQL Instrumentation (Micrometer)"),(0,n.yg)("h3",{id:"overview-1"},"Overview"),(0,n.yg)("p",null,"DataHub provides comprehensive instrumentation for its GraphQL API through Micrometer metrics, enabling detailed performance\nmonitoring and debugging capabilities. The instrumentation system offers flexible configuration options to balance between\nobservability depth and performance overhead."),(0,n.yg)("h3",{id:"why-path-level-graphql-instrumentation-matters"},"Why Path-Level GraphQL Instrumentation Matters"),(0,n.yg)("p",null,'Traditional GraphQL monitoring only tells you "the search query is slow" but not ',(0,n.yg)("strong",{parentName:"p"},"why"),". Without path-level instrumentation,\nyou're blind to which specific fields are causing performance bottlenecks in complex nested queries."),(0,n.yg)("h3",{id:"real-world-example"},"Real-World Example"),(0,n.yg)("p",null,"Consider this GraphQL query:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-graphql"},'query getSearchResults {\n  search(input: { query: "sales data" }) {\n    searchResults {\n      entity {\n        ... on Dataset {\n          name\n          owner {\n            # Path: /search/searchResults/entity/owner\n            corpUser {\n              displayName\n            }\n          }\n          lineage {\n            # Path: /search/searchResults/entity/lineage\n            upstreamCount\n            downstreamCount\n            upstreamEntities {\n              urn\n              name\n            }\n          }\n          schemaMetadata {\n            # Path: /search/searchResults/entity/schemaMetadata\n            fields {\n              fieldPath\n              description\n            }\n          }\n        }\n      }\n    }\n  }\n}\n')),(0,n.yg)("h3",{id:"what-path-level-instrumentation-reveals"},"What Path-Level Instrumentation Reveals"),(0,n.yg)("p",null,"With path-level metrics, you discover:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"/search/searchResults/entity/owner")," - 50ms (fast, well-cached)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"/search/searchResults/entity/lineage")," - 2500ms (SLOW! hitting graph database)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"/search/searchResults/entity/schemaMetadata")," - 150ms (acceptable)")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Without path metrics"),': "Search query takes 3 seconds"',(0,n.yg)("br",{parentName:"p"}),"\n",(0,n.yg)("strong",{parentName:"p"},"With path metrics"),': "Lineage resolution is the bottleneck"'),(0,n.yg)("h3",{id:"key-benefits"},"Key Benefits"),(0,n.yg)("h4",{id:"1-surgical-optimization"},"1. ",(0,n.yg)("strong",{parentName:"h4"},"Surgical Optimization")),(0,n.yg)("p",null,"Instead of guessing, you know exactly which resolver needs optimization. Maybe lineage needs better caching or pagination."),(0,n.yg)("h4",{id:"2-smart-query-patterns"},"2. ",(0,n.yg)("strong",{parentName:"h4"},"Smart Query Patterns")),(0,n.yg)("p",null,"Identify expensive patterns like:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},"# These paths consistently slow:\n/*/lineage/upstreamEntities/*\n/*/siblings/*/platform\n# Action: Add field-level caching or lazy loading\n")),(0,n.yg)("h4",{id:"3-client-specific-debugging"},"3. ",(0,n.yg)("strong",{parentName:"h4"},"Client-Specific Debugging")),(0,n.yg)("p",null,"Different clients request different fields. Path instrumentation shows:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Web UI requests are slow (requesting everything)"),(0,n.yg)("li",{parentName:"ul"},"API integrations timeout (requesting deep lineage)")),(0,n.yg)("h4",{id:"4-n1-query-detection"},"4. ",(0,n.yg)("strong",{parentName:"h4"},"N+1 Query Detection")),(0,n.yg)("p",null,"Spot resolver patterns that indicate N+1 problems:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"/users/0/permissions - 10ms\n/users/1/permissions - 10ms\n/users/2/permissions - 10ms\n... (100 more times)\n")),(0,n.yg)("h3",{id:"configuration-strategy"},"Configuration Strategy"),(0,n.yg)("p",null,"Start targeted to minimize overhead:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'# Focus on known slow operations\nfieldLevelOperations: "searchAcrossEntities,getDataset"\n\n# Target expensive resolver paths\nfieldLevelPaths: "/**/lineage/**,/**/relationships/**,/**/privileges"\n')),(0,n.yg)("h3",{id:"architecture"},"Architecture"),(0,n.yg)("p",null,"The GraphQL instrumentation is implemented through ",(0,n.yg)("inlineCode",{parentName:"p"},"GraphQLTimingInstrumentation"),", which extends GraphQL Java's instrumentation framework. It provides:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Request-level metrics"),": Overall query performance and error tracking"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Field-level metrics"),": Detailed timing for individual field resolvers"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Smart filtering"),": Configurable targeting of specific operations or field paths"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Low overhead"),": Minimal performance impact through efficient instrumentation")),(0,n.yg)("h3",{id:"metrics-collected"},"Metrics Collected"),(0,n.yg)("h4",{id:"request-level-metrics"},"Request-Level Metrics"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Metric: ",(0,n.yg)("inlineCode",{parentName:"strong"},"graphql.request.duration"))),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Type"),": Timer with percentiles (p50, p95, p99)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Tags"),":",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"operation"),': Operation name (e.g., "getSearchResultsForMultiple")'),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"operation.type"),": Query, mutation, or subscription"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"success"),": true/false based on error presence"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"field.filtering"),": Filtering mode applied (DISABLED, ALL_FIELDS, BY_OPERATION, BY_PATH, BY_BOTH)"))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Use Case"),": Monitor overall GraphQL performance, identify slow operations")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Metric: ",(0,n.yg)("inlineCode",{parentName:"strong"},"graphql.request.errors"))),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Type"),": Counter"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Tags"),":",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"operation"),": Operation name"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"operation.type"),": Query, mutation, or subscription"))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Use Case"),": Track error rates by operation")),(0,n.yg)("h4",{id:"field-level-metrics"},"Field-Level Metrics"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Metric: ",(0,n.yg)("inlineCode",{parentName:"strong"},"graphql.field.duration"))),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Type"),": Timer with percentiles (p50, p95, p99)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Tags"),":",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"parent.type"),': GraphQL parent type (e.g., "Dataset", "User")'),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"field"),": Field name being resolved"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"operation"),": Operation name context"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"success"),": true/false"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"path"),": Field path (optional, controlled by ",(0,n.yg)("inlineCode",{parentName:"li"},"fieldLevelPathEnabled"),")"))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Use Case"),": Identify slow field resolvers, optimize data fetching")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Metric: ",(0,n.yg)("inlineCode",{parentName:"strong"},"graphql.field.errors"))),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Type"),": Counter"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Tags"),": Same as field duration (minus success tag)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Use Case"),": Track field-specific error patterns")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Metric: ",(0,n.yg)("inlineCode",{parentName:"strong"},"graphql.fields.instrumented"))),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Type"),": Counter"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Tags"),":",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"operation"),": Operation name"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"filtering.mode"),": Active filtering mode"))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Use Case"),": Monitor instrumentation coverage and overhead")),(0,n.yg)("h3",{id:"configuration-guide"},"Configuration Guide"),(0,n.yg)("h4",{id:"master-controls"},"Master Controls"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},"graphQL:\n  metrics:\n    # Master switch for all GraphQL metrics\n    enabled: ${GRAPHQL_METRICS_ENABLED:true}\n\n    # Enable field-level resolver metrics\n    fieldLevelEnabled: ${GRAPHQL_METRICS_FIELD_LEVEL_ENABLED:false}\n")),(0,n.yg)("h4",{id:"selective-field-instrumentation"},"Selective Field Instrumentation"),(0,n.yg)("p",null,"Field-level metrics can add significant overhead for complex queries. DataHub provides multiple strategies to control which fields are instrumented:"),(0,n.yg)("h5",{id:"1-operation-based-filtering"},"1. ",(0,n.yg)("strong",{parentName:"h5"},"Operation-Based Filtering")),(0,n.yg)("p",null,"Target specific GraphQL operations known to be slow or critical:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'fieldLevelOperations: "getSearchResultsForMultiple,searchAcrossLineageStructure"\n')),(0,n.yg)("h5",{id:"2-path-based-filtering"},"2. ",(0,n.yg)("strong",{parentName:"h5"},"Path-Based Filtering")),(0,n.yg)("p",null,"Use path patterns to instrument specific parts of your schema:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'fieldLevelPaths: "/search/results/**,/user/*/permissions,/**/lineage/*"\n')),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Path Pattern Syntax"),":"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"/user")," - Exact match for the user field"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"/user/*")," - Direct children of user (e.g., ",(0,n.yg)("inlineCode",{parentName:"li"},"/user/name"),", ",(0,n.yg)("inlineCode",{parentName:"li"},"/user/email"),")"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"/user/**")," - User field and all descendants at any depth"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"/*/comments/*")," - Comments field under any parent")),(0,n.yg)("h5",{id:"3-combined-filtering"},"3. ",(0,n.yg)("strong",{parentName:"h5"},"Combined Filtering")),(0,n.yg)("p",null,"When both operation and path filters are configured, only fields matching BOTH criteria are instrumented:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'# Only instrument search results within specific operations\nfieldLevelOperations: "searchAcrossEntities"\nfieldLevelPaths: "/searchResults/**"\n')),(0,n.yg)("h4",{id:"advanced-options"},"Advanced Options"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},"# Include field paths as metric tags (WARNING: high cardinality risk)\nfieldLevelPathEnabled: false\n\n# Include metrics for trivial property access\ntrivialDataFetchersEnabled: false\n")),(0,n.yg)("h3",{id:"filtering-modes-explained"},"Filtering Modes Explained"),(0,n.yg)("p",null,"The instrumentation automatically determines the most efficient filtering mode:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"DISABLED"),": Field-level metrics completely disabled"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"ALL_FIELDS"),": No filtering, all fields instrumented (highest overhead)"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"BY_OPERATION"),": Only instrument fields within specified operations"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"BY_PATH"),": Only instrument fields matching path patterns"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"BY_BOTH"),": Most restrictive - both operation and path must match")),(0,n.yg)("h3",{id:"performance-considerations"},"Performance Considerations"),(0,n.yg)("h4",{id:"impact-assessment"},"Impact Assessment"),(0,n.yg)("p",null,"Field-level instrumentation overhead varies by:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Query complexity"),": More fields = more overhead"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Resolver performance"),": Fast resolvers have higher relative overhead"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Filtering effectiveness"),": Better targeting = less overhead")),(0,n.yg)("h4",{id:"best-practices"},"Best Practices"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Start Conservative"),": Begin with field-level metrics disabled"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},"fieldLevelEnabled: false\n"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Target Known Issues"),": Enable selectively for problematic operations"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'fieldLevelEnabled: true\nfieldLevelOperations: "slowSearchQuery,complexLineageQuery"\n'))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Use Path Patterns Wisely"),": Focus on expensive resolver paths"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'fieldLevelPaths: "/search/**,/**/lineage/**"\n'))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Avoid Path Tags in Production"),": High cardinality risk"),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},"fieldLevelPathEnabled: false # Keep this false\n"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Monitor Instrumentation Overhead"),": Track the ",(0,n.yg)("inlineCode",{parentName:"p"},"graphql.fields.instrumented")," metric"))),(0,n.yg)("h3",{id:"example-configurations"},"Example Configurations"),(0,n.yg)("h4",{id:"development-environment-full-visibility"},"Development Environment (Full Visibility)"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'graphQL:\n  metrics:\n    enabled: true\n    fieldLevelEnabled: true\n    fieldLevelOperations: "" # All operations\n    fieldLevelPathEnabled: true # Include paths for debugging\n    trivialDataFetchersEnabled: true\n')),(0,n.yg)("h4",{id:"production---targeted-monitoring"},"Production - Targeted Monitoring"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'graphQL:\n  metrics:\n    enabled: true\n    fieldLevelEnabled: true\n    fieldLevelOperations: "getSearchResultsForMultiple,searchAcrossLineage"\n    fieldLevelPaths: "/search/results/*,/lineage/upstream/**,/lineage/downstream/**"\n    fieldLevelPathEnabled: false\n    trivialDataFetchersEnabled: false\n')),(0,n.yg)("h4",{id:"production---minimal-overhead"},"Production - Minimal Overhead"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},"graphQL:\n  metrics:\n    enabled: true\n    fieldLevelEnabled: false # Only request-level metrics\n")),(0,n.yg)("h3",{id:"debugging-slow-queries"},"Debugging Slow Queries"),(0,n.yg)("p",null,"When investigating GraphQL performance issues:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Enable request-level metrics first")," to identify slow operations"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Temporarily enable field-level metrics")," for the slow operation:",(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'fieldLevelOperations: "problematicQuery"\n'))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Analyze field duration metrics")," to find bottlenecks"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Optionally enable path tags")," (briefly) for precise identification:",(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},"fieldLevelPathEnabled: true # Temporary only!\n"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Optimize identified resolvers")," and disable detailed instrumentation")),(0,n.yg)("h3",{id:"integration-with-monitoring-stack"},"Integration with Monitoring Stack"),(0,n.yg)("p",null,"The GraphQL metrics integrate seamlessly with DataHub's monitoring infrastructure:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Prometheus"),": Metrics exposed at ",(0,n.yg)("inlineCode",{parentName:"li"},"/actuator/prometheus")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Grafana"),": Create dashboards showing:",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Request rates and latencies by operation"),(0,n.yg)("li",{parentName:"ul"},"Error rates and types"),(0,n.yg)("li",{parentName:"ul"},"Field resolver performance heatmaps"),(0,n.yg)("li",{parentName:"ul"},"Top slow operations and fields")))),(0,n.yg)("p",null,"Example Prometheus queries:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-promql"},"# Average request duration by operation\nrate(graphql_request_duration_seconds_sum[5m])\n/ rate(graphql_request_duration_seconds_count[5m])\n\n# Field resolver p99 latency\nhistogram_quantile(0.99,\n  rate(graphql_field_duration_seconds_bucket[5m])\n)\n\n# Error rate by operation\nrate(graphql_request_errors_total[5m])\n")),(0,n.yg)("h2",{id:"kafka-consumer-instrumentation-micrometer"},"Kafka Consumer Instrumentation (Micrometer)"),(0,n.yg)("h3",{id:"overview-2"},"Overview"),(0,n.yg)("p",null,"DataHub provides comprehensive instrumentation for Kafka message consumption through Micrometer metrics, enabling\nreal-time monitoring of message queue latency and consumer performance. This instrumentation is critical for\nmaintaining data freshness SLAs and identifying processing bottlenecks across DataHub's event-driven architecture."),(0,n.yg)("h3",{id:"why-kafka-queue-time-monitoring-matters"},"Why Kafka Queue Time Monitoring Matters"),(0,n.yg)("p",null,'Traditional Kafka lag monitoring only tells you "we\'re behind by 10,000 messages"\nWithout queue time metrics, you can\'t answer critical questions like "are we meeting our 5-minute data freshness SLA?"\nor "which consumer groups are experiencing delays?"'),(0,n.yg)("h4",{id:"real-world-impact"},"Real-World Impact"),(0,n.yg)("p",null,"Consider these scenarios:"),(0,n.yg)("p",null,"Variable Production Rate:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Morning: 100 messages/second \u2192 1000 message lag = 10 seconds old"),(0,n.yg)("li",{parentName:"ul"},"Evening: 10 messages/second \u2192 1000 message lag = 100 seconds old"),(0,n.yg)("li",{parentName:"ul"},"Same lag count, vastly different business impact!")),(0,n.yg)("p",null,"Burst Traffic Patterns:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Bulk ingestion creates 1M message backlog"),(0,n.yg)("li",{parentName:"ul"},"Are these messages from the last hour (recoverable) or last 24 hours (SLA breach)?")),(0,n.yg)("p",null,"Consumer Group Performance:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Real-time processors need < 1 minute latency"),(0,n.yg)("li",{parentName:"ul"},"Analytics consumers can tolerate 1 hour latency"),(0,n.yg)("li",{parentName:"ul"},"Different groups require different monitoring thresholds")),(0,n.yg)("h3",{id:"architecture-1"},"Architecture"),(0,n.yg)("p",null,"Kafka queue time instrumentation is implemented across all DataHub consumers:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"MetadataChangeProposals (MCP) Processor - SQL entity updates",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"BatchMetadataChangeProposals (MCP) Processor - Bulk SQL entity updates"))),(0,n.yg)("li",{parentName:"ul"},"MetadataChangeLog (MCL) Processor & Hooks - Elasticsearch & downstream aspect operations"),(0,n.yg)("li",{parentName:"ul"},"DataHubUsageEventsProcessor - Usage analytics events"),(0,n.yg)("li",{parentName:"ul"},"PlatformEventProcessor - Platform operations & external consumers")),(0,n.yg)("p",null,"Each consumer automatically records queue time metrics using the message's embedded timestamp."),(0,n.yg)("h3",{id:"metrics-collected-1"},"Metrics Collected"),(0,n.yg)("h4",{id:"core-metric"},"Core Metric"),(0,n.yg)("p",null,"Metric: ",(0,n.yg)("inlineCode",{parentName:"p"},"kafka.message.queue.time")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Type: Timer with configurable percentiles and SLO buckets"),(0,n.yg)("li",{parentName:"ul"},"Unit: Milliseconds"),(0,n.yg)("li",{parentName:"ul"},"Tags:",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},'topic: Kafka topic name (e.g., "MetadataChangeProposal_v1")'),(0,n.yg)("li",{parentName:"ul"},'consumer.group: Consumer group ID (e.g., "generic-mce-consumer")'))),(0,n.yg)("li",{parentName:"ul"},"Use Case: Monitor end-to-end latency from message production to SQL transaction")),(0,n.yg)("h4",{id:"statistical-distribution"},"Statistical Distribution"),(0,n.yg)("p",null,"The timer automatically tracks:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Count: Total messages processed"),(0,n.yg)("li",{parentName:"ul"},"Sum: Cumulative queue time"),(0,n.yg)("li",{parentName:"ul"},"Max: Highest queue time observed"),(0,n.yg)("li",{parentName:"ul"},"Percentiles: p50, p95, p99, p99.9 (configurable)"),(0,n.yg)("li",{parentName:"ul"},"SLO Buckets: Percentage of messages meeting latency targets")),(0,n.yg)("h4",{id:"configuration-guide-1"},"Configuration Guide"),(0,n.yg)("p",null,"Default Configuration:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'kafka:\n  consumer:\n    metrics:\n      # Percentiles to calculate\n      percentiles: "0.5,0.95,0.99,0.999"\n\n      # Service Level Objective buckets (seconds)\n      slo: "300,1800,3600,10800,21600,43200" # 5m,30m,1h,3h,6h,12h\n\n      # Maximum expected queue time\n      maxExpectedValue: 86400 # 24 hours (seconds)\n')),(0,n.yg)("h4",{id:"key-monitoring-patterns"},"Key Monitoring Patterns"),(0,n.yg)("p",null,"SLA Compliance Monitoring:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-promql"},'# Percentage of messages processed within 5-minute SLA\nsum(rate(kafka_message_queue_time_seconds_bucket{le="300"}[5m])) by (topic)\n/ sum(rate(kafka_message_queue_time_seconds_count[5m])) by (topic) * 100\n')),(0,n.yg)("p",null,"Consumer Group Comparison:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-promql"},"# P99 queue time by consumer group\nhistogram_quantile(0.99,\n  sum by (consumer_group, le) (\n    rate(kafka_message_queue_time_seconds_bucket[5m])\n  )\n)\n")),(0,n.yg)("h4",{id:"performance-considerations-1"},"Performance Considerations"),(0,n.yg)("p",null,"Metric Cardinality:"),(0,n.yg)("p",null,"The instrumentation is designed for low cardinality:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Only two tags: ",(0,n.yg)("inlineCode",{parentName:"li"},"topic")," and ",(0,n.yg)("inlineCode",{parentName:"li"},"consumer.group")),(0,n.yg)("li",{parentName:"ul"},"No partition-level tags (avoiding explosion with high partition counts)"),(0,n.yg)("li",{parentName:"ul"},"No message-specific tags")),(0,n.yg)("p",null,"Overhead Assessment:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"CPU Impact: Minimal - single timestamp calculation per message"),(0,n.yg)("li",{parentName:"ul"},"Memory Impact: ~5KB per topic/consumer-group combination"),(0,n.yg)("li",{parentName:"ul"},"Network Impact: Negligible - metrics aggregated before export")),(0,n.yg)("h4",{id:"migration-from-legacy-metrics"},"Migration from Legacy Metrics"),(0,n.yg)("p",null,"The new Micrometer-based queue time metrics coexist with the legacy DropWizard ",(0,n.yg)("inlineCode",{parentName:"p"},"kafkaLag")," histogram:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Legacy: ",(0,n.yg)("inlineCode",{parentName:"li"},"kafkaLag")," histogram via JMX"),(0,n.yg)("li",{parentName:"ul"},"New: ",(0,n.yg)("inlineCode",{parentName:"li"},"kafka.message.queue.time")," timer via Micrometer"),(0,n.yg)("li",{parentName:"ul"},"Migration: Both metrics collected during transition period"),(0,n.yg)("li",{parentName:"ul"},"Future: Legacy metrics will be deprecated in favor of Micrometer")),(0,n.yg)("p",null,"The new metrics provide:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Better percentile accuracy"),(0,n.yg)("li",{parentName:"ul"},"SLO bucket tracking"),(0,n.yg)("li",{parentName:"ul"},"Multi-backend support"),(0,n.yg)("li",{parentName:"ul"},"Dimensional tagging")),(0,n.yg)("h2",{id:"datahub-request-hook-latency-instrumentation-micrometer"},"DataHub Request Hook Latency Instrumentation (Micrometer)"),(0,n.yg)("h3",{id:"overview-3"},"Overview"),(0,n.yg)("p",null,"DataHub provides comprehensive instrumentation for measuring the latency from initial request submission to post-MCL\n(Metadata Change Log) hook execution. This metric is crucial for understanding the end-to-end processing time of metadata\nchanges, including both the time spent in Kafka queues and the time taken to process through the system to the final hooks."),(0,n.yg)("h3",{id:"why-hook-latency-monitoring-matters"},"Why Hook Latency Monitoring Matters"),(0,n.yg)("p",null,"Traditional metrics only show individual component performance. Request hook latency provides the complete picture of how long\nit takes for a metadata change to be fully processed through DataHub's pipeline:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Request Submission: When a metadata change request is initially submitted"),(0,n.yg)("li",{parentName:"ul"},"Queue Time: Time spent in Kafka topics waiting to be consumed"),(0,n.yg)("li",{parentName:"ul"},"Processing Time: Time for the change to be persisted and processed"),(0,n.yg)("li",{parentName:"ul"},"Hook Execution: Final execution of MCL hooks")),(0,n.yg)("p",null,"This end-to-end view is essential for:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Meeting data freshness SLAs"),(0,n.yg)("li",{parentName:"ul"},"Identifying bottlenecks in the metadata pipeline"),(0,n.yg)("li",{parentName:"ul"},"Understanding the impact of system load on processing times"),(0,n.yg)("li",{parentName:"ul"},"Ensuring timely updates to downstream systems")),(0,n.yg)("h3",{id:"configuration"},"Configuration"),(0,n.yg)("p",null,"Hook latency metrics are configured separately from Kafka consumer metrics to allow fine-tuning based on your specific requirements:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'datahub:\n  metrics:\n    # Measures the time from request to post-MCL hook execution\n    hookLatency:\n      # Percentiles to calculate for latency distribution\n      percentiles: "0.5,0.95,0.99,0.999"\n\n      # Service Level Objective buckets (seconds)\n      # These define the latency targets you want to track\n      slo: "300,1800,3000,10800,21600,43200" # 5m, 30m, 1h, 3h, 6h, 12h\n\n      # Maximum expected latency (seconds)\n      # Values above this are considered outliers\n      maxExpectedValue: 86000 # 24 hours\n')),(0,n.yg)("h3",{id:"metrics-collected-2"},"Metrics Collected"),(0,n.yg)("h4",{id:"core-metric-1"},"Core Metric"),(0,n.yg)("p",null,"Metric: ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub.request.hook.queue.time")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Type: Timer with configurable percentiles and SLO buckets"),(0,n.yg)("li",{parentName:"ul"},"Unit: Milliseconds"),(0,n.yg)("li",{parentName:"ul"},"Tags:",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"hook"),': Name of the MCL hook being executed (e.g., "IngestionSchedulerHook", "SiblingsHook")'))),(0,n.yg)("li",{parentName:"ul"},"Use Case: Monitor the complete latency from request submission to hook exe")),(0,n.yg)("h4",{id:"key-monitoring-patterns-1"},"Key Monitoring Patterns"),(0,n.yg)("p",null,"SLA Compliance by Hook:"),(0,n.yg)("p",null,"Monitor which hooks are meeting their latency SLAs:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-promql"},'# Percentage of requests processed within 5-minute SLA per hook\nsum(rate(datahub_request_hook_queue_time_seconds_bucket{le="300"}[5m])) by (hook)\n/ sum(rate(datahub_request_hook_queue_time_seconds_count[5m])) by (hook) * 100\n')),(0,n.yg)("p",null,"Hook Performance Comparison:"),(0,n.yg)("p",null,"Identify which hooks have the highest latency:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-promql"},"# P99 latency by hook\nhistogram_quantile(0.99,\n  sum by (hook, le) (\n    rate(datahub_request_hook_queue_time_seconds_bucket[5m])\n  )\n)\n")),(0,n.yg)("p",null,"Latency Trends:"),(0,n.yg)("p",null,"Track how hook latency changes over time:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-promql"},"# Average hook latency trend\navg by (hook) (\n  rate(datahub_request_hook_queue_time_seconds_sum[5m])\n  / rate(datahub_request_hook_queue_time_seconds_count[5m])\n)\n")),(0,n.yg)("h4",{id:"implementation-details"},"Implementation Details"),(0,n.yg)("p",null,"The hook latency metric leverages the trace ID embedded in the system metadata of each request:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Trace ID Generation: Each request generates a unique trace ID with an embedded timestamp"),(0,n.yg)("li",{parentName:"ol"},"Propagation: The trace ID flows through the entire processing pipeline via system metadata"),(0,n.yg)("li",{parentName:"ol"},"Measurement: When an MCL hook executes, the metric calculates the time difference between the current time and the trace ID timestamp"),(0,n.yg)("li",{parentName:"ol"},"Recording: The latency is recorded as a timer metric with the hook name as a tag")),(0,n.yg)("h4",{id:"performance-considerations-2"},"Performance Considerations"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Overhead: Minimal - only requires trace ID extraction and time calculation per hook execution"),(0,n.yg)("li",{parentName:"ul"},"Cardinality: Low - only one tag (hook name) with typically < 20 unique values"),(0,n.yg)("li",{parentName:"ul"},"Accuracy: High - measures actual wall-clock time from request to hook execution")),(0,n.yg)("h4",{id:"relationship-to-kafka-queue-time-metrics"},"Relationship to Kafka Queue Time Metrics"),(0,n.yg)("p",null,"While Kafka queue time metrics (",(0,n.yg)("inlineCode",{parentName:"p"},"kafka.message.queue.time"),") measure the time messages spend in Kafka topics, request hook\nlatency metrics provide the complete picture:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Kafka Queue Time: Time from message production to consumption"),(0,n.yg)("li",{parentName:"ul"},"Hook Latency: Time from initial request to final hook execution")),(0,n.yg)("p",null,"Together, these metrics help identify where delays occur:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"High Kafka queue time but low hook latency: Bottleneck in Kafka consumption"),(0,n.yg)("li",{parentName:"ul"},"Low Kafka queue time but high hook latency: Bottleneck in processing or persistence"),(0,n.yg)("li",{parentName:"ul"},"Both high: System-wide performance issues")),(0,n.yg)("h2",{id:"cache-monitoring-micrometer"},"Cache Monitoring (Micrometer)"),(0,n.yg)("h3",{id:"overview-4"},"Overview"),(0,n.yg)("p",null,"Micrometer provides automatic instrumentation for cache implementations, offering deep insights into cache performance\nand efficiency. This instrumentation is crucial for DataHub, where caching significantly impacts query performance and system load."),(0,n.yg)("h3",{id:"automatic-cache-metrics"},"Automatic Cache Metrics"),(0,n.yg)("p",null,"When caches are registered with Micrometer, comprehensive metrics are automatically collected without code changes:"),(0,n.yg)("h4",{id:"core-metrics"},"Core Metrics"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"strong"},"cache.size"))," (Gauge) - Current number of entries in the cache"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"strong"},"cache.gets"))," (Counter) - Cache access attempts, tagged with:",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"result=hit")," - Successful cache hits"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"result=miss")," - Cache misses requiring backend fetch"))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"strong"},"cache.puts"))," (Counter) - Number of entries added to cache"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"strong"},"cache.evictions"))," (Counter) - Number of entries evicted"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"strong"},"cache.eviction.weight"))," (Counter) - Total weight of evicted entries (for size-based eviction)")),(0,n.yg)("h4",{id:"derived-metrics"},"Derived Metrics"),(0,n.yg)("p",null,"Calculate key performance indicators using Prometheus queries:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-promql"},'# Cache hit rate (should be >80% for hot caches)\nsum(rate(cache_gets_total{result="hit"}[5m])) by (cache) /\nsum(rate(cache_gets_total[5m])) by (cache)\n\n# Cache miss rate\n1 - (cache_hit_rate)\n\n# Eviction rate (indicates cache pressure)\nrate(cache_evictions_total[5m])\n')),(0,n.yg)("h3",{id:"datahub-cache-configuration"},"DataHub Cache Configuration"),(0,n.yg)("p",null,"DataHub uses multiple cache layers, each automatically instrumented:"),(0,n.yg)("h4",{id:"1-entity-client-cache"},"1. Entity Client Cache"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},"cache.client.entityClient:\n  enabled: true\n  maxBytes: 104857600 # 100MB\n  entityAspectTTLSeconds:\n    corpuser:\n      corpUserInfo: 20 # Short TTL for frequently changing data\n      corpUserKey: 300 # Longer TTL for stable data\n    structuredProperty:\n      propertyDefinition: 300\n      structuredPropertyKey: 86400 # 1 day for very stable data\n")),(0,n.yg)("h4",{id:"2-usage-statistics-cache"},"2. Usage Statistics Cache"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},"cache.client.usageClient:\n  enabled: true\n  maxBytes: 52428800 # 50MB\n  defaultTTLSeconds: 86400 # 1 day\n  # Caches expensive usage calculations\n")),(0,n.yg)("h4",{id:"3-search--lineage-cache"},"3. Search & Lineage Cache"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},"cache.search.lineage:\n  ttlSeconds: 86400 # 1 day\n")),(0,n.yg)("h3",{id:"monitoring-best-practices"},"Monitoring Best Practices"),(0,n.yg)("h4",{id:"key-indicators-to-watch"},"Key Indicators to Watch"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Hit Rate by Cache Type")),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-promql"},"# Alert if hit rate drops below 70%\ncache_hit_rate < 0.7\n"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Memory Pressure")),(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre",className:"language-promql"},"# High eviction rate relative to puts\nrate(cache_evictions_total[5m]) / rate(cache_puts_total[5m]) > 0.1\n")))),(0,n.yg)("h2",{id:"thread-pool-executor-monitoring-micrometer"},"Thread Pool Executor Monitoring (Micrometer)"),(0,n.yg)("h3",{id:"overview-5"},"Overview"),(0,n.yg)("p",null,"Micrometer automatically instruments Java ",(0,n.yg)("inlineCode",{parentName:"p"},"ThreadPoolExecutor")," instances, providing crucial visibility into concurrency\nbottlenecks and resource utilization. For DataHub's concurrent operations, this monitoring is essential for maintaining\nperformance under load."),(0,n.yg)("h3",{id:"automatic-executor-metrics"},"Automatic Executor Metrics"),(0,n.yg)("h4",{id:"pool-state-metrics"},"Pool State Metrics"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"strong"},"executor.pool.size"))," (Gauge) - Current number of threads in pool"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"strong"},"executor.pool.core"))," (Gauge) - Core (minimum) pool size"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"strong"},"executor.pool.max"))," (Gauge) - Maximum allowed pool size"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"strong"},"executor.active"))," (Gauge) - Threads actively executing tasks")),(0,n.yg)("h4",{id:"queue-metrics"},"Queue Metrics"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"strong"},"executor.queued"))," (Gauge) - Tasks waiting in queue"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"strong"},"executor.queue.remaining"))," (Gauge) - Available queue capacity")),(0,n.yg)("h4",{id:"performance-metrics"},"Performance Metrics"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"strong"},"executor.completed"))," (Counter) - Total completed tasks"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"strong"},"executor.seconds"))," (Timer) - Task execution time distribution"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"strong"},"executor.rejected"))," (Counter) - Tasks rejected due to saturation")),(0,n.yg)("h3",{id:"datahub-executor-configurations"},"DataHub Executor Configurations"),(0,n.yg)("h4",{id:"1-graphql-query-executor"},"1. GraphQL Query Executor"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},"graphQL.concurrency:\n  separateThreadPool: true\n  corePoolSize: 20 # Base threads\n  maxPoolSize: 200 # Scale under load\n  keepAlive: 60 # Seconds before idle thread removal\n  # Handles complex GraphQL query resolution\n")),(0,n.yg)("h4",{id:"2-batch-processing-executors"},"2. Batch Processing Executors"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},"entityClient.restli:\n  get:\n    batchConcurrency: 2 # Parallel batch processors\n    batchQueueSize: 500 # Task buffer\n    batchThreadKeepAlive: 60\n  ingest:\n    batchConcurrency: 2\n    batchQueueSize: 500\n")),(0,n.yg)("h4",{id:"3-search--analytics-executors"},"3. Search & Analytics Executors"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},"timeseriesAspectService.query:\n  concurrency: 10 # Parallel query threads\n  queueSize: 500 # Buffered queries\n")),(0,n.yg)("h3",{id:"critical-monitoring-patterns"},"Critical Monitoring Patterns"),(0,n.yg)("h4",{id:"saturation-detection"},"Saturation Detection"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-promql"},"# Thread pool utilization (>0.8 indicates pressure)\nexecutor_active / executor_pool_size > 0.8\n\n# Queue filling up (>0.7 indicates backpressure)\nexecutor_queued / (executor_queued + executor_queue_remaining) > 0.7\n")),(0,n.yg)("h4",{id:"rejection--starvation"},"Rejection & Starvation"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-promql"},"# Task rejections (should be zero)\nrate(executor_rejected_total[1m]) > 0\n\n# Thread starvation (all threads busy for extended period)\navg_over_time(executor_active[5m]) >= executor_pool_core\n")),(0,n.yg)("h4",{id:"performance-analysis"},"Performance Analysis"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-promql"},"# Average task execution time\nrate(executor_seconds_sum[5m]) / rate(executor_seconds_count[5m])\n\n# Task throughput by executor\nrate(executor_completed_total[5m])\n")),(0,n.yg)("h3",{id:"tuning-guidelines"},"Tuning Guidelines"),(0,n.yg)("h4",{id:"symptoms--solutions"},"Symptoms & Solutions"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Symptom"),(0,n.yg)("th",{parentName:"tr",align:null},"Metric Pattern"),(0,n.yg)("th",{parentName:"tr",align:null},"Solution"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"High latency"),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"executor_queued")," rising"),(0,n.yg)("td",{parentName:"tr",align:null},"Increase pool size")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Rejections"),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"executor_rejected")," > 0"),(0,n.yg)("td",{parentName:"tr",align:null},"Increase queue size or pool max")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Memory pressure"),(0,n.yg)("td",{parentName:"tr",align:null},"Many idle threads"),(0,n.yg)("td",{parentName:"tr",align:null},"Reduce ",(0,n.yg)("inlineCode",{parentName:"td"},"keepAlive")," time")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"CPU waste"),(0,n.yg)("td",{parentName:"tr",align:null},"Low ",(0,n.yg)("inlineCode",{parentName:"td"},"executor_active")),(0,n.yg)("td",{parentName:"tr",align:null},"Reduce core pool size")))),(0,n.yg)("h4",{id:"capacity-planning"},"Capacity Planning"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Measure baseline"),": Monitor under normal load"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Stress test"),": Identify saturation points"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Set alerts"),":",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Warning at 70% utilization"),(0,n.yg)("li",{parentName:"ul"},"Critical at 90% utilization"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Auto-scale"),": Consider dynamic pool sizing based on queue depth")),(0,n.yg)("h2",{id:"distributed-tracing"},"Distributed Tracing"),(0,n.yg)("p",null,"Traces let us track the life of a request across multiple components. Each trace is consisted of multiple spans, which\nare units of work, containing various context about the work being done as well as time taken to finish the work. By\nlooking at the trace, we can more easily identify performance bottlenecks."),(0,n.yg)("p",null,"We enable tracing by using the ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/open-telemetry/opentelemetry-java-instrumentation"},"OpenTelemetry java instrumentation library"),".\nThis project provides a Java agent JAR that is attached to java applications. The agent injects bytecode to capture\ntelemetry from popular libraries."),(0,n.yg)("p",null,"Using the agent we are able to"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Plug and play different tracing tools based on the user's setup: Jaeger, Zipkin, or other tools"),(0,n.yg)("li",{parentName:"ol"},"Get traces for Kafka, JDBC, and Elasticsearch without any additional code"),(0,n.yg)("li",{parentName:"ol"},"Track traces of any function with a simple ",(0,n.yg)("inlineCode",{parentName:"li"},"@WithSpan")," annotation")),(0,n.yg)("p",null,"You can enable the agent by setting env variable ",(0,n.yg)("inlineCode",{parentName:"p"},"ENABLE_OTEL")," to ",(0,n.yg)("inlineCode",{parentName:"p"},"true")," for GMS and MAE/MCE consumers. In our\nexample ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/docker/monitoring/docker-compose.monitoring.yml"},"docker-compose"),", we export metrics to a local Jaeger\ninstance by setting env variable ",(0,n.yg)("inlineCode",{parentName:"p"},"OTEL_TRACES_EXPORTER")," to ",(0,n.yg)("inlineCode",{parentName:"p"},"jaeger"),"\nand ",(0,n.yg)("inlineCode",{parentName:"p"},"OTEL_EXPORTER_JAEGER_ENDPOINT")," to ",(0,n.yg)("inlineCode",{parentName:"p"},"http://jaeger-all-in-one:14250"),", but you can easily change this behavior by\nsetting the correct env variables. Refer to\nthis ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/open-telemetry/opentelemetry-java/blob/main/sdk-extensions/autoconfigure/README.md"},"doc")," for\nall configs."),(0,n.yg)("p",null,"Once the above is set up, you should be able to see a detailed trace as a request is sent to GMS. We added\nthe ",(0,n.yg)("inlineCode",{parentName:"p"},"@WithSpan")," annotation in various places to make the trace more readable. You should start to see traces in the\ntracing collector of choice. Our example ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/docker/monitoring/docker-compose.monitoring.yml"},"docker-compose")," deploys\nan instance of Jaeger with port 16686. The traces should be available at http://localhost:16686."),(0,n.yg)("h3",{id:"configuration-note"},"Configuration Note"),(0,n.yg)("p",null,"We recommend using either ",(0,n.yg)("inlineCode",{parentName:"p"},"grpc")," or ",(0,n.yg)("inlineCode",{parentName:"p"},"http/protobuf"),", configured using ",(0,n.yg)("inlineCode",{parentName:"p"},"OTEL_EXPORTER_OTLP_PROTOCOL"),". Avoid using ",(0,n.yg)("inlineCode",{parentName:"p"},"http")," will not work as expected due to the size of\nthe generated spans."),(0,n.yg)("h2",{id:"micrometer"},"Micrometer"),(0,n.yg)("p",null,"DataHub is transitioning to Micrometer as its primary metrics framework, representing a significant upgrade in observability\ncapabilities. Micrometer is a vendor-neutral application metrics facade that provides a simple, consistent API for the most\npopular monitoring systems, allowing you to instrument your JVM-based application code without vendor lock-in."),(0,n.yg)("h3",{id:"why-micrometer"},"Why Micrometer?"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Native Spring Integration"),(0,n.yg)("p",{parentName:"li"},"As DataHub uses Spring Boot, Micrometer provides seamless integration with:"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Auto-configuration of common metrics"),(0,n.yg)("li",{parentName:"ul"},"Built-in metrics for HTTP requests, JVM, caches, and more"),(0,n.yg)("li",{parentName:"ul"},"Spring Boot Actuator endpoints for metrics exposure"),(0,n.yg)("li",{parentName:"ul"},"Automatic instrumentation of Spring components"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Multi-Backend Support"),(0,n.yg)("p",{parentName:"li"},"Unlike the legacy DropWizard approach that primarily targets JMX, Micrometer natively supports:"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Prometheus (recommended for cloud-native deployments)"),(0,n.yg)("li",{parentName:"ul"},"JMX (for backward compatibility)"),(0,n.yg)("li",{parentName:"ul"},"StatsD"),(0,n.yg)("li",{parentName:"ul"},"CloudWatch"),(0,n.yg)("li",{parentName:"ul"},"Datadog"),(0,n.yg)("li",{parentName:"ul"},"New Relic"),(0,n.yg)("li",{parentName:"ul"},"And many more..."))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Dimensional Metrics"),(0,n.yg)("p",{parentName:"li"},"Micrometer embraces modern dimensional metrics with ",(0,n.yg)("strong",{parentName:"p"},"labels/tags"),", enabling:"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Rich querying and aggregation capabilities"),(0,n.yg)("li",{parentName:"ul"},"Better cardinality control"),(0,n.yg)("li",{parentName:"ul"},"More flexible dashboards and alerts"),(0,n.yg)("li",{parentName:"ul"},"Natural integration with cloud-native monitoring systems")))),(0,n.yg)("h2",{id:"micrometer-transition-plan"},"Micrometer Transition Plan"),(0,n.yg)("p",null,"DataHub is undertaking a strategic transition from DropWizard metrics (exposed via JMX) to Micrometer, a modern vendor-neutral metrics facade.\nThis transition aims to provide better cloud-native monitoring capabilities while maintaining backward compatibility for existing\nmonitoring infrastructure."),(0,n.yg)("h3",{id:"current-state"},"Current State"),(0,n.yg)("p",null,"What We Have Now:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Primary System: DropWizard metrics exposed through JMX"),(0,n.yg)("li",{parentName:"ul"},"Collection Method: Prometheus-JMX exporter scrapes JMX metrics"),(0,n.yg)("li",{parentName:"ul"},"Dashboards: Grafana dashboards consuming JMX-sourced metrics"),(0,n.yg)("li",{parentName:"ul"},"Code Pattern: MetricUtils class for creating counters and timers"),(0,n.yg)("li",{parentName:"ul"},"Integration: Basic Spring integration with manual metric creation")),(0,n.yg)("p",{align:"center"},(0,n.yg)("img",{width:"80%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/0f6ae5ae889ee4e780504ca566670867acf975ff/imgs/advanced/monitoring/monitoring_current.svg"})),(0,n.yg)("p",null,"Limitations:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"JMX-centric approach limits monitoring backend options"),(0,n.yg)("li",{parentName:"ul"},"No unified observability (separate instrumentation for metrics and traces)"),(0,n.yg)("li",{parentName:"ul"},"No support for dimensional metrics and tags"),(0,n.yg)("li",{parentName:"ul"},"Manual instrumentation required for most components"),(0,n.yg)("li",{parentName:"ul"},"Legacy naming conventions without proper tagging")),(0,n.yg)("h3",{id:"transition-state"},"Transition State"),(0,n.yg)("p",null,"What We're Building:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Primary System: Micrometer with native Prometheus support"),(0,n.yg)("li",{parentName:"ul"},"Collection Method: Direct Prometheus scraping via /actuator/prometheus"),(0,n.yg)("li",{parentName:"ul"},"Unified Telemetry: Single instrumentation point for both metrics and traces"),(0,n.yg)("li",{parentName:"ul"},"Modern Patterns: Dimensional metrics with rich tagging"),(0,n.yg)("li",{parentName:"ul"},"Multi-Backend: Support for Prometheus, StatsD, CloudWatch, Datadog, etc."),(0,n.yg)("li",{parentName:"ul"},"Auto-Instrumentation: Automatic metrics for Spring components")),(0,n.yg)("p",{align:"center"},(0,n.yg)("img",{width:"80%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/0f6ae5ae889ee4e780504ca566670867acf975ff/imgs/advanced/monitoring/monitoring_transition.svg"})),(0,n.yg)("p",null,"Key Decisions and Rationale:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Dual Registry Approach"),(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Decision:")," Run both systems in parallel with tag-based routing"),(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Rationale:")),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Zero downtime or disruption"),(0,n.yg)("li",{parentName:"ul"},"Gradual migration at component level"),(0,n.yg)("li",{parentName:"ul"},"Easy rollback if issues arise"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Prometheus as Primary Target"),(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Decision:")," Focus on Prometheus for new metrics"),(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Rationale:")),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Industry standard for cloud-native applications"),(0,n.yg)("li",{parentName:"ul"},"Rich query language and ecosystem"),(0,n.yg)("li",{parentName:"ul"},"Better suited for dimensional metrics"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Observation API Adoption"),(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Decision:")," Promote Observation API for new instrumentation"),(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Rationale:")),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Single instrumentation for metrics + traces"),(0,n.yg)("li",{parentName:"ul"},"Reduced code complexity"),(0,n.yg)("li",{parentName:"ul"},"Consistent naming across telemetry types")))),(0,n.yg)("h3",{id:"future-state"},"Future State"),(0,n.yg)("p",{align:"center"},(0,n.yg)("img",{width:"80%",src:"https://raw.githubusercontent.com/datahub-project/static-assets/0f6ae5ae889ee4e780504ca566670867acf975ff/imgs/advanced/monitoring/monitoring_future.svg"})),(0,n.yg)("p",null,'Once fully adopted, Micrometer will transform DataHub\'s observability from a collection of separate tools into a unified platform.\nThis means developers can focus on building features while getting comprehensive telemetry "for free."'),(0,n.yg)("p",null,"Intelligent and Adaptive Monitoring"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Dynamic Instrumentation: Enable detailed metrics for specific entities or operations on-demand without code changes"),(0,n.yg)("li",{parentName:"ul"},"Environment-Aware Metrics: Automatically route metrics to Prometheus in Kubernetes, CloudWatch in AWS, or Azure Monitor in Azure"),(0,n.yg)("li",{parentName:"ul"},"Built-in SLO Tracking: Define Service Level Objectives declaratively and automatically track error budgets")),(0,n.yg)("p",null,"Developer and Operator Experience"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Adding @Observed to a method automatically generates latency percentiles, error rates, and distributed trace spans"),(0,n.yg)("li",{parentName:"ul"},"Every service exposes golden signals (latency, traffic, errors, saturation) out-of-the-box"),(0,n.yg)("li",{parentName:"ul"},"Business metrics (entity ingestion rates, search performance) seamlessly correlate with system metrics"),(0,n.yg)("li",{parentName:"ul"},"Self-documenting telemetry where metrics, traces, and logs tell a coherent operational story")),(0,n.yg)("h2",{id:"dropwizard--jmx"},"DropWizard & JMX"),(0,n.yg)("p",null,"We originally decided to use ",(0,n.yg)("a",{parentName:"p",href:"https://metrics.dropwizard.io/4.2.0/"},"Dropwizard Metrics")," to export custom metrics to JMX,\nand then use ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/prometheus/jmx_exporter"},"Prometheus-JMX exporter")," to export all JMX metrics to\nPrometheus. This allows our code base to be independent of the metrics collection tool, making it easy for people to use\ntheir tool of choice. You can enable the agent by setting env variable ",(0,n.yg)("inlineCode",{parentName:"p"},"ENABLE_PROMETHEUS")," to ",(0,n.yg)("inlineCode",{parentName:"p"},"true")," for GMS and MAE/MCE\nconsumers. Refer to this example ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/docker/monitoring/docker-compose.monitoring.yml"},"docker-compose")," for setting the\nvariables."),(0,n.yg)("p",null,"In our example ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/docker/monitoring/docker-compose.monitoring.yml"},"docker-compose"),", we have configured prometheus to\nscrape from 4318 ports of each container used by the JMX exporter to export metrics. We also configured grafana to\nlisten to prometheus and create useful dashboards. By default, we provide two\ndashboards: ",(0,n.yg)("a",{parentName:"p",href:"https://grafana.com/grafana/dashboards/14845"},"JVM dashboard")," and DataHub dashboard."),(0,n.yg)("p",null,"In the JVM dashboard, you can find detailed charts based on JVM metrics like CPU/memory/disk usage. In the DataHub\ndashboard, you can find charts to monitor each endpoint and the kafka topics. Using the example implementation, go\nto http://localhost:3001 to find the grafana dashboards! (Username: admin, PW: admin)"),(0,n.yg)("p",null,"To make it easy to track various metrics within the code base, we created MetricUtils class. This util class creates a\ncentral metric registry, sets up the JMX reporter, and provides convenient functions for setting up counters and timers.\nYou can run the following to create a counter and increment."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'metricUtils.counter(this.getClass(),"metricName").increment();\n')),(0,n.yg)("p",null,"You can run the following to time a block of code."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'try(Timer.Context ignored=metricUtils.timer(this.getClass(),"timerName").timer()){\n    ...block of code\n    }\n')),(0,n.yg)("h4",{id:"enable-monitoring-through-docker-compose"},"Enable monitoring through docker-compose"),(0,n.yg)("p",null,"We provide some example configuration for enabling monitoring in\nthis ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/tree/master/docker/monitoring"},"directory"),". Take a look at the docker-compose\nfiles, which adds necessary env variables to existing containers, and spawns new containers (Jaeger, Prometheus,\nGrafana)."),(0,n.yg)("p",null,"You can add in the above docker-compose using the ",(0,n.yg)("inlineCode",{parentName:"p"},"-f <<path-to-compose-file>>")," when running docker-compose commands.\nFor instance,"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-shell"},"docker-compose \\\n  -f quickstart/docker-compose.quickstart.yml \\\n  -f monitoring/docker-compose.monitoring.yml \\\n  pull && \\\ndocker-compose -p datahub \\\n  -f quickstart/docker-compose.quickstart.yml \\\n  -f monitoring/docker-compose.monitoring.yml \\\n  up\n")),(0,n.yg)("p",null,"We set up quickstart.sh, dev.sh, and dev-without-neo4j.sh to add the above docker-compose when MONITORING=true. For\ninstance ",(0,n.yg)("inlineCode",{parentName:"p"},"MONITORING=true ./docker/quickstart.sh")," will add the correct env variables to start collecting traces and\nmetrics, and also deploy Jaeger, Prometheus, and Grafana. We will soon support this as a flag during quickstart."),(0,n.yg)("h2",{id:"health-check-endpoint"},"Health check endpoint"),(0,n.yg)("p",null,"For monitoring healthiness of your DataHub service, ",(0,n.yg)("inlineCode",{parentName:"p"},"/admin")," endpoint can be used."))}y.isMDXComponent=!0}}]);