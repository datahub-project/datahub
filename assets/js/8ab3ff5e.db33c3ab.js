"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[897],{15680:(e,n,t)=>{t.d(n,{xA:()=>s,yg:()=>c});var a=t(96540);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function o(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var d=a.createContext({}),g=function(e){var n=a.useContext(d),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},s=function(e){var n=g(e.components);return a.createElement(d.Provider,{value:n},e.children)},m="mdxType",p={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},u=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,i=e.originalType,d=e.parentName,s=o(e,["components","mdxType","originalType","parentName"]),m=g(t),u=r,c=m["".concat(d,".").concat(u)]||m[u]||p[u]||i;return t?a.createElement(c,l(l({ref:n},s),{},{components:t})):a.createElement(c,l({ref:n},s))}));function c(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=t.length,l=new Array(i);l[0]=u;var o={};for(var d in n)hasOwnProperty.call(n,d)&&(o[d]=n[d]);o.originalType=e,o[m]="string"==typeof e?e:r,l[1]=o;for(var g=2;g<i;g++)l[g]=t[g];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}u.displayName="MDXCreateElement"},73950:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>d,default:()=>c,frontMatter:()=>o,metadata:()=>g,toc:()=>m});t(96540);var a=t(15680);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){return n=null!=n?n:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):function(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))})),e}function l(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}const o={title:"Semantic Search Configuration Guide",slug:"/dev-guides/semantic-search/configuration",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/dev-guides/semantic-search/CONFIGURATION.md"},d="Semantic Search Configuration Guide",g={unversionedId:"docs/dev-guides/semantic-search/CONFIGURATION",id:"docs/dev-guides/semantic-search/CONFIGURATION",title:"Semantic Search Configuration Guide",description:"This guide covers all configuration options for DataHub's semantic search, including embedding models, index settings, and environment variables.",source:"@site/genDocs/docs/dev-guides/semantic-search/CONFIGURATION.md",sourceDirName:"docs/dev-guides/semantic-search",slug:"/dev-guides/semantic-search/configuration",permalink:"/docs/dev-guides/semantic-search/configuration",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/dev-guides/semantic-search/CONFIGURATION.md",tags:[],version:"current",frontMatter:{title:"Semantic Search Configuration Guide",slug:"/dev-guides/semantic-search/configuration",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/dev-guides/semantic-search/CONFIGURATION.md"},sidebar:"overviewSidebar",previous:{title:"Semantic Search Architecture",permalink:"/docs/dev-guides/semantic-search/architecture"},next:{title:"Switching Embedding Providers",permalink:"/docs/dev-guides/semantic-search/switching_providers"}},s={},m=[{value:"Enabling Semantic Search",id:"enabling-semantic-search",level:2},{value:"Environment Variables",id:"environment-variables",level:3},{value:"Application Configuration",id:"application-configuration",level:3},{value:"Index Model Mappings",id:"index-model-mappings",level:4},{value:"Embedding Provider Configuration",id:"embedding-provider-configuration",level:4},{value:"Embedding Models",id:"embedding-models",level:2},{value:"Understanding Embedding Providers",id:"understanding-embedding-providers",level:3},{value:"Ingestion via MCP (Metadata Change Proposal)",id:"ingestion-via-mcp-metadata-change-proposal",level:3},{value:"GMS Query Embedding Provider (Java)",id:"gms-query-embedding-provider-java",level:3},{value:"AWS Bedrock",id:"aws-bedrock",level:4},{value:"OpenAI (Default)",id:"openai-default",level:4},{value:"Cohere (Direct API)",id:"cohere-direct-api",level:4},{value:"Switching Between Providers",id:"switching-between-providers",level:4},{value:"Future Providers",id:"future-providers",level:4},{value:"Custom/Self-Hosted Providers",id:"customself-hosted-providers",level:4},{value:"Index Configuration",id:"index-configuration",level:2},{value:"Adding a Model to the Index",id:"adding-a-model-to-the-index",level:3},{value:"k-NN Settings",id:"k-nn-settings",level:3},{value:"Parameter Tuning",id:"parameter-tuning",level:3},{value:"Multiple Models (Advanced)",id:"multiple-models-advanced",level:3},{value:"Query Configuration",id:"query-configuration",level:2},{value:"GMS Query Settings",id:"gms-query-settings",level:3},{value:"Query Parameters",id:"query-parameters",level:3},{value:"Chunking Configuration",id:"chunking-configuration",level:2},{value:"Monitoring",id:"monitoring",level:2},{value:"Useful Queries",id:"useful-queries",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Debug Logging",id:"debug-logging",level:3}],p={toc:m},u="wrapper";function c(e){var{components:n}=e,t=l(e,["components"]);return(0,a.yg)(u,i(function(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{},a=Object.keys(t);"function"==typeof Object.getOwnPropertySymbols&&(a=a.concat(Object.getOwnPropertySymbols(t).filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable})))),a.forEach((function(n){r(e,n,t[n])}))}return e}({},p,t),{components:n,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"semantic-search-configuration-guide"},"Semantic Search Configuration Guide"),(0,a.yg)("p",null,"This guide covers all configuration options for DataHub's semantic search, including embedding models, index settings, and environment variables."),(0,a.yg)("h2",{id:"enabling-semantic-search"},"Enabling Semantic Search"),(0,a.yg)("h3",{id:"environment-variables"},"Environment Variables"),(0,a.yg)("p",null,"Set these in your deployment configuration (e.g., ",(0,a.yg)("inlineCode",{parentName:"p"},"docker/profiles/empty2.env"),"):"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"# Enable semantic search feature\nELASTICSEARCH_SEMANTIC_SEARCH_ENABLED=true\n\n# Entity types to enable (comma-separated)\nELASTICSEARCH_SEMANTIC_SEARCH_ENTITIES=document\n\n# Vector dimensions (must match embedding model)\nELASTICSEARCH_SEMANTIC_VECTOR_DIMENSION=3072\n")),(0,a.yg)("h3",{id:"application-configuration"},"Application Configuration"),(0,a.yg)("p",null,"In ",(0,a.yg)("inlineCode",{parentName:"p"},"metadata-service/configuration/src/main/resources/application.yaml"),", you need to configure:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Semantic search settings")," - Enable the feature and specify which entities support it"),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Index model mappings")," - Define the embedding model(s) and their vector dimensions"),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Embedding provider")," - Configure credentials for generating query embeddings")),(0,a.yg)("h4",{id:"index-model-mappings"},"Index Model Mappings"),(0,a.yg)("p",null,"The ",(0,a.yg)("inlineCode",{parentName:"p"},"models:")," section defines the structure of the semantic index. ",(0,a.yg)("strong",{parentName:"p"},"You must add a mapping for each embedding model you plan to use.")," The model key (e.g., ",(0,a.yg)("inlineCode",{parentName:"p"},"text_embedding_3_large"),") must match the key used when ingesting document embeddings."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-yaml"},"elasticsearch:\n  entityIndex:\n    semanticSearch:\n      enabled: ${ELASTICSEARCH_SEMANTIC_SEARCH_ENABLED:false}\n      enabledEntities: ${ELASTICSEARCH_SEMANTIC_SEARCH_ENTITIES:document}\n\n      # Define index mappings for each embedding model you use\n      models:\n        # Default: OpenAI text-embedding-3-large\n        text_embedding_3_large:\n          vectorDimension: 3072\n          knnEngine: faiss\n          spaceType: cosinesimil\n          efConstruction: 128\n          m: 16\n")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Common model configurations:")),(0,a.yg)("table",null,(0,a.yg)("thead",{parentName:"table"},(0,a.yg)("tr",{parentName:"thead"},(0,a.yg)("th",{parentName:"tr",align:null},"Model"),(0,a.yg)("th",{parentName:"tr",align:null},"Key"),(0,a.yg)("th",{parentName:"tr",align:null},"Dimensions"))),(0,a.yg)("tbody",{parentName:"table"},(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},"AWS Bedrock Cohere"),(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("inlineCode",{parentName:"td"},"cohere_embed_v3")),(0,a.yg)("td",{parentName:"tr",align:null},"1024")),(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},"OpenAI text-embedding-3-small"),(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("inlineCode",{parentName:"td"},"text_embedding_3_small")),(0,a.yg)("td",{parentName:"tr",align:null},"1536")),(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},"OpenAI text-embedding-3-large"),(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("inlineCode",{parentName:"td"},"text_embedding_3_large")),(0,a.yg)("td",{parentName:"tr",align:null},"3072")),(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},"Cohere embed-english-v3.0"),(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("inlineCode",{parentName:"td"},"embed_english_v3_0")),(0,a.yg)("td",{parentName:"tr",align:null},"1024")))),(0,a.yg)("blockquote",null,(0,a.yg)("p",{parentName:"blockquote"},(0,a.yg)("strong",{parentName:"p"},"Important:")," The model key is derived from the configured model ID using explicit mappings for known models, with a fallback that replaces dots, hyphens, and colons with underscores. For example:"),(0,a.yg)("ul",{parentName:"blockquote"},(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"cohere.embed-english-v3")," (AWS Bedrock, via ",(0,a.yg)("inlineCode",{parentName:"li"},"BEDROCK_EMBEDDING_MODEL"),") \u2192 ",(0,a.yg)("inlineCode",{parentName:"li"},"cohere_embed_v3")),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"embed-english-v3.0")," (Cohere direct, via ",(0,a.yg)("inlineCode",{parentName:"li"},"COHERE_EMBEDDING_MODEL"),") \u2192 ",(0,a.yg)("inlineCode",{parentName:"li"},"embed_english_v3_0")),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"text-embedding-3-small")," (OpenAI, via ",(0,a.yg)("inlineCode",{parentName:"li"},"OPENAI_EMBEDDING_MODEL"),") \u2192 ",(0,a.yg)("inlineCode",{parentName:"li"},"text_embedding_3_small"))),(0,a.yg)("p",{parentName:"blockquote"},"The explicit mappings ensure AWS Bedrock Cohere models (",(0,a.yg)("inlineCode",{parentName:"p"},"cohere.embed-english-v3"),") and Cohere direct API models (",(0,a.yg)("inlineCode",{parentName:"p"},"embed-english-v3.0"),") produce distinct keys despite similar names.")),(0,a.yg)("blockquote",null,(0,a.yg)("p",{parentName:"blockquote"},(0,a.yg)("strong",{parentName:"p"},"Matching with Ingestion:")," The model key in the index mapping must match the key used in the ",(0,a.yg)("inlineCode",{parentName:"p"},"semanticContent")," aspect when documents are ingested. The ingestion connector (e.g., ",(0,a.yg)("inlineCode",{parentName:"p"},"datahub-documents"),") uses the configured model to determine this key. For example, if your index has ",(0,a.yg)("inlineCode",{parentName:"p"},"text_embedding_3_small"),", the ingested ",(0,a.yg)("inlineCode",{parentName:"p"},"semanticContent")," aspect must have embeddings under ",(0,a.yg)("inlineCode",{parentName:"p"},"embeddings.text_embedding_3_small"),".")),(0,a.yg)("h4",{id:"embedding-provider-configuration"},"Embedding Provider Configuration"),(0,a.yg)("p",null,"Configure the provider used to generate query embeddings at search time:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-yaml"},'elasticsearch:\n  entityIndex:\n    semanticSearch:\n      # ... models section above ...\n\n      embeddingProvider:\n        type: ${EMBEDDING_PROVIDER_TYPE:openai}\n        maxCharacterLength: ${EMBEDDING_PROVIDER_MAX_CHAR_LENGTH:2048}\n        bedrock:\n          awsRegion: ${BEDROCK_EMBEDDING_AWS_REGION:us-west-2}\n          model: ${BEDROCK_EMBEDDING_MODEL:cohere.embed-english-v3}\n\n        # OpenAI configuration (used when type is "openai")\n        openai:\n          apiKey: ${OPENAI_API_KEY:}\n          model: ${OPENAI_EMBEDDING_MODEL:text-embedding-3-large}\n          endpoint: ${OPENAI_EMBEDDING_ENDPOINT:https://api.openai.com/v1/embeddings}\n\n        # Cohere configuration (used when type is "cohere")\n        cohere:\n          apiKey: ${COHERE_API_KEY:}\n          model: ${COHERE_EMBEDDING_MODEL:embed-english-v3.0}\n          endpoint: ${COHERE_EMBEDDING_ENDPOINT:https://api.cohere.ai/v1/embed}\n')),(0,a.yg)("h2",{id:"embedding-models"},"Embedding Models"),(0,a.yg)("h3",{id:"understanding-embedding-providers"},"Understanding Embedding Providers"),(0,a.yg)("p",null,"There are ",(0,a.yg)("strong",{parentName:"p"},"two separate embedding contexts")," in DataHub's semantic search:"),(0,a.yg)("table",null,(0,a.yg)("thead",{parentName:"table"},(0,a.yg)("tr",{parentName:"thead"},(0,a.yg)("th",{parentName:"tr",align:null},"Context"),(0,a.yg)("th",{parentName:"tr",align:null},"When"),(0,a.yg)("th",{parentName:"tr",align:null},"Who Generates"),(0,a.yg)("th",{parentName:"tr",align:null},"Configuration"))),(0,a.yg)("tbody",{parentName:"table"},(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("strong",{parentName:"td"},"Document Embeddings")),(0,a.yg)("td",{parentName:"tr",align:null},"At ingestion time"),(0,a.yg)("td",{parentName:"tr",align:null},"Ingestion Connector"),(0,a.yg)("td",{parentName:"tr",align:null},"Configured in connector")),(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("strong",{parentName:"td"},"Query Embeddings")),(0,a.yg)("td",{parentName:"tr",align:null},"At search time"),(0,a.yg)("td",{parentName:"tr",align:null},"GMS"),(0,a.yg)("td",{parentName:"tr",align:null},"Configured in GMS (below)")))),(0,a.yg)("blockquote",null,(0,a.yg)("p",{parentName:"blockquote"},(0,a.yg)("strong",{parentName:"p"},"Important:")," The GMS embedding provider (configured below) is used ",(0,a.yg)("strong",{parentName:"p"},"only for query embedding"),".\nDocument embeddings are generated by the ingestion connector using its own embedding configuration.\nBoth must use the ",(0,a.yg)("strong",{parentName:"p"},"same embedding model")," for semantic search to work correctly.")),(0,a.yg)("h3",{id:"ingestion-via-mcp-metadata-change-proposal"},"Ingestion via MCP (Metadata Change Proposal)"),(0,a.yg)("p",null,"Document embeddings are sent to DataHub via MCP using the ",(0,a.yg)("inlineCode",{parentName:"p"},"SemanticContent")," aspect. This is the standard DataHub pattern for ingesting metadata."),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"MCP Payload Structure:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "entityType": "document",\n  "entityUrn": "urn:li:document:my-doc-123",\n  "aspectName": "semanticContent",\n  "aspect": {\n    "value": "{...JSON encoded SemanticContent...}",\n    "contentType": "application/json"\n  }\n}\n')),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"SemanticContent Aspect:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "embeddings": {\n    "text_embedding_3_large": {\n      "modelVersion": "openai/text-embedding-3-large",\n      "generatedAt": 1702234567890,\n      "chunkingStrategy": "sentence_boundary_400t",\n      "totalChunks": 2,\n      "totalTokens": 450,\n      "chunks": [\n        {\n          "position": 0,\n          "vector": [0.123, -0.456, ...],\n          "characterOffset": 0,\n          "characterLength": 512,\n          "tokenCount": 230,\n          "text": "First chunk of text..."\n        }\n      ]\n    }\n  }\n}\n')),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Privacy Option:")," The ",(0,a.yg)("inlineCode",{parentName:"p"},"text")," field in each chunk is optional. For sensitive data sources, you can omit the source text and send only the embedding vectors."),(0,a.yg)("h3",{id:"gms-query-embedding-provider-java"},"GMS Query Embedding Provider (Java)"),(0,a.yg)("p",null,"GMS uses an ",(0,a.yg)("inlineCode",{parentName:"p"},"EmbeddingProvider")," implementation to generate query embeddings at search time."),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Interface:")," ",(0,a.yg)("inlineCode",{parentName:"p"},"com.linkedin.metadata.search.embedding.EmbeddingProvider")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-java"},"public interface EmbeddingProvider {\n  /**\n   * Returns an embedding vector for the given text.\n   * @param text The text to embed\n   * @param model The model identifier (nullable, uses default if null)\n   * @return The embedding vector\n   */\n  @Nonnull\n  float[] embed(@Nonnull String text, @Nullable String model);\n}\n")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Built-in Implementations:")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"OpenAiEmbeddingProvider")," - Uses OpenAI API (default)"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"AwsBedrockEmbeddingProvider")," - Uses AWS Bedrock"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"NoOpEmbeddingProvider")," - Throws exception if called (used when semantic search disabled)")),(0,a.yg)("p",null,"The following providers can be configured:"),(0,a.yg)("h4",{id:"aws-bedrock"},"AWS Bedrock"),(0,a.yg)("p",null,"AWS Bedrock provides managed access to embedding models:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"# Environment\nEMBED_PROVIDER=bedrock\nBEDROCK_EMBEDDING_MODEL=cohere.embed-english-v3\nAWS_REGION=us-west-2\n\n# For local development\nAWS_PROFILE=your-profile\n\n# For production (IAM role or explicit credentials)\nAWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\n")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Available Bedrock Models:")),(0,a.yg)("table",null,(0,a.yg)("thead",{parentName:"table"},(0,a.yg)("tr",{parentName:"thead"},(0,a.yg)("th",{parentName:"tr",align:null},"Model ID"),(0,a.yg)("th",{parentName:"tr",align:null},"Dimensions"),(0,a.yg)("th",{parentName:"tr",align:null},"Max Tokens"),(0,a.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,a.yg)("tbody",{parentName:"table"},(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("inlineCode",{parentName:"td"},"cohere.embed-english-v3")),(0,a.yg)("td",{parentName:"tr",align:null},"1024"),(0,a.yg)("td",{parentName:"tr",align:null},"512"),(0,a.yg)("td",{parentName:"tr",align:null},"Best for English")),(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("inlineCode",{parentName:"td"},"cohere.embed-multilingual-v3")),(0,a.yg)("td",{parentName:"tr",align:null},"1024"),(0,a.yg)("td",{parentName:"tr",align:null},"512"),(0,a.yg)("td",{parentName:"tr",align:null},"Multi-language support")),(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("inlineCode",{parentName:"td"},"amazon.titan-embed-text-v1")),(0,a.yg)("td",{parentName:"tr",align:null},"1536"),(0,a.yg)("td",{parentName:"tr",align:null},"8192"),(0,a.yg)("td",{parentName:"tr",align:null},"Amazon's model")),(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("inlineCode",{parentName:"td"},"amazon.titan-embed-text-v2:0")),(0,a.yg)("td",{parentName:"tr",align:null},"256/512/1024"),(0,a.yg)("td",{parentName:"tr",align:null},"8192"),(0,a.yg)("td",{parentName:"tr",align:null},"Configurable dimensions")))),(0,a.yg)("h4",{id:"openai-default"},"OpenAI (Default)"),(0,a.yg)("p",null,"OpenAI provides high-quality embedding models with simple API access:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"# Required\nEMBEDDING_PROVIDER_TYPE=openai\nOPENAI_API_KEY=sk-your-api-key-here\n\n# Optional - defaults shown\n# OPENAI_EMBEDDING_MODEL: Model used to generate query embeddings via OpenAI API\nOPENAI_EMBEDDING_MODEL=text-embedding-3-large\n")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Available OpenAI Models:")),(0,a.yg)("table",null,(0,a.yg)("thead",{parentName:"table"},(0,a.yg)("tr",{parentName:"thead"},(0,a.yg)("th",{parentName:"tr",align:null},"Model ID"),(0,a.yg)("th",{parentName:"tr",align:null},"Dimensions"),(0,a.yg)("th",{parentName:"tr",align:null},"Max Tokens"),(0,a.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,a.yg)("tbody",{parentName:"table"},(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("inlineCode",{parentName:"td"},"text-embedding-3-small")),(0,a.yg)("td",{parentName:"tr",align:null},"1536"),(0,a.yg)("td",{parentName:"tr",align:null},"8191"),(0,a.yg)("td",{parentName:"tr",align:null},"Fast, cost-effective")),(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("inlineCode",{parentName:"td"},"text-embedding-3-large")),(0,a.yg)("td",{parentName:"tr",align:null},"3072"),(0,a.yg)("td",{parentName:"tr",align:null},"8191"),(0,a.yg)("td",{parentName:"tr",align:null},"Higher quality")))),(0,a.yg)("h4",{id:"cohere-direct-api"},"Cohere (Direct API)"),(0,a.yg)("p",null,"Use Cohere's embedding API directly (without AWS Bedrock):"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"# Required\nEMBEDDING_PROVIDER_TYPE=cohere\nCOHERE_API_KEY=your-cohere-api-key\n\n# Optional - defaults shown\n# COHERE_EMBEDDING_MODEL: Model used to generate query embeddings via Cohere API\nCOHERE_EMBEDDING_MODEL=embed-english-v3.0\n")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Available Cohere Models:")),(0,a.yg)("table",null,(0,a.yg)("thead",{parentName:"table"},(0,a.yg)("tr",{parentName:"thead"},(0,a.yg)("th",{parentName:"tr",align:null},"Model ID"),(0,a.yg)("th",{parentName:"tr",align:null},"Dimensions"),(0,a.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,a.yg)("tbody",{parentName:"table"},(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("inlineCode",{parentName:"td"},"embed-english-v3.0")),(0,a.yg)("td",{parentName:"tr",align:null},"1024"),(0,a.yg)("td",{parentName:"tr",align:null},"English optimized")),(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("inlineCode",{parentName:"td"},"embed-multilingual-v3.0")),(0,a.yg)("td",{parentName:"tr",align:null},"1024"),(0,a.yg)("td",{parentName:"tr",align:null},"100+ languages")))),(0,a.yg)("h4",{id:"switching-between-providers"},"Switching Between Providers"),(0,a.yg)("p",null,"When switching embedding providers, you must delete and recreate the semantic index because different models produce vectors with different dimensions."),(0,a.yg)("p",null,"See ",(0,a.yg)("strong",{parentName:"p"},(0,a.yg)("a",{parentName:"strong",href:"/docs/dev-guides/semantic-search/switching_providers"},"SWITCHING_PROVIDERS.md"))," for detailed step-by-step instructions."),(0,a.yg)("h4",{id:"future-providers"},"Future Providers"),(0,a.yg)("p",null,"Potential future built-in providers:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Azure OpenAI"),(0,a.yg)("li",{parentName:"ul"},"Google Vertex AI")),(0,a.yg)("h4",{id:"customself-hosted-providers"},"Custom/Self-Hosted Providers"),(0,a.yg)("p",null,"To add a custom embedding provider, implement the ",(0,a.yg)("inlineCode",{parentName:"p"},"EmbeddingProvider")," interface and register it in the factory."),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"1. Implement the interface")," (",(0,a.yg)("inlineCode",{parentName:"p"},"metadata-io/src/main/java/com/linkedin/metadata/search/embedding/"),"):"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-java"},"package com.linkedin.metadata.search.embedding;\n\nimport javax.annotation.Nonnull;\nimport javax.annotation.Nullable;\n\npublic class CustomEmbeddingProvider implements EmbeddingProvider {\n\n  private final String endpoint;\n  private final int dimensions;\n\n  public CustomEmbeddingProvider(String endpoint, int dimensions) {\n    this.endpoint = endpoint;\n    this.dimensions = dimensions;\n  }\n\n  @Override\n  @Nonnull\n  public float[] embed(@Nonnull String text, @Nullable String model) {\n    // Call your embedding service\n    // Return float array with `dimensions` elements\n    return callEmbeddingService(endpoint, text, model);\n  }\n}\n")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"2. Register in the factory")," (",(0,a.yg)("inlineCode",{parentName:"p"},"metadata-service/factories/.../EmbeddingProviderFactory.java"),"):"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-java"},'@Bean(name = "embeddingProvider")\n@Nonnull\nprotected EmbeddingProvider getInstance() {\n  // ... existing code ...\n\n  String providerType = config.getType();\n\n  if ("aws-bedrock".equalsIgnoreCase(providerType)) {\n    return new AwsBedrockEmbeddingProvider(...);\n  } else if ("custom".equalsIgnoreCase(providerType)) {\n    return new CustomEmbeddingProvider(\n        config.getCustomEndpoint(),\n        config.getCustomDimensions()\n    );\n  } else {\n    throw new IllegalStateException("Unsupported provider: " + providerType);\n  }\n}\n')),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"3. Add configuration")," (",(0,a.yg)("inlineCode",{parentName:"p"},"EmbeddingProviderConfiguration.java"),"):"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-java"},'@Data\npublic class EmbeddingProviderConfiguration {\n  private String type = "aws-bedrock";\n  private String awsRegion = "us-west-2";\n  private String modelId = "cohere.embed-english-v3";\n\n  // Add custom provider fields\n  private String customEndpoint;\n  private int customDimensions = 1024;\n}\n')),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"4. Configure in application.yaml"),":"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-yaml"},"elasticsearch:\n  search:\n    semanticSearch:\n      embeddingProvider:\n        type: custom\n        customEndpoint: http://your-embedding-service:8080/embed\n        customDimensions: 1024\n")),(0,a.yg)("h2",{id:"index-configuration"},"Index Configuration"),(0,a.yg)("h3",{id:"adding-a-model-to-the-index"},"Adding a Model to the Index"),(0,a.yg)("p",null,"Each embedding model you use must have a corresponding entry in the ",(0,a.yg)("inlineCode",{parentName:"p"},"models:")," section of application.yaml. The index mapping is created when GMS starts, so you must configure this ",(0,a.yg)("strong",{parentName:"p"},"before")," ingesting documents."),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Example: Adding OpenAI text-embedding-3-small:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-yaml"},"models:\n  text_embedding_3_small: # Key derived from model name (dots/hyphens \u2192 underscores)\n    vectorDimension: 1536 # Must match the model's output dimensions\n    knnEngine: faiss\n    spaceType: cosinesimil\n    efConstruction: 128\n    m: 16\n")),(0,a.yg)("h3",{id:"k-nn-settings"},"k-NN Settings"),(0,a.yg)("p",null,"The semantic index uses OpenSearch's k-NN plugin. Key parameters:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-yaml"},"models:\n  your_model_key:\n    # Vector size (must match model output)\n    vectorDimension: 1024\n\n    # k-NN engine: faiss (recommended) or nmslib\n    knnEngine: faiss\n\n    # Similarity metric\n    # - cosinesimil: Cosine similarity (recommended for text)\n    # - l2: Euclidean distance\n    # - innerproduct: Dot product\n    spaceType: cosinesimil\n\n    # HNSW graph parameters\n    efConstruction: 128 # Build-time accuracy (32-512)\n    m: 16 # Connections per node (4-64)\n")),(0,a.yg)("h3",{id:"parameter-tuning"},"Parameter Tuning"),(0,a.yg)("table",null,(0,a.yg)("thead",{parentName:"table"},(0,a.yg)("tr",{parentName:"thead"},(0,a.yg)("th",{parentName:"tr",align:null},"Parameter"),(0,a.yg)("th",{parentName:"tr",align:null},"Low"),(0,a.yg)("th",{parentName:"tr",align:null},"Medium"),(0,a.yg)("th",{parentName:"tr",align:null},"High"),(0,a.yg)("th",{parentName:"tr",align:null},"Trade-off"))),(0,a.yg)("tbody",{parentName:"table"},(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("inlineCode",{parentName:"td"},"efConstruction")),(0,a.yg)("td",{parentName:"tr",align:null},"32"),(0,a.yg)("td",{parentName:"tr",align:null},"128"),(0,a.yg)("td",{parentName:"tr",align:null},"512"),(0,a.yg)("td",{parentName:"tr",align:null},"Speed vs accuracy at build time")),(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},(0,a.yg)("inlineCode",{parentName:"td"},"m")),(0,a.yg)("td",{parentName:"tr",align:null},"4"),(0,a.yg)("td",{parentName:"tr",align:null},"16"),(0,a.yg)("td",{parentName:"tr",align:null},"64"),(0,a.yg)("td",{parentName:"tr",align:null},"Memory vs accuracy")))),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Recommendations:")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Development"),": ",(0,a.yg)("inlineCode",{parentName:"li"},"efConstruction: 64, m: 8")),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Production"),": ",(0,a.yg)("inlineCode",{parentName:"li"},"efConstruction: 128, m: 16")),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"High Accuracy"),": ",(0,a.yg)("inlineCode",{parentName:"li"},"efConstruction: 256, m: 32"))),(0,a.yg)("h3",{id:"multiple-models-advanced"},"Multiple Models (Advanced)"),(0,a.yg)("p",null,"You can configure multiple embedding models in the index for migration scenarios or A/B testing. This allows you to have documents with embeddings from different models coexisting in the same index."),(0,a.yg)("blockquote",null,(0,a.yg)("p",{parentName:"blockquote"},(0,a.yg)("strong",{parentName:"p"},"Note:")," Most deployments only need a single model. Only add multiple models if you're migrating between providers or testing different models.")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-yaml"},"models:\n  # Current production model (OpenAI default)\n  text_embedding_3_large:\n    vectorDimension: 3072\n    knnEngine: faiss\n    spaceType: cosinesimil\n    efConstruction: 128\n    m: 16\n\n  # Alternative model being tested\n  cohere_embed_v3:\n    vectorDimension: 1024\n    knnEngine: faiss\n    spaceType: cosinesimil\n    efConstruction: 128\n    m: 16\n")),(0,a.yg)("p",null,"When using multiple models, the configured provider model (",(0,a.yg)("inlineCode",{parentName:"p"},"BEDROCK_EMBEDDING_MODEL"),", ",(0,a.yg)("inlineCode",{parentName:"p"},"OPENAI_EMBEDDING_MODEL"),", or ",(0,a.yg)("inlineCode",{parentName:"p"},"COHERE_EMBEDDING_MODEL"),") determines which model is used for query embeddings at search time."),(0,a.yg)("h2",{id:"query-configuration"},"Query Configuration"),(0,a.yg)("h3",{id:"gms-query-settings"},"GMS Query Settings"),(0,a.yg)("p",null,"GMS needs credentials to generate query embeddings at search time:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"# Mount AWS credentials in Docker\nvolumes:\n  - ${HOME}/.aws:/home/datahub/.aws:ro\n\n# Set profile\nAWS_PROFILE=your-profile\n")),(0,a.yg)("h3",{id:"query-parameters"},"Query Parameters"),(0,a.yg)("p",null,"In GraphQL queries:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-graphql"},"query SemanticSearch($input: SearchAcrossEntitiesInput!) {\n  semanticSearchAcrossEntities(input: $input) {\n    # ...\n  }\n}\n")),(0,a.yg)("p",null,"Variables:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "input": {\n    "query": "your natural language query",\n    "types": ["DOCUMENT"], // Entity types to search\n    "start": 0, // Pagination start\n    "count": 10 // Results per page\n  }\n}\n')),(0,a.yg)("h2",{id:"chunking-configuration"},"Chunking Configuration"),(0,a.yg)("p",null,"Chunking is handled by the ",(0,a.yg)("strong",{parentName:"p"},"ingestion connector")," when generating document embeddings. The typical strategy:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Chunk size"),": ~400 tokens per chunk"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Boundary"),": Split at sentence boundaries when possible"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Overlap"),": Optional overlap between chunks for context continuity")),(0,a.yg)("p",null,"The chunking parameters are configured in the ingestion connector, not in GMS. GMS receives pre-chunked embeddings from the connector."),(0,a.yg)("h2",{id:"monitoring"},"Monitoring"),(0,a.yg)("h3",{id:"useful-queries"},"Useful Queries"),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Check embedding coverage:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},'curl "http://localhost:9200/documentindex_v2_semantic/_search" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "size": 0,\n    "aggs": {\n      "with_embeddings": {\n        "filter": { "exists": { "field": "embeddings.text_embedding_3_large" } }\n      },\n      "without_embeddings": {\n        "filter": { "bool": { "must_not": { "exists": { "field": "embeddings.text_embedding_3_large" } } } }\n      }\n    }\n  }\'\n')),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Check index health:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},'curl "http://localhost:9200/_cat/indices/*semantic*?v"\n')),(0,a.yg)("h2",{id:"troubleshooting"},"Troubleshooting"),(0,a.yg)("h3",{id:"common-issues"},"Common Issues"),(0,a.yg)("table",null,(0,a.yg)("thead",{parentName:"table"},(0,a.yg)("tr",{parentName:"thead"},(0,a.yg)("th",{parentName:"tr",align:null},"Issue"),(0,a.yg)("th",{parentName:"tr",align:null},"Cause"),(0,a.yg)("th",{parentName:"tr",align:null},"Solution"))),(0,a.yg)("tbody",{parentName:"table"},(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},'"Unable to locate credentials"'),(0,a.yg)("td",{parentName:"tr",align:null},"AWS creds not available"),(0,a.yg)("td",{parentName:"tr",align:null},"Mount ",(0,a.yg)("inlineCode",{parentName:"td"},".aws")," to ",(0,a.yg)("inlineCode",{parentName:"td"},"/home/datahub/.aws"))),(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},'"Profile file contained no credentials"'),(0,a.yg)("td",{parentName:"tr",align:null},"SSO session expired"),(0,a.yg)("td",{parentName:"tr",align:null},"Run ",(0,a.yg)("inlineCode",{parentName:"td"},"aws sso login --profile your-profile"))),(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},"Empty search results"),(0,a.yg)("td",{parentName:"tr",align:null},"No embeddings in index"),(0,a.yg)("td",{parentName:"tr",align:null},"Verify ingestion connector is generating embeddings")),(0,a.yg)("tr",{parentName:"tbody"},(0,a.yg)("td",{parentName:"tr",align:null},"Wrong results"),(0,a.yg)("td",{parentName:"tr",align:null},"Model mismatch"),(0,a.yg)("td",{parentName:"tr",align:null},"Ensure ingestion connector and GMS use the same embedding model")))),(0,a.yg)("h3",{id:"debug-logging"},"Debug Logging"),(0,a.yg)("p",null,"Enable debug logging in GMS:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-yaml"},"logging:\n  level:\n    com.linkedin.metadata.search: DEBUG\n")),(0,a.yg)("p",null,"Check logs for:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},"[DEBUG-DUALWRITE] shouldWriteToSemanticIndex returned: true\nSemantic dual-write enabled=true, enabledEntities=[document]\n")))}c.isMDXComponent=!0}}]);