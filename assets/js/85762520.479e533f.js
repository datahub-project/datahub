"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[71740],{15680:(e,n,a)=>{a.d(n,{xA:()=>p,yg:()=>y});var t=a(96540);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function i(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function l(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?i(Object(a),!0).forEach((function(n){r(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function o(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},i=Object.keys(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=t.createContext({}),g=function(e){var n=t.useContext(s),a=n;return e&&(a="function"==typeof e?e(n):l(l({},n),e)),a},p=function(e){var n=g(e.components);return t.createElement(s.Provider,{value:n},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},c=t.forwardRef((function(e,n){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),d=g(a),c=r,y=d["".concat(s,".").concat(c)]||d[c]||u[c]||i;return a?t.createElement(y,l(l({ref:n},p),{},{components:a})):t.createElement(y,l({ref:n},p))}));function y(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=a.length,l=new Array(i);l[0]=c;var o={};for(var s in n)hasOwnProperty.call(n,s)&&(o[s]=n[s]);o.originalType=e,o[d]="string"==typeof e?e:r,l[1]=o;for(var g=2;g<i;g++)l[g]=a[g];return t.createElement.apply(null,l)}return t.createElement.apply(null,a)}c.displayName="MDXCreateElement"},87429:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>p,contentTitle:()=>s,default:()=>y,frontMatter:()=>o,metadata:()=>g,toc:()=>d});a(96540);var t=a(15680);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function i(e,n){return n=null!=n?n:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):function(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}(Object(n)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))})),e}function l(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},i=Object.keys(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}const o={title:"Ingestion Recording and Replay",slug:"/metadata-ingestion/src/datahub/ingestion/recording",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/ingestion/recording/README.md"},s="Ingestion Recording and Replay",g={unversionedId:"metadata-ingestion/src/datahub/ingestion/recording/README",id:"metadata-ingestion/src/datahub/ingestion/recording/README",title:"Ingestion Recording and Replay",description:"Beta Feature: Recording and replay is currently in beta. The feature is stable for debugging purposes but the archive format may change in future releases.",source:"@site/genDocs/metadata-ingestion/src/datahub/ingestion/recording/README.md",sourceDirName:"metadata-ingestion/src/datahub/ingestion/recording",slug:"/metadata-ingestion/src/datahub/ingestion/recording",permalink:"/docs/metadata-ingestion/src/datahub/ingestion/recording",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/ingestion/recording/README.md",tags:[],version:"current",frontMatter:{title:"Ingestion Recording and Replay",slug:"/metadata-ingestion/src/datahub/ingestion/recording",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/ingestion/recording/README.md"}},p={},d=[{value:"Overview",id:"overview",level:2},{value:"Comparing Recording and Replay Output",id:"comparing-recording-and-replay-output",level:3},{value:"Installation",id:"installation",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Recording an Ingestion Run",id:"recording-an-ingestion-run",level:3},{value:"Replaying a Recording",id:"replaying-a-recording",level:3},{value:"Inspecting Recordings",id:"inspecting-recordings",level:3},{value:"Configuration",id:"configuration",level:2},{value:"Recipe Configuration",id:"recipe-configuration",level:3},{value:"Environment Variables",id:"environment-variables",level:3},{value:"CLI Options",id:"cli-options",level:3},{value:"Archive Format",id:"archive-format",level:2},{value:"Manifest Contents",id:"manifest-contents",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Use Consistent Passwords",id:"1-use-consistent-passwords",level:3},{value:"2. Record in Production-Like Environments",id:"2-record-in-production-like-environments",level:3},{value:"3. Use Descriptive Run IDs",id:"3-use-descriptive-run-ids",level:3},{value:"4. Test Replay Immediately",id:"4-test-replay-immediately",level:3},{value:"5. Include Exception Context",id:"5-include-exception-context",level:3},{value:"6. Secure Archive Handling",id:"6-secure-archive-handling",level:3},{value:"7. Minimize Recording Scope",id:"7-minimize-recording-scope",level:3},{value:"Limitations",id:"limitations",level:2},{value:"1. Thread-Safe Recording Impact",id:"1-thread-safe-recording-impact",level:3},{value:"2. Timestamps Differ Between Runs",id:"2-timestamps-differ-between-runs",level:3},{value:"3. Non-Deterministic Source Behavior",id:"3-non-deterministic-source-behavior",level:3},{value:"4. Database Connection Mocking",id:"4-database-connection-mocking",level:3},{value:"5. Large Recordings",id:"5-large-recordings",level:3},{value:"6. Secret Handling",id:"6-secret-handling",level:3},{value:"7. HTTP-Only for Some Sources",id:"7-http-only-for-some-sources",level:3},{value:"8. Vendored HTTP Libraries (Snowflake, Databricks)",id:"8-vendored-http-libraries-snowflake-databricks",level:3},{value:"9. Stateful Ingestion",id:"9-stateful-ingestion",level:3},{value:"10. Memory Usage",id:"10-memory-usage",level:3},{value:"11. Lazy Imports",id:"11-lazy-imports",level:3},{value:"Supported Sources",id:"supported-sources",level:2},{value:"Fully Supported (HTTP-based)",id:"fully-supported-http-based",level:3},{value:"Database Sources",id:"database-sources",level:3},{value:"Hybrid Recording Strategy",id:"hybrid-recording-strategy",level:4},{value:"Database Connection Architecture",id:"database-connection-architecture",level:4},{value:"DataHub Backend",id:"datahub-backend",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"&quot;Module not found: vcrpy&quot;",id:"module-not-found-vcrpy",level:3},{value:"&quot;Checksum verification failed&quot;",id:"checksum-verification-failed",level:3},{value:"&quot;No match for request&quot; during replay",id:"no-match-for-request-during-replay",level:3},{value:"Replay produces different event count",id:"replay-produces-different-event-count",level:3},{value:"Recording takes too long",id:"recording-takes-too-long",level:3},{value:"Archive too large for S3 upload",id:"archive-too-large-for-s3-upload",level:3},{value:"Architecture",id:"architecture",level:2},{value:"Contributing",id:"contributing",level:2},{value:"See Also",id:"see-also",level:2}],u={toc:d},c="wrapper";function y(e){var{components:n}=e,a=l(e,["components"]);return(0,t.yg)(c,i(function(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{},t=Object.keys(a);"function"==typeof Object.getOwnPropertySymbols&&(t=t.concat(Object.getOwnPropertySymbols(a).filter((function(e){return Object.getOwnPropertyDescriptor(a,e).enumerable})))),t.forEach((function(n){r(e,n,a[n])}))}return e}({},u,a),{components:n,mdxType:"MDXLayout"}),(0,t.yg)("h1",{id:"ingestion-recording-and-replay"},"Ingestion Recording and Replay"),(0,t.yg)("blockquote",null,(0,t.yg)("p",{parentName:"blockquote"},(0,t.yg)("strong",{parentName:"p"},"Beta Feature"),": Recording and replay is currently in beta. The feature is stable for debugging purposes but the archive format may change in future releases.")),(0,t.yg)("p",null,"Debug ingestion issues by capturing all external I/O (HTTP requests and database queries) during ingestion runs, then replaying them locally in an air-gapped environment with full debugger support."),(0,t.yg)("h2",{id:"overview"},"Overview"),(0,t.yg)("p",null,"The recording system captures:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"HTTP Traffic"),": All requests to external APIs (Looker, PowerBI, Snowflake REST, etc.) and DataHub GMS"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Database Queries"),": SQL queries and results from native database connectors (Snowflake, Redshift, BigQuery, Databricks, etc.)")),(0,t.yg)("p",null,"Recordings are stored in encrypted, compressed archives that can be replayed offline to reproduce issues exactly as they occurred in production."),(0,t.yg)("h3",{id:"comparing-recording-and-replay-output"},"Comparing Recording and Replay Output"),(0,t.yg)("p",null,"The recorded and replayed MCPs are ",(0,t.yg)("strong",{parentName:"p"},"semantically identical")," - they contain the same source data. However, certain metadata fields will differ because they reflect ",(0,t.yg)("em",{parentName:"p"},"when")," MCPs are emitted, not the source data itself:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"systemMetadata.lastObserved")," - timestamp of MCP emission"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"systemMetadata.runId")," - unique run identifier"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"auditStamp.time")," - audit timestamp")),(0,t.yg)("p",null,"Use ",(0,t.yg)("inlineCode",{parentName:"p"},"datahub check metadata-diff")," to compare recordings semantically:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# Compare MCPs ignoring system metadata\ndatahub check metadata-diff \\\n    --ignore-path \"root['*']['systemMetadata']['lastObserved']\" \\\n    --ignore-path \"root['*']['systemMetadata']['runId']\" \\\n    recording_output.json replay_output.json\n")),(0,t.yg)("p",null,"A successful replay will show ",(0,t.yg)("strong",{parentName:"p"},"PERFECT SEMANTIC MATCH")," when ignoring these fields."),(0,t.yg)("h2",{id:"installation"},"Installation"),(0,t.yg)("p",null,"Install the optional ",(0,t.yg)("inlineCode",{parentName:"p"},"debug-recording")," plugin:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"pip install 'acryl-datahub[debug-recording]'\n\n# Or with your source connectors\npip install 'acryl-datahub[looker,debug-recording]'\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Dependencies:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"vcrpy>=8.0.0")," (Python 3.10+) or ",(0,t.yg)("inlineCode",{parentName:"li"},"vcrpy>=7.0.0,<8.0.0")," (Python 3.9) - HTTP recording/replay"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"pyzipper>=0.3.6")," - AES-256 encrypted archives")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Note:")," The recording module uses lazy imports to avoid requiring optional dependencies (like ",(0,t.yg)("inlineCode",{parentName:"p"},"sqlalchemy"),") when recording is not used. This means you can install the recording plugin without pulling in database connector dependencies unless you actually use them."),(0,t.yg)("h2",{id:"quick-start"},"Quick Start"),(0,t.yg)("h3",{id:"recording-an-ingestion-run"},"Recording an Ingestion Run"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# Record with password protection\ndatahub ingest run -c recipe.yaml --record --record-password mysecret\n\n# Record without S3 upload (for local testing)\ndatahub ingest run -c recipe.yaml --record --record-password mysecret --no-s3-upload\n")),(0,t.yg)("p",null,"The recording creates an encrypted ZIP archive containing:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"HTTP cassette with all request/response pairs"),(0,t.yg)("li",{parentName:"ul"},"Database query recordings (if applicable)"),(0,t.yg)("li",{parentName:"ul"},"Redacted recipe (secrets replaced with safe markers)"),(0,t.yg)("li",{parentName:"ul"},"Manifest with metadata and checksums")),(0,t.yg)("h3",{id:"replaying-a-recording"},"Replaying a Recording"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# Replay in air-gapped mode (default) - no network required\ndatahub ingest replay recording.zip --password mysecret\n\n# Replay with live sink - replay source data, emit to real DataHub\ndatahub ingest replay recording.zip --password mysecret \\\n    --live-sink --server http://localhost:8080\n")),(0,t.yg)("h3",{id:"inspecting-recordings"},"Inspecting Recordings"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# View archive metadata\ndatahub recording info recording.zip --password mysecret\n\n# Extract archive contents\ndatahub recording extract recording.zip --password mysecret --output-dir ./extracted\n\n# List contents of a recording archive\ndatahub recording list recording.zip --password mysecret\n")),(0,t.yg)("h2",{id:"configuration"},"Configuration"),(0,t.yg)("h3",{id:"recipe-configuration"},"Recipe Configuration"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"source:\n  type: looker\n  config:\n    # ... source config ...\n\n# Optional recording configuration\nrecording:\n  enabled: true\n  password: ${DATAHUB_RECORDING_PASSWORD} # Or use --record-password CLI flag\n  s3_upload: true # Upload directly to S3 (default: false)\n  output_path: s3://my-bucket/recordings/ # Required when s3_upload=true\n")),(0,t.yg)("p",null,"When ",(0,t.yg)("inlineCode",{parentName:"p"},"s3_upload")," is disabled (default), the recording is saved locally:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"To ",(0,t.yg)("inlineCode",{parentName:"li"},"output_path")," if specified"),(0,t.yg)("li",{parentName:"ul"},"To ",(0,t.yg)("inlineCode",{parentName:"li"},"INGESTION_ARTIFACT_DIR")," directory if set"),(0,t.yg)("li",{parentName:"ul"},"To a temp directory otherwise")),(0,t.yg)("h3",{id:"environment-variables"},"Environment Variables"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Variable"),(0,t.yg)("th",{parentName:"tr",align:null},"Description"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"DATAHUB_RECORDING_PASSWORD")),(0,t.yg)("td",{parentName:"tr",align:null},"Default password for recording encryption")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"ADMIN_PASSWORD")),(0,t.yg)("td",{parentName:"tr",align:null},"Fallback password (used in managed environments)")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},"INGESTION_ARTIFACT_DIR")),(0,t.yg)("td",{parentName:"tr",align:null},"Directory to save recordings when S3 upload is disabled. If not set, recordings are saved to temp directory.")))),(0,t.yg)("h3",{id:"cli-options"},"CLI Options"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Recording:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"datahub ingest run -c recipe.yaml \\\n    --record                    # Enable recording\n    --record-password <pwd>     # Encryption password\n    --record-output-path <path> # Override output path (for debugging)\n    --no-s3-upload              # Disable S3 upload\n    --no-secret-redaction       # Keep real credentials (for local debugging)\n\n# Or save to specific directory\nexport INGESTION_ARTIFACT_DIR=/path/to/recordings\ndatahub ingest run -c recipe.yaml --record --record-password <pwd> --no-s3-upload\n# Recording saved as: /path/to/recordings/recording-{run_id}.zip\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Replay:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"datahub ingest replay <archive> \\\n    --password <pwd>            # Decryption password\n    --live-sink                 # Enable real GMS sink\n    --server <url>              # GMS server for live sink\n    --token <token>             # Auth token for live sink\n")),(0,t.yg)("h2",{id:"archive-format"},"Archive Format"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre"},"recording-{run_id}.zip (AES-256 encrypted, LZMA compressed)\n\u251c\u2500\u2500 manifest.json           # Metadata, versions, checksums\n\u251c\u2500\u2500 recipe.yaml             # Recipe with redacted secrets\n\u251c\u2500\u2500 http/\n\u2502   \u2514\u2500\u2500 cassette.yaml       # VCR HTTP recordings (YAML for binary data support)\n\u2514\u2500\u2500 db/\n    \u2514\u2500\u2500 queries.jsonl       # Database query recordings\n")),(0,t.yg)("h3",{id:"manifest-contents"},"Manifest Contents"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "format_version": "1.0.0",\n  "run_id": "looker-2024-12-03-10_30_00-abc123",\n  "source_type": "looker",\n  "sink_type": "datahub-rest",\n  "datahub_cli_version": "0.14.0",\n  "python_version": "3.10.15",\n  "created_at": "2024-12-03T10:35:00Z",\n  "recording_start_time": "2024-12-03T10:30:00Z",\n  "files": ["http/cassette.yaml", "db/queries.jsonl"],\n  "checksums": { "http/cassette.yaml": "sha256:..." },\n  "has_exception": false,\n  "exception_info": null\n}\n')),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"source_type"),": The type of source connector (e.g., snowflake, looker, bigquery)"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"sink_type"),": The type of sink (e.g., datahub-rest, file)"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"datahub_cli_version"),": The DataHub CLI version used for recording"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"python_version"),': The Python version used for recording (e.g., "3.10.15")'),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"recording_start_time"),": When recording began (informational)"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"has_exception"),": Whether the recording captured an exception"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"exception_info"),": Stack trace and details if an exception occurred")),(0,t.yg)("h2",{id:"best-practices"},"Best Practices"),(0,t.yg)("h3",{id:"1-use-consistent-passwords"},"1. Use Consistent Passwords"),(0,t.yg)("p",null,"Store the recording password in a secure location (secrets manager, environment variable) and use the same password across your team:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"export DATAHUB_RECORDING_PASSWORD=$(vault read -field=password secret/datahub/recording)\ndatahub ingest run -c recipe.yaml --record\n")),(0,t.yg)("h3",{id:"2-record-in-production-like-environments"},"2. Record in Production-Like Environments"),(0,t.yg)("p",null,"For best debugging results, record in an environment that matches production:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Same credentials and permissions"),(0,t.yg)("li",{parentName:"ul"},"Same network access"),(0,t.yg)("li",{parentName:"ul"},"Same data volume (or representative sample)")),(0,t.yg)("h3",{id:"3-use-descriptive-run-ids"},"3. Use Descriptive Run IDs"),(0,t.yg)("p",null,"The archive filename includes the run_id. Use meaningful recipe names for easy identification:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"# Recipe: snowflake-prod-daily.yaml\n# Archive: snowflake-prod-daily-2024-12-03-10_30_00-abc123.zip\n")),(0,t.yg)("h3",{id:"4-test-replay-immediately"},"4. Test Replay Immediately"),(0,t.yg)("p",null,"After recording, test the replay to ensure the recording is complete:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# Record (save MCP output for comparison)\ndatahub ingest run -c recipe.yaml --record --record-password test --no-s3-upload \\\n    | tee recording_output.json\n\n# Immediately test replay (save output)\ndatahub ingest replay /tmp/recording.zip --password test \\\n    | tee replay_output.json\n\n# Verify semantic equivalence\ndatahub check metadata-diff \\\n    --ignore-path \"root['*']['systemMetadata']['lastObserved']\" \\\n    --ignore-path \"root['*']['systemMetadata']['runId']\" \\\n    recording_output.json replay_output.json\n")),(0,t.yg)("h3",{id:"5-include-exception-context"},"5. Include Exception Context"),(0,t.yg)("p",null,"If recording captures an exception, the archive includes exception details:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"datahub recording info recording.zip --password mysecret\n# Output includes: has_exception: true, exception_info: {...}\n")),(0,t.yg)("h3",{id:"6-secure-archive-handling"},"6. Secure Archive Handling"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Never commit recordings to source control"),(0,t.yg)("li",{parentName:"ul"},"Use strong passwords (16+ characters)"),(0,t.yg)("li",{parentName:"ul"},"Delete recordings after debugging is complete"),(0,t.yg)("li",{parentName:"ul"},"Use S3 lifecycle policies for automatic cleanup")),(0,t.yg)("h3",{id:"7-minimize-recording-scope"},"7. Minimize Recording Scope"),(0,t.yg)("p",null,"For faster recordings and smaller archives, limit the scope:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},'source:\n  type: looker\n  config:\n    dashboard_pattern:\n      allow:\n        - "^specific-dashboard-id$"\n')),(0,t.yg)("h2",{id:"limitations"},"Limitations"),(0,t.yg)("h3",{id:"1-thread-safe-recording-impact"},"1. Thread-Safe Recording Impact"),(0,t.yg)("p",null,"To capture all HTTP requests reliably, recording serializes HTTP calls. This has performance implications:"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Scenario"),(0,t.yg)("th",{parentName:"tr",align:null},"Without Recording"),(0,t.yg)("th",{parentName:"tr",align:null},"With Recording"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Parallel API calls"),(0,t.yg)("td",{parentName:"tr",align:null},"~10s"),(0,t.yg)("td",{parentName:"tr",align:null},"~90s")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Single-threaded"),(0,t.yg)("td",{parentName:"tr",align:null},"~90s"),(0,t.yg)("td",{parentName:"tr",align:null},"~90s")))),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Mitigation:")," Recording is intended for debugging, not production. Use ",(0,t.yg)("inlineCode",{parentName:"p"},"--no-s3-upload")," for faster local testing."),(0,t.yg)("h3",{id:"2-timestamps-differ-between-runs"},"2. Timestamps Differ Between Runs"),(0,t.yg)("p",null,"MCP metadata timestamps will always differ between recording and replay:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"systemMetadata.lastObserved")," - set when MCP is emitted"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"systemMetadata.runId")," - unique per run"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"auditStamp.time")," - set during processing")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Mitigation:")," The actual source data is identical. Use ",(0,t.yg)("inlineCode",{parentName:"p"},"datahub check metadata-diff")," with ",(0,t.yg)("inlineCode",{parentName:"p"},"--ignore-path"),' to verify semantic equivalence (see "Comparing Recording and Replay Output" above).'),(0,t.yg)("h3",{id:"3-non-deterministic-source-behavior"},"3. Non-Deterministic Source Behavior"),(0,t.yg)("p",null,"Some sources have non-deterministic behavior:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Random sampling or ordering of results"),(0,t.yg)("li",{parentName:"ul"},"Rate limiting/retry timing variations"),(0,t.yg)("li",{parentName:"ul"},"Parallel processing order")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Mitigation:")," The replay serves recorded API responses, so data is identical. The system includes custom VCR matchers that handle non-deterministic request ordering (e.g., Looker usage queries with varying filter orders)."),(0,t.yg)("h3",{id:"4-database-connection-mocking"},"4. Database Connection Mocking"),(0,t.yg)("p",null,"Database replay mocks the connection entirely - authentication is bypassed. This means:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Connection pooling behavior may differ"),(0,t.yg)("li",{parentName:"ul"},"Transaction semantics are simplified"),(0,t.yg)("li",{parentName:"ul"},"Cursor state is simulated")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Mitigation:")," For complex database debugging, use database-specific profiling tools alongside recording."),(0,t.yg)("h3",{id:"5-large-recordings"},"5. Large Recordings"),(0,t.yg)("p",null,"Recordings can be large for high-volume sources:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Looker with 1000+ dashboards: ~50MB"),(0,t.yg)("li",{parentName:"ul"},"PowerBI with many workspaces: ~100MB"),(0,t.yg)("li",{parentName:"ul"},"Snowflake with full schema extraction: ~200MB")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Mitigation:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Use patterns to limit scope"),(0,t.yg)("li",{parentName:"ul"},"Enable LZMA compression (default)"),(0,t.yg)("li",{parentName:"ul"},"Use S3 for storage instead of local disk")),(0,t.yg)("h3",{id:"6-secret-handling"},"6. Secret Handling"),(0,t.yg)("p",null,"Secrets are redacted in the stored recipe using ",(0,t.yg)("inlineCode",{parentName:"p"},"__REPLAY_DUMMY__")," markers. During replay:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Pydantic validation receives valid dummy values"),(0,t.yg)("li",{parentName:"ul"},"Actual API/DB calls use recorded responses (no real auth needed)"),(0,t.yg)("li",{parentName:"ul"},"Some sources may have validation that fails with dummy values")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Mitigation:")," The replay system auto-injects valid dummy values that pass common validators."),(0,t.yg)("h3",{id:"7-http-only-for-some-sources"},"7. HTTP-Only for Some Sources"),(0,t.yg)("p",null,"Sources using non-HTTP protocols cannot be fully recorded:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Direct TCP/binary database protocols (partially supported via db_proxy)"),(0,t.yg)("li",{parentName:"ul"},"gRPC (not currently supported)"),(0,t.yg)("li",{parentName:"ul"},"WebSocket (not currently supported)")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Mitigation:")," Most sources use HTTP REST APIs which are fully supported."),(0,t.yg)("h3",{id:"8-vendored-http-libraries-snowflake-databricks"},"8. Vendored HTTP Libraries (Snowflake, Databricks)"),(0,t.yg)("p",null,"Some database connectors use non-standard HTTP implementations:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Snowflake"),": Uses ",(0,t.yg)("inlineCode",{parentName:"li"},"snowflake.connector.vendored.urllib3")," and ",(0,t.yg)("inlineCode",{parentName:"li"},"vendored.requests")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Databricks"),": Uses internal Thrift HTTP client")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Impact:")," HTTP authentication calls are NOT recorded during connection setup."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Why recording still works:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Authentication happens once during ",(0,t.yg)("inlineCode",{parentName:"li"},"connect()")),(0,t.yg)("li",{parentName:"ul"},"SQL queries use standard DB-API cursors (no HTTP involved)"),(0,t.yg)("li",{parentName:"ul"},"During replay, authentication is bypassed entirely (mock connection)"),(0,t.yg)("li",{parentName:"ul"},"All SQL queries and results are perfectly recorded/replayed")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"What IS recorded:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"\u2705 All SQL queries via ",(0,t.yg)("inlineCode",{parentName:"li"},"cursor.execute()")),(0,t.yg)("li",{parentName:"ul"},"\u2705 All query results"),(0,t.yg)("li",{parentName:"ul"},"\u2705 Cursor metadata (description, rowcount)")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"What is NOT recorded:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"\u274c HTTP authentication calls (not needed for replay)"),(0,t.yg)("li",{parentName:"ul"},"\u274c PUT/GET file operations (not used in metadata ingestion)")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Automatic error handling:"),"\nThe recording system detects when VCR interferes with connection and automatically retries with VCR bypassed. You'll see warnings in logs but recording will succeed. SQL queries are captured normally regardless of HTTP recording status."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"For debugging:")," SQL query recordings are sufficient for all metadata extraction scenarios."),(0,t.yg)("h3",{id:"9-stateful-ingestion"},"9. Stateful Ingestion"),(0,t.yg)("p",null,"Stateful ingestion checkpoints may behave differently during replay:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Recorded state may reference timestamps that don't match replay time"),(0,t.yg)("li",{parentName:"ul"},"State backend calls are mocked")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Mitigation:")," For stateful debugging, record a fresh run without existing state."),(0,t.yg)("h3",{id:"10-memory-usage"},"10. Memory Usage"),(0,t.yg)("p",null,"Large recordings are loaded into memory during replay:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"HTTP cassette is fully loaded"),(0,t.yg)("li",{parentName:"ul"},"DB queries are streamed from JSONL")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Mitigation:")," For very large recordings, extract and inspect specific parts:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"datahub recording extract recording.zip --password mysecret --output-dir ./extracted\n# Manually inspect http/cassette.yaml\n")),(0,t.yg)("h3",{id:"11-lazy-imports"},"11. Lazy Imports"),(0,t.yg)("p",null,"The recording module uses lazy imports to avoid requiring optional dependencies when recording is not used:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"sqlalchemy")," is only imported when actually recording/replaying SQLAlchemy-based sources"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"RecordingConfig"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"IngestionRecorder"),", and ",(0,t.yg)("inlineCode",{parentName:"li"},"IngestionReplayer")," are imported on-demand via ",(0,t.yg)("inlineCode",{parentName:"li"},"__getattr__")),(0,t.yg)("li",{parentName:"ul"},"This allows installing the recording plugin without pulling in database connector dependencies"),(0,t.yg)("li",{parentName:"ul"},"This also allows other sources not depending on SQLAlchemy (e.g., HTTP-based sources like Looker, PowerBI) to be safely installed when no recording is used")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Impact:")," This is transparent to users - the recording system works exactly the same, but with better dependency isolation. The ",(0,t.yg)("inlineCode",{parentName:"p"},"debug-recording")," plugin is designed to be installed alongside source connectors, not as a standalone package. Dependencies like ",(0,t.yg)("inlineCode",{parentName:"p"},"sqlalchemy")," are expected to be provided by the source connector itself when needed."),(0,t.yg)("h2",{id:"supported-sources"},"Supported Sources"),(0,t.yg)("h3",{id:"fully-supported-http-based"},"Fully Supported (HTTP-based)"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Source"),(0,t.yg)("th",{parentName:"tr",align:null},"HTTP Recording"),(0,t.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Looker"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Full support including SDK calls")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"PowerBI"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Full support")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Tableau"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Full support")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Superset"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Full support")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Mode"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Full support")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Sigma"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Full support")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"dbt Cloud"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Full support")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Fivetran"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Full support")))),(0,t.yg)("h3",{id:"database-sources"},"Database Sources"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Source"),(0,t.yg)("th",{parentName:"tr",align:null},"HTTP Recording"),(0,t.yg)("th",{parentName:"tr",align:null},"DB Recording"),(0,t.yg)("th",{parentName:"tr",align:null},"Strategy"),(0,t.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Snowflake"),(0,t.yg)("td",{parentName:"tr",align:null},"\u274c Not needed"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Connection wrapper"),(0,t.yg)("td",{parentName:"tr",align:null},"Native connector wrapped at ",(0,t.yg)("inlineCode",{parentName:"td"},"connect()"))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Redshift"),(0,t.yg)("td",{parentName:"tr",align:null},"N/A"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Connection wrapper"),(0,t.yg)("td",{parentName:"tr",align:null},"Native connector wrapped at ",(0,t.yg)("inlineCode",{parentName:"td"},"connect()"))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Databricks"),(0,t.yg)("td",{parentName:"tr",align:null},"\u274c Not needed"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Connection wrapper"),(0,t.yg)("td",{parentName:"tr",align:null},"Native connector wrapped at ",(0,t.yg)("inlineCode",{parentName:"td"},"connect()"))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"BigQuery"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 (REST API)"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Client wrapper"),(0,t.yg)("td",{parentName:"tr",align:null},"Client class wrapped")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"PostgreSQL"),(0,t.yg)("td",{parentName:"tr",align:null},"N/A"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Connection.execute() wrapper"),(0,t.yg)("td",{parentName:"tr",align:null},"SQLAlchemy ",(0,t.yg)("inlineCode",{parentName:"td"},"connection.execute()")," wrapped")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"MySQL"),(0,t.yg)("td",{parentName:"tr",align:null},"N/A"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Connection.execute() wrapper"),(0,t.yg)("td",{parentName:"tr",align:null},"SQLAlchemy ",(0,t.yg)("inlineCode",{parentName:"td"},"connection.execute()")," wrapped")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"SQLite"),(0,t.yg)("td",{parentName:"tr",align:null},"N/A"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Connection.execute() wrapper"),(0,t.yg)("td",{parentName:"tr",align:null},"SQLAlchemy ",(0,t.yg)("inlineCode",{parentName:"td"},"connection.execute()")," wrapped")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"MSSQL"),(0,t.yg)("td",{parentName:"tr",align:null},"N/A"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Connection.execute() wrapper"),(0,t.yg)("td",{parentName:"tr",align:null},"SQLAlchemy ",(0,t.yg)("inlineCode",{parentName:"td"},"connection.execute()")," wrapped")))),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Note:")," File staging operations (PUT/GET) are not used in metadata extraction and are therefore not a concern for recording/replay."),(0,t.yg)("h4",{id:"hybrid-recording-strategy"},"Hybrid Recording Strategy"),(0,t.yg)("p",null,"The recording system uses a ",(0,t.yg)("strong",{parentName:"p"},"hybrid approach")," that selects the best interception method for each database connector type:"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"1. Wrapper Strategy (Native Connectors)")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Used for:")," Snowflake, Redshift, Databricks, BigQuery"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"How it works:")," Wraps the connector's ",(0,t.yg)("inlineCode",{parentName:"li"},"connect()")," function or Client class"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Why:")," These connectors have direct ",(0,t.yg)("inlineCode",{parentName:"li"},"connect()")," functions that return connections we can wrap"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Implementation:")," ",(0,t.yg)("inlineCode",{parentName:"li"},"ConnectionProxy")," wraps the real connection, ",(0,t.yg)("inlineCode",{parentName:"li"},"CursorProxy")," intercepts queries")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"2. Connection.execute() Wrapper Strategy (SQLAlchemy-based)")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Used for:")," PostgreSQL, MySQL, SQLite, MSSQL, and other SQLAlchemy-based sources"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"How it works:")," Wraps ",(0,t.yg)("inlineCode",{parentName:"li"},"engine.connect()")," to intercept connections, then wraps ",(0,t.yg)("inlineCode",{parentName:"li"},"connection.execute()")," to capture queries and results"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Why:")," SQLAlchemy 2.x uses Result objects that are best captured at the ",(0,t.yg)("inlineCode",{parentName:"li"},"execute()")," level, avoiding import reference issues with modules that import ",(0,t.yg)("inlineCode",{parentName:"li"},"create_engine")," directly"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Implementation:")," Wraps ",(0,t.yg)("inlineCode",{parentName:"li"},"engine.connect()")," to return connections with wrapped ",(0,t.yg)("inlineCode",{parentName:"li"},"execute()")," methods that materialize and record results"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Benefits:")," Works even when modules import ",(0,t.yg)("inlineCode",{parentName:"li"},"create_engine")," directly (e.g., ",(0,t.yg)("inlineCode",{parentName:"li"},"from sqlalchemy import create_engine"),"), avoiding stale reference issues")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Why Different Strategies?")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Native connectors")," (Snowflake, Redshift) expose direct ",(0,t.yg)("inlineCode",{parentName:"li"},"connect()")," functions that are easy to wrap"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"SQLAlchemy-based sources")," use connection pooling and engines. Wrapping ",(0,t.yg)("inlineCode",{parentName:"li"},"connection.execute()")," captures Result objects directly, avoiding issues with modules that import ",(0,t.yg)("inlineCode",{parentName:"li"},"create_engine")," directly"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"BigQuery")," uses a Client class pattern, requiring class-level wrapping")),(0,t.yg)("p",null,"Both strategies achieve the same goal: intercepting SQL queries and results for recording/replay, but use the most appropriate method for each connector's architecture."),(0,t.yg)("h4",{id:"database-connection-architecture"},"Database Connection Architecture"),(0,t.yg)("p",null,"Database sources have a two-phase execution model:"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Phase 1: Authentication (During ",(0,t.yg)("inlineCode",{parentName:"strong"},"connect()"),")")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Uses source-specific HTTP clients (may be vendored/custom)"),(0,t.yg)("li",{parentName:"ul"},"NOT recorded (but also not needed during replay)"),(0,t.yg)("li",{parentName:"ul"},"During replay: Bypassed entirely with mock connection"),(0,t.yg)("li",{parentName:"ul"},"Automatic retry if VCR interferes with connection")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Phase 2: SQL Execution (After ",(0,t.yg)("inlineCode",{parentName:"strong"},"connect()"),")")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Uses standard Python DB-API 2.0 cursor interface"),(0,t.yg)("li",{parentName:"ul"},"Fully recorded via ",(0,t.yg)("inlineCode",{parentName:"li"},"CursorProxy")," (works for both wrapper strategies)"),(0,t.yg)("li",{parentName:"ul"},"Protocol-agnostic (works for any DB-API connector)"),(0,t.yg)("li",{parentName:"ul"},"During replay: Served from recorded ",(0,t.yg)("inlineCode",{parentName:"li"},"queries.jsonl"))),(0,t.yg)("p",null,"This architecture makes recording resilient to HTTP library changes while maintaining perfect SQL replay fidelity. For Snowflake and Databricks, all metadata extraction happens via SQL queries in Phase 2, making HTTP recording unnecessary."),(0,t.yg)("h3",{id:"datahub-backend"},"DataHub Backend"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Component"),(0,t.yg)("th",{parentName:"tr",align:null},"Recording"),(0,t.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"GMS REST API"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Sink emissions captured")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"GraphQL API"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"If used by source")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Stateful Backend"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Checkpoint calls captured")))),(0,t.yg)("h2",{id:"troubleshooting"},"Troubleshooting"),(0,t.yg)("h3",{id:"module-not-found-vcrpy"},'"Module not found: vcrpy"'),(0,t.yg)("p",null,"Install the debug-recording plugin:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"pip install 'acryl-datahub[debug-recording]'\n")),(0,t.yg)("h3",{id:"checksum-verification-failed"},'"Checksum verification failed"'),(0,t.yg)("p",null,"The archive may be corrupted. Re-download or re-record:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"datahub recording info recording.zip --password mysecret\n# Check for checksum errors in output\n")),(0,t.yg)("h3",{id:"no-match-for-request-during-replay"},'"No match for request" during replay'),(0,t.yg)("p",null,"The recorded cassette doesn't have a matching request. This can happen if:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Recording was incomplete (check ",(0,t.yg)("inlineCode",{parentName:"li"},"has_exception")," in manifest)"),(0,t.yg)("li",{parentName:"ol"},"Source behavior changed between recording and replay"),(0,t.yg)("li",{parentName:"ol"},"Different credentials caused different API paths")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Solution:")," Re-record with the exact same configuration."),(0,t.yg)("h3",{id:"replay-produces-different-event-count"},"Replay produces different event count"),(0,t.yg)("p",null,"A small difference in event count (e.g., 3259 vs 3251) is normal due to:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Duplicate MCP emissions during recording"),(0,t.yg)("li",{parentName:"ul"},"Timing-dependent code paths"),(0,t.yg)("li",{parentName:"ul"},"Non-deterministic processing order")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Verification:")," Use ",(0,t.yg)("inlineCode",{parentName:"p"},"datahub check metadata-diff")," to confirm semantic equivalence:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"datahub check metadata-diff \\\n    --ignore-path \"root['*']['systemMetadata']['lastObserved']\" \\\n    --ignore-path \"root['*']['systemMetadata']['runId']\" \\\n    recording_output.json replay_output.json\n")),(0,t.yg)("p",null,'A "PERFECT SEMANTIC MATCH" confirms the replay is correct despite count differences.'),(0,t.yg)("h3",{id:"recording-takes-too-long"},"Recording takes too long"),(0,t.yg)("p",null,"HTTP requests are serialized during recording for reliability. To speed up:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Reduce source scope with patterns"),(0,t.yg)("li",{parentName:"ol"},"Use ",(0,t.yg)("inlineCode",{parentName:"li"},"--no-s3-upload")," for local testing"),(0,t.yg)("li",{parentName:"ol"},"Accept that recording is slower than normal ingestion")),(0,t.yg)("h3",{id:"archive-too-large-for-s3-upload"},"Archive too large for S3 upload"),(0,t.yg)("p",null,"Large archives may timeout during upload:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# Record locally first\ndatahub ingest run -c recipe.yaml --record --record-password mysecret --no-s3-upload\n\n# Upload manually with multipart\naws s3 cp recording.zip s3://bucket/recordings/ --expected-size $(stat -f%z recording.zip)\n")),(0,t.yg)("h2",{id:"architecture"},"Architecture"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre"},"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    IngestionRecorder                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  HTTPRecorder   \u2502  \u2502   ModulePatcher \u2502  \u2502 QueryRecorder\u2502 \u2502\n\u2502  \u2502  (VCR.py)       \u2502  \u2502   (DB proxies)  \u2502  \u2502  (JSONL)     \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502           \u2502                    \u2502                   \u2502         \u2502\n\u2502           \u25bc                    \u25bc                   \u25bc         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502                 Encrypted Archive                        \u2502\u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\u2502\n\u2502  \u2502  \u2502manifest  \u2502 \u2502 recipe   \u2502 \u2502cassette  \u2502 \u2502queries.jsonl\u2502 \u2502\u2502\n\u2502  \u2502  \u2502.json     \u2502 \u2502 .yaml    \u2502 \u2502.yaml     \u2502 \u2502            \u2502  \u2502\u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    IngestionReplayer                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  HTTPReplayer   \u2502  \u2502  ReplayPatcher  \u2502  \u2502 QueryReplayer\u2502 \u2502\n\u2502  \u2502  (VCR replay)   \u2502  \u2502  (Mock conns)   \u2502  \u2502  (Mock cursor\u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                              \u2502                               \u2502\n\u2502                              \u25bc                               \u2502\n\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n\u2502              \u2502    Air-Gapped Replay         \u2502               \u2502\n\u2502              \u2502  - No network required       \u2502               \u2502\n\u2502              \u2502  - Full debugger support     \u2502               \u2502\n\u2502              \u2502  - Exact reproduction        \u2502               \u2502\n\u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n")),(0,t.yg)("h2",{id:"contributing"},"Contributing"),(0,t.yg)("p",null,"When adding new source connectors:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"HTTP-based sources work automatically via VCR"),(0,t.yg)("li",{parentName:"ol"},"Database sources may need additions to ",(0,t.yg)("inlineCode",{parentName:"li"},"patcher.py")," for their specific connector"),(0,t.yg)("li",{parentName:"ol"},"Test recording and replay with the new source before releasing")),(0,t.yg)("h2",{id:"see-also"},"See Also"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"https://datahubproject.io/docs/metadata-ingestion"},"DataHub Ingestion Framework")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"https://vcrpy.readthedocs.io/"},"VCR.py Documentation")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"https://datahubproject.io/docs/how/debugging"},"Debugging Ingestion Issues"))))}y.isMDXComponent=!0}}]);