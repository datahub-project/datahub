"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[25026],{15680:(e,a,n)=>{n.d(a,{xA:()=>g,yg:()=>m});var t=n(96540);function l(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function r(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),n.push.apply(n,t)}return n}function i(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?r(Object(n),!0).forEach((function(a){l(e,a,n[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))}))}return e}function o(e,a){if(null==e)return{};var n,t,l=function(e,a){if(null==e)return{};var n,t,l={},r=Object.keys(e);for(t=0;t<r.length;t++)n=r[t],a.indexOf(n)>=0||(l[n]=e[n]);return l}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)n=r[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(l[n]=e[n])}return l}var p=t.createContext({}),s=function(e){var a=t.useContext(p),n=a;return e&&(n="function"==typeof e?e(a):i(i({},a),e)),n},g=function(e){var a=s(e.components);return t.createElement(p.Provider,{value:a},e.children)},u="mdxType",y={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},d=t.forwardRef((function(e,a){var n=e.components,l=e.mdxType,r=e.originalType,p=e.parentName,g=o(e,["components","mdxType","originalType","parentName"]),u=s(n),d=l,m=u["".concat(p,".").concat(d)]||u[d]||y[d]||r;return n?t.createElement(m,i(i({ref:a},g),{},{components:n})):t.createElement(m,i({ref:a},g))}));function m(e,a){var n=arguments,l=a&&a.mdxType;if("string"==typeof e||l){var r=n.length,i=new Array(r);i[0]=d;var o={};for(var p in a)hasOwnProperty.call(a,p)&&(o[p]=a[p]);o.originalType=e,o[u]="string"==typeof e?e:l,i[1]=o;for(var s=2;s<r;s++)i[s]=n[s];return t.createElement.apply(null,i)}return t.createElement.apply(null,n)}d.displayName="MDXCreateElement"},294:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>g,contentTitle:()=>p,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>u});n(96540);var t=n(15680);function l(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function r(e,a){return a=null!=a?a:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):function(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),n.push.apply(n,t)}return n}(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})),e}function i(e,a){if(null==e)return{};var n,t,l=function(e,a){if(null==e)return{};var n,t,l={},r=Object.keys(e);for(t=0;t<r.length;t++)n=r[t],a.indexOf(n)>=0||(l[n]=e[n]);return l}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)n=r[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(l[n]=e[n])}return l}const o={title:"Optional PySpark Support for S3 Source",slug:"/pyspark",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/PYSPARK.md"},p="Optional PySpark Support for S3 Source",s={unversionedId:"docs/PYSPARK",id:"docs/PYSPARK",title:"Optional PySpark Support for S3 Source",description:"DataHub's S3 source now supports optional PySpark installation through the s3-slim variant. This allows users to choose a lightweight installation when data lake profiling is not needed.",source:"@site/genDocs/docs/PYSPARK.md",sourceDirName:"docs",slug:"/pyspark",permalink:"/docs/pyspark",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/PYSPARK.md",tags:[],version:"current",frontMatter:{title:"Optional PySpark Support for S3 Source",slug:"/pyspark",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/PYSPARK.md"}},g={},u=[{value:"Overview",id:"overview",level:2},{value:"PySpark Version",id:"pyspark-version",level:2},{value:"Installation Options",id:"installation-options",level:2},{value:"Standard Installation (includes PySpark)",id:"standard-installation-includes-pyspark",level:3},{value:"Lightweight Installation (without PySpark)",id:"lightweight-installation-without-pyspark",level:3},{value:"What&#39;s Included",id:"whats-included",level:3},{value:"Feature Comparison",id:"feature-comparison",level:2},{value:"Configuration",id:"configuration",level:2},{value:"With Standard Installation (PySpark included)",id:"with-standard-installation-pyspark-included",level:3},{value:"With Slim Installation (no PySpark)",id:"with-slim-installation-no-pyspark",level:3},{value:"Developer Guide",id:"developer-guide",level:2},{value:"Implementation Pattern",id:"implementation-pattern",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Error: &quot;PySpark is not installed, but is required for profiling&quot;",id:"error-pyspark-is-not-installed-but-is-required-for-profiling",level:3},{value:"Verifying Installation",id:"verifying-installation",level:3},{value:"Migration Guide",id:"migration-guide",level:2},{value:"Upgrading from Previous Versions",id:"upgrading-from-previous-versions",level:3},{value:"No Breaking Changes",id:"no-breaking-changes",level:3},{value:"Benefits for DataHub Actions",id:"benefits-for-datahub-actions",level:2},{value:"Reduced Installation Size",id:"reduced-installation-size",level:3},{value:"Faster Deployment",id:"faster-deployment",level:3},{value:"Fewer Dependency Conflicts",id:"fewer-dependency-conflicts",level:3},{value:"When Actions Needs Profiling",id:"when-actions-needs-profiling",level:3},{value:"Benefits Summary",id:"benefits-summary",level:2}],y={toc:u},d="wrapper";function m(e){var{components:a}=e,n=i(e,["components"]);return(0,t.yg)(d,r(function(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{},t=Object.keys(n);"function"==typeof Object.getOwnPropertySymbols&&(t=t.concat(Object.getOwnPropertySymbols(n).filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable})))),t.forEach((function(a){l(e,a,n[a])}))}return e}({},y,n),{components:a,mdxType:"MDXLayout"}),(0,t.yg)("h1",{id:"optional-pyspark-support-for-s3-source"},"Optional PySpark Support for S3 Source"),(0,t.yg)("p",null,"DataHub's S3 source now supports optional PySpark installation through the ",(0,t.yg)("inlineCode",{parentName:"p"},"s3-slim")," variant. This allows users to choose a lightweight installation when data lake profiling is not needed."),(0,t.yg)("h2",{id:"overview"},"Overview"),(0,t.yg)("p",null,"The S3 source includes PySpark by default for backward compatibility and profiling support. For users who only need metadata extraction without profiling, the ",(0,t.yg)("inlineCode",{parentName:"p"},"s3-slim")," variant provides a ~500MB smaller installation."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Current implementation status:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"S3"),": SparkProfiler pattern fully implemented (optional PySpark)"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"ABS"),": Not yet implemented (still requires PySpark for profiling)"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Unity Catalog"),": Not affected by this change (uses separate profiling mechanisms)"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"GCS"),": Does not support profiling")),(0,t.yg)("blockquote",null,(0,t.yg)("p",{parentName:"blockquote"},(0,t.yg)("strong",{parentName:"p"},"Note:")," This change implements the SparkProfiler pattern for S3 only. The same pattern can be applied to other sources (ABS, etc.) in future PRs.")),(0,t.yg)("h2",{id:"pyspark-version"},"PySpark Version"),(0,t.yg)("blockquote",null,(0,t.yg)("p",{parentName:"blockquote"},(0,t.yg)("strong",{parentName:"p"},"Current Version:")," PySpark 3.5.x (3.5.6)"),(0,t.yg)("p",{parentName:"blockquote"},"PySpark 4.0 support is planned for a future release. Until then, all DataHub components use PySpark 3.5.x for compatibility and stability.")),(0,t.yg)("h2",{id:"installation-options"},"Installation Options"),(0,t.yg)("h3",{id:"standard-installation-includes-pyspark"},"Standard Installation (includes PySpark)"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"pip install 'acryl-datahub[s3]'         # S3 with PySpark/profiling support\n")),(0,t.yg)("h3",{id:"lightweight-installation-without-pyspark"},"Lightweight Installation (without PySpark)"),(0,t.yg)("p",null,"For installations where you don't need profiling capabilities and want to save ~500MB:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"pip install 'acryl-datahub[s3-slim]'    # S3 without profiling (~500MB smaller)\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Recommendation:")," Use ",(0,t.yg)("inlineCode",{parentName:"p"},"s3-slim")," when profiling is not needed."),(0,t.yg)("p",null,"The ",(0,t.yg)("inlineCode",{parentName:"p"},"data-lake-profiling")," dependencies (included in standard ",(0,t.yg)("inlineCode",{parentName:"p"},"s3")," by default):"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"pyspark~=3.5.6")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"pydeequ>=1.1.0")),(0,t.yg)("li",{parentName:"ul"},"Profiling dependencies (cachetools)")),(0,t.yg)("blockquote",null,(0,t.yg)("p",{parentName:"blockquote"},(0,t.yg)("strong",{parentName:"p"},"Note:")," In a future major release (e.g., DataHub 2.0), the ",(0,t.yg)("inlineCode",{parentName:"p"},"s3-slim")," variant may become the default, and PySpark will be truly optional. This current approach provides backward compatibility while giving users time to adapt.")),(0,t.yg)("h3",{id:"whats-included"},"What's Included"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"S3 source:")),(0,t.yg)("p",null,"Standard ",(0,t.yg)("inlineCode",{parentName:"p"},"s3")," extra:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"\u2705 Metadata extraction (schemas, tables, file listing)"),(0,t.yg)("li",{parentName:"ul"},"\u2705 Data format detection (Parquet, Avro, CSV, JSON, etc.)"),(0,t.yg)("li",{parentName:"ul"},"\u2705 Schema inference from files"),(0,t.yg)("li",{parentName:"ul"},"\u2705 Table and column-level metadata"),(0,t.yg)("li",{parentName:"ul"},"\u2705 Tags and properties extraction"),(0,t.yg)("li",{parentName:"ul"},"\u2705 Data profiling (min/max, nulls, distinct counts)"),(0,t.yg)("li",{parentName:"ul"},"\u2705 Data quality checks (PyDeequ-based)"),(0,t.yg)("li",{parentName:"ul"},"Includes: PySpark 3.5.6 + PyDeequ")),(0,t.yg)("p",null,(0,t.yg)("inlineCode",{parentName:"p"},"s3-slim")," variant:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"\u2705 All metadata features (same as above)"),(0,t.yg)("li",{parentName:"ul"},"\u274c Data profiling disabled"),(0,t.yg)("li",{parentName:"ul"},"No PySpark dependencies (~500MB smaller)")),(0,t.yg)("h2",{id:"feature-comparison"},"Feature Comparison"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Feature"),(0,t.yg)("th",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"th"},"s3-slim")),(0,t.yg)("th",{parentName:"tr",align:null},"Standard ",(0,t.yg)("inlineCode",{parentName:"th"},"s3")))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Metadata extraction")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full support"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full support")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Schema inference")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full support"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full support")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Tags & properties")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full support"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full support")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Data profiling")),(0,t.yg)("td",{parentName:"tr",align:null},"\u274c Not available"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full profiling")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Installation size")),(0,t.yg)("td",{parentName:"tr",align:null},"~200MB"),(0,t.yg)("td",{parentName:"tr",align:null},"~700MB")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Install time")),(0,t.yg)("td",{parentName:"tr",align:null},"Fast"),(0,t.yg)("td",{parentName:"tr",align:null},"Slower (PySpark build)")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"PySpark dependencies")),(0,t.yg)("td",{parentName:"tr",align:null},"\u274c None"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 PySpark 3.5.6 + PyDeequ")))),(0,t.yg)("h2",{id:"configuration"},"Configuration"),(0,t.yg)("h3",{id:"with-standard-installation-pyspark-included"},"With Standard Installation (PySpark included)"),(0,t.yg)("p",null,"When you install ",(0,t.yg)("inlineCode",{parentName:"p"},"acryl-datahub[s3]"),", profiling works out of the box:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"source:\n  type: s3\n  config:\n    path_specs:\n      - include: s3://my-bucket/data/**/*.parquet\n    profiling:\n      enabled: true # Works seamlessly with standard installation\n      profile_table_level_only: false\n")),(0,t.yg)("h3",{id:"with-slim-installation-no-pyspark"},"With Slim Installation (no PySpark)"),(0,t.yg)("p",null,"When you install ",(0,t.yg)("inlineCode",{parentName:"p"},"s3-slim"),", disable profiling in your config:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"source:\n  type: s3\n  config:\n    path_specs:\n      - include: s3://my-bucket/data/**/*.parquet\n    profiling:\n      enabled: false # Required for s3-slim installation\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"If you enable profiling with s3-slim installation"),", you'll see a clear error message at runtime:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre"},"RuntimeError: PySpark is not installed, but is required for S3 profiling.\nPlease install with: pip install 'acryl-datahub[s3]'\n")),(0,t.yg)("h2",{id:"developer-guide"},"Developer Guide"),(0,t.yg)("h3",{id:"implementation-pattern"},"Implementation Pattern"),(0,t.yg)("p",null,"The S3 source demonstrates the recommended pattern for isolating PySpark-dependent code. This pattern can be applied to ABS and other sources in future PRs."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Architecture (currently implemented for S3 only):")),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Main source class")," (",(0,t.yg)("inlineCode",{parentName:"li"},"source.py"),") - Contains no PySpark imports at module level"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Profiler class")," (",(0,t.yg)("inlineCode",{parentName:"li"},"profiling.py"),") - Encapsulates all PySpark/PyDeequ logic in ",(0,t.yg)("inlineCode",{parentName:"li"},"SparkProfiler")," class"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Conditional instantiation")," - ",(0,t.yg)("inlineCode",{parentName:"li"},"SparkProfiler")," created only when profiling is enabled"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"TYPE_CHECKING imports")," - Type annotations use TYPE_CHECKING block for optional dependencies")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Key Benefits:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"\u2705 Type safety preserved (mypy passes without issues)"),(0,t.yg)("li",{parentName:"ul"},"\u2705 Proper code layer separation"),(0,t.yg)("li",{parentName:"ul"},"\u2705 Works with both standard and ",(0,t.yg)("inlineCode",{parentName:"li"},"-slim")," installations"),(0,t.yg)("li",{parentName:"ul"},"\u2705 Clear error messages when dependencies missing"),(0,t.yg)("li",{parentName:"ul"},"\u2705 Pattern can be reused for ABS and other sources")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Example structure:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'# source.py\nif TYPE_CHECKING:\n    from datahub.ingestion.source.s3.profiling import SparkProfiler\n\nclass S3Source:\n    profiler: Optional["SparkProfiler"]\n\n    def __init__(self, config, ctx):\n        if config.is_profiling_enabled():\n            from datahub.ingestion.source.s3.profiling import SparkProfiler\n            self.profiler = SparkProfiler(...)\n        else:\n            self.profiler = None\n')),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'# profiling.py\nclass SparkProfiler:\n    """Encapsulates all PySpark/PyDeequ profiling logic."""\n\n    def init_spark(self) -> Any:\n        # Spark session initialization\n\n    def read_file_spark(self, file: str, ext: str):\n        # File reading with Spark\n\n    def get_table_profile(self, table_data, dataset_urn):\n        # Table profiling coordination\n')),(0,t.yg)("p",null,"For more details, see the ",(0,t.yg)("a",{parentName:"p",href:"/docs/metadata-ingestion/adding-source#31-using-optional-dependencies-eg-pyspark"},"Adding a Metadata Ingestion Source")," guide."),(0,t.yg)("h2",{id:"troubleshooting"},"Troubleshooting"),(0,t.yg)("h3",{id:"error-pyspark-is-not-installed-but-is-required-for-profiling"},'Error: "PySpark is not installed, but is required for profiling"'),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Problem:")," You installed a ",(0,t.yg)("inlineCode",{parentName:"p"},"-slim")," variant but have profiling enabled in your config."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Solutions:")),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Recommended:")," Use standard installation with PySpark:"),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"pip uninstall acryl-datahub\npip install 'acryl-datahub[s3]'    # For S3 profiling\n"))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Alternative:")," Disable profiling in your recipe:"),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"profiling:\n  enabled: false\n")))),(0,t.yg)("h3",{id:"verifying-installation"},"Verifying Installation"),(0,t.yg)("p",null,"Check if PySpark is installed:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},'# Check installed packages\npip list | grep pyspark\n\n# Test import in Python\npython -c "import pyspark; print(pyspark.__version__)"\n')),(0,t.yg)("p",null,"Expected output:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Standard installation (",(0,t.yg)("inlineCode",{parentName:"li"},"s3"),"): Shows ",(0,t.yg)("inlineCode",{parentName:"li"},"pyspark 3.5.x")),(0,t.yg)("li",{parentName:"ul"},"Slim installation (",(0,t.yg)("inlineCode",{parentName:"li"},"s3-slim"),"): Import fails or package not found")),(0,t.yg)("h2",{id:"migration-guide"},"Migration Guide"),(0,t.yg)("h3",{id:"upgrading-from-previous-versions"},"Upgrading from Previous Versions"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"No action required!")," This change is fully backward compatible:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# Existing installations continue to work exactly as before\npip install 'acryl-datahub[s3]'  # Still includes PySpark by default (profiling supported)\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Recommended: Optimize installations")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"S3 with profiling:")," Keep using ",(0,t.yg)("inlineCode",{parentName:"li"},"acryl-datahub[s3]")," (includes PySpark)"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"S3 without profiling:")," Switch to ",(0,t.yg)("inlineCode",{parentName:"li"},"acryl-datahub[s3-slim]")," to save ~500MB")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# Recommended installations\npip install 'acryl-datahub[s3]'         # S3 with profiling support\npip install 'acryl-datahub[s3-slim]'    # S3 metadata only (no profiling)\n")),(0,t.yg)("h3",{id:"no-breaking-changes"},"No Breaking Changes"),(0,t.yg)("p",null,"This implementation maintains full backward compatibility:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Standard ",(0,t.yg)("inlineCode",{parentName:"li"},"s3")," extra includes PySpark (unchanged behavior)"),(0,t.yg)("li",{parentName:"ul"},"All existing recipes and configs continue to work"),(0,t.yg)("li",{parentName:"ul"},"New ",(0,t.yg)("inlineCode",{parentName:"li"},"s3-slim")," variant available for users who want smaller installations"),(0,t.yg)("li",{parentName:"ul"},"Future DataHub 2.0 may flip defaults, but provides migration path")),(0,t.yg)("h2",{id:"benefits-for-datahub-actions"},"Benefits for DataHub Actions"),(0,t.yg)("p",null,(0,t.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/tree/master/datahub-actions"},"DataHub Actions")," depends on ",(0,t.yg)("inlineCode",{parentName:"p"},"acryl-datahub")," and can benefit from ",(0,t.yg)("inlineCode",{parentName:"p"},"s3-slim")," when profiling is not needed:"),(0,t.yg)("h3",{id:"reduced-installation-size"},"Reduced Installation Size"),(0,t.yg)("p",null,"DataHub Actions typically doesn't need data lake profiling capabilities since it focuses on reacting to metadata events, not extracting metadata from data lakes. Use ",(0,t.yg)("inlineCode",{parentName:"p"},"s3-slim")," to reduce footprint:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# If Actions needs S3 metadata access but not profiling\npip install acryl-datahub-actions\npip install 'acryl-datahub[s3-slim]'\n# Result: ~500MB smaller than standard s3 extra\n\n# If Actions needs full S3 with profiling\npip install acryl-datahub-actions\npip install 'acryl-datahub[s3]'\n# Result: Includes PySpark for profiling capabilities\n")),(0,t.yg)("h3",{id:"faster-deployment"},"Faster Deployment"),(0,t.yg)("p",null,"Actions services using ",(0,t.yg)("inlineCode",{parentName:"p"},"s3-slim")," deploy faster in containerized environments:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Faster pip install"),": No PySpark compilation required"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Smaller Docker images"),": Reduced base image size"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Quicker cold starts"),": Less code to load and initialize")),(0,t.yg)("h3",{id:"fewer-dependency-conflicts"},"Fewer Dependency Conflicts"),(0,t.yg)("p",null,"Actions workflows often integrate with other tools (Slack, Teams, email services). Using ",(0,t.yg)("inlineCode",{parentName:"p"},"s3-slim")," reduces:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Python version constraint conflicts"),(0,t.yg)("li",{parentName:"ul"},"Java/Spark runtime conflicts in restricted environments"),(0,t.yg)("li",{parentName:"ul"},"Transitive dependency version mismatches")),(0,t.yg)("h3",{id:"when-actions-needs-profiling"},"When Actions Needs Profiling"),(0,t.yg)("p",null,"If your Actions workflow needs to trigger data lake profiling jobs, use the standard extra:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# Actions with data lake profiling capability\npip install 'acryl-datahub-actions'\npip install 'acryl-datahub[s3]'  # Includes PySpark by default\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Common Actions use cases that DON'T need PySpark:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Slack notifications on schema changes"),(0,t.yg)("li",{parentName:"ul"},"Propagating tags and terms to downstream systems"),(0,t.yg)("li",{parentName:"ul"},"Triggering dbt runs on metadata updates"),(0,t.yg)("li",{parentName:"ul"},"Sending emails on data quality failures"),(0,t.yg)("li",{parentName:"ul"},"Creating Jira tickets for governance issues"),(0,t.yg)("li",{parentName:"ul"},"Updating external catalogs (e.g., Alation, Collibra)")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Rare Actions use cases that MIGHT need PySpark:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Custom actions that programmatically trigger S3 profiling"),(0,t.yg)("li",{parentName:"ul"},"Actions that directly process data lake files (not typical)")),(0,t.yg)("h2",{id:"benefits-summary"},"Benefits Summary"),(0,t.yg)("p",null,"\u2705 ",(0,t.yg)("strong",{parentName:"p"},"Backward compatible"),": Standard ",(0,t.yg)("inlineCode",{parentName:"p"},"s3")," extra unchanged, existing users unaffected\n\u2705 ",(0,t.yg)("strong",{parentName:"p"},"Smaller installations"),": Save ~500MB with ",(0,t.yg)("inlineCode",{parentName:"p"},"s3-slim"),"\n\u2705 ",(0,t.yg)("strong",{parentName:"p"},"Faster setup"),": No PySpark compilation with ",(0,t.yg)("inlineCode",{parentName:"p"},"s3-slim"),"\n\u2705 ",(0,t.yg)("strong",{parentName:"p"},"Flexible deployment"),": Choose based on profiling needs\n\u2705 ",(0,t.yg)("strong",{parentName:"p"},"Type safety maintained"),": Refactored with proper code layer separation (mypy passes)\n\u2705 ",(0,t.yg)("strong",{parentName:"p"},"Clear error messages"),": Runtime errors guide users to correct installation\n\u2705 ",(0,t.yg)("strong",{parentName:"p"},"Actions-friendly"),": DataHub Actions benefits from reduced footprint with ",(0,t.yg)("inlineCode",{parentName:"p"},"s3-slim")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Key Takeaways:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Use ",(0,t.yg)("inlineCode",{parentName:"li"},"s3")," if you need S3 profiling, ",(0,t.yg)("inlineCode",{parentName:"li"},"s3-slim")," if you don't"),(0,t.yg)("li",{parentName:"ul"},"Pattern can be applied to other sources (ABS, etc.) in future PRs"),(0,t.yg)("li",{parentName:"ul"},"Existing installations continue working without changes")))}m.isMDXComponent=!0}}]);