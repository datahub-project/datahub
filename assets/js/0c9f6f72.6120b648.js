"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[9494],{87225:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>u,contentTitle:()=>c,default:()=>f,frontMatter:()=>p,metadata:()=>g,toc:()=>m});a(96540);var t=a(15680),l=a(53720),r=a(5400);function i(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function o(e,n){return n=null!=n?n:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):function(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}(Object(n)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))})),e}function s(e,n){if(null==e)return{};var a,t,l=function(e,n){if(null==e)return{};var a,t,l={},r=Object.keys(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||(l[a]=e[a]);return l}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(l[a]=e[a])}return l}const p={sidebar_position:42,title:"Kafka Connect",slug:"/generated/ingestion/sources/kafka-connect",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/generated/ingestion/sources/kafka-connect.md"},c="Kafka Connect",g={unversionedId:"docs/generated/ingestion/sources/kafka-connect",id:"docs/generated/ingestion/sources/kafka-connect",title:"Kafka Connect",description:"Integration Details",source:"@site/genDocs/docs/generated/ingestion/sources/kafka-connect.md",sourceDirName:"docs/generated/ingestion/sources",slug:"/generated/ingestion/sources/kafka-connect",permalink:"/docs/generated/ingestion/sources/kafka-connect",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/generated/ingestion/sources/kafka-connect.md",tags:[],version:"current",sidebarPosition:42,frontMatter:{sidebar_position:42,title:"Kafka Connect",slug:"/generated/ingestion/sources/kafka-connect",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/generated/ingestion/sources/kafka-connect.md"},sidebar:"overviewSidebar",previous:{title:"Kafka",permalink:"/docs/generated/ingestion/sources/kafka"},next:{title:"LDAP",permalink:"/docs/generated/ingestion/sources/ldap"}},u={},m=[{value:"Integration Details",id:"integration-details",level:2},{value:"Requirements",id:"requirements",level:3},{value:"Environment Support",id:"environment-support",level:3},{value:"Self-hosted Kafka Connect",id:"self-hosted-kafka-connect",level:4},{value:"Confluent Cloud",id:"confluent-cloud",level:4},{value:"Concept Mapping",id:"concept-mapping",level:3},{value:"Supported Connectors and Lineage Extraction",id:"supported-connectors-and-lineage-extraction",level:2},{value:"Source Connectors",id:"source-connectors",level:3},{value:"Sink Connectors",id:"sink-connectors",level:3},{value:"Supported Transforms",id:"supported-transforms",level:3},{value:"Topic Routing Transforms",id:"topic-routing-transforms",level:4},{value:"Non-Topic Routing Transforms",id:"non-topic-routing-transforms",level:4},{value:"Transform Pipeline Strategy",id:"transform-pipeline-strategy",level:4},{value:"Capabilities and Limitations",id:"capabilities-and-limitations",level:2},{value:"Transform Pipeline Support",id:"transform-pipeline-support",level:3},{value:"Environment-Specific Behavior",id:"environment-specific-behavior",level:3},{value:"Self-hosted Kafka Connect",id:"self-hosted-kafka-connect-1",level:4},{value:"Confluent Cloud",id:"confluent-cloud-1",level:4},{value:"Configuration Control",id:"configuration-control",level:3},{value:"Advanced Scenarios",id:"advanced-scenarios",level:3},{value:"Important Capabilities",id:"important-capabilities",level:3},{value:"CLI based Ingestion",id:"cli-based-ingestion",level:3},{value:"Starter Recipe",id:"starter-recipe",level:3},{value:"Config Details",id:"config-details",level:3},{value:"Advanced Configurations",id:"advanced-configurations",level:2},{value:"Environment-Specific Topic Discovery",id:"environment-specific-topic-discovery",level:3},{value:"Self-hosted Kafka Connect",id:"self-hosted-kafka-connect-2",level:4},{value:"Confluent Cloud",id:"confluent-cloud-2",level:4},{value:"Air-gapped or Performance-Optimized Environments",id:"air-gapped-or-performance-optimized-environments",level:4},{value:"Enhanced Topic Resolution for Source and Sink Connectors",id:"enhanced-topic-resolution-for-source-and-sink-connectors",level:3},{value:"How It Works",id:"how-it-works",level:4},{value:"Configuration Examples",id:"configuration-examples",level:4},{value:"Key Benefits",id:"key-benefits",level:4},{value:"When DataHub Topic Querying is Used",id:"when-datahub-topic-querying-is-used",level:4},{value:"Using DataHub Schema Resolver for Pattern Expansion and Column-Level Lineage",id:"using-datahub-schema-resolver-for-pattern-expansion-and-column-level-lineage",level:3},{value:"Auto-Enabled for Confluent Cloud",id:"auto-enabled-for-confluent-cloud",level:4},{value:"Configuration Overview",id:"configuration-overview",level:4},{value:"Pattern Expansion",id:"pattern-expansion",level:4},{value:"Column-Level Lineage",id:"column-level-lineage",level:4},{value:"Complete Configuration Example",id:"complete-configuration-example",level:4},{value:"Performance Impact",id:"performance-impact",level:4},{value:"Troubleshooting",id:"troubleshooting",level:4},{value:"Working with Platform Instances",id:"working-with-platform-instances",level:3},{value:"Example - Multiple MySQL Source Connectors each reading from different mysql instance",id:"example---multiple-mysql-source-connectors-each-reading-from-different-mysql-instance",level:4},{value:"Example - Multiple MySQL Source Connectors each reading from difference mysql instance and writing to different kafka cluster",id:"example---multiple-mysql-source-connectors-each-reading-from-difference-mysql-instance-and-writing-to-different-kafka-cluster",level:4},{value:"Example - Multiple BigQuery Sink Connectors each writing to different kafka cluster",id:"example---multiple-bigquery-sink-connectors-each-writing-to-different-kafka-cluster",level:4},{value:"Provided Configurations from External Sources",id:"provided-configurations-from-external-sources",level:3},{value:"Troubleshooting",id:"troubleshooting-1",level:2},{value:"Topic Discovery Issues",id:"topic-discovery-issues",level:3},{value:"Environment-Specific Issues",id:"environment-specific-issues",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Code Coordinates",id:"code-coordinates",level:3}],d={toc:m},y="wrapper";function f(e){var{components:n}=e,a=s(e,["components"]);return(0,t.yg)(y,o(function(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{},t=Object.keys(a);"function"==typeof Object.getOwnPropertySymbols&&(t=t.concat(Object.getOwnPropertySymbols(a).filter((function(e){return Object.getOwnPropertyDescriptor(a,e).enumerable})))),t.forEach((function(n){i(e,n,a[n])}))}return e}({},d,a),{components:n,mdxType:"MDXLayout"}),(0,t.yg)("h1",{id:"kafka-connect"},"Kafka Connect"),(0,t.yg)("h2",{id:"integration-details"},"Integration Details"),(0,t.yg)("p",null,"This plugin extracts the following:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Source and Sink Connectors in Kafka Connect as Data Pipelines"),(0,t.yg)("li",{parentName:"ul"},"For Source connectors - Data Jobs to represent lineage information between source dataset to Kafka topic per ",(0,t.yg)("inlineCode",{parentName:"li"},"{connector_name}:{source_dataset}")," combination"),(0,t.yg)("li",{parentName:"ul"},"For Sink connectors - Data Jobs to represent lineage information between Kafka topic to destination dataset per ",(0,t.yg)("inlineCode",{parentName:"li"},"{connector_name}:{topic}")," combination")),(0,t.yg)("h3",{id:"requirements"},"Requirements"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Java Runtime Dependency:")),(0,t.yg)("p",null,"This source requires Java to be installed and available on the system for transform pipeline support (RegexRouter, etc.). The Java runtime is accessed via JPype to enable Java regex pattern matching that's compatible with Kafka Connect transforms."),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Python installations"),": Install Java separately (e.g., ",(0,t.yg)("inlineCode",{parentName:"li"},"apt-get install openjdk-11-jre-headless")," on Debian/Ubuntu)"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Docker deployments"),": Ensure your DataHub ingestion Docker image includes a Java runtime. The official DataHub images include Java by default."),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Impact"),": Without Java, transform pipeline features will be disabled and lineage accuracy may be reduced for connectors using transforms")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Note for Docker users"),": If you're building custom Docker images for DataHub ingestion, ensure a Java Runtime Environment (JRE) is included in your image to support full transform pipeline functionality."),(0,t.yg)("h3",{id:"environment-support"},"Environment Support"),(0,t.yg)("p",null,"DataHub's Kafka Connect source supports both ",(0,t.yg)("strong",{parentName:"p"},"self-hosted")," and ",(0,t.yg)("strong",{parentName:"p"},"Confluent Cloud")," environments with automatic detection and environment-specific topic retrieval strategies:"),(0,t.yg)("h4",{id:"self-hosted-kafka-connect"},"Self-hosted Kafka Connect"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Topic Discovery"),": Uses runtime ",(0,t.yg)("inlineCode",{parentName:"li"},"/connectors/{name}/topics")," API endpoint"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Accuracy"),": Returns actual topics that connectors are currently reading from/writing to"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Benefits"),": Most accurate topic information as it reflects actual runtime state"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Requirements"),": Standard Kafka Connect REST API access")),(0,t.yg)("h4",{id:"confluent-cloud"},"Confluent Cloud"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Topic Discovery"),": Uses comprehensive Kafka REST API v3 for optimal transform pipeline support with config-based fallback"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Method"),": Gets all topics from Kafka cluster via REST API, applies reverse transform pipeline for accurate mappings"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Transform Support"),": Full support for complex transform pipelines via reverse pipeline strategy using actual cluster topics"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Fallback"),": Falls back to config-based derivation if Kafka API is unavailable")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Environment Detection"),": Automatically detects environment based on ",(0,t.yg)("inlineCode",{parentName:"p"},"connect_uri")," patterns containing ",(0,t.yg)("inlineCode",{parentName:"p"},"confluent.cloud"),"."),(0,t.yg)("h3",{id:"concept-mapping"},"Concept Mapping"),(0,t.yg)("p",null,"This ingestion source maps the following Source System Concepts to DataHub Concepts:"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Source Concept"),(0,t.yg)("th",{parentName:"tr",align:null},"DataHub Concept"),(0,t.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("inlineCode",{parentName:"td"},'"kafka-connect"')),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("a",{parentName:"td",href:"/docs/generated/metamodel/entities/dataplatform/"},"Data Platform")),(0,t.yg)("td",{parentName:"tr",align:null})),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("a",{parentName:"td",href:"https://kafka.apache.org/documentation/#connect_connectorsandtasks"},"Connector")),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("a",{parentName:"td",href:"/docs/generated/metamodel/entities/dataflow/"},"DataFlow")),(0,t.yg)("td",{parentName:"tr",align:null})),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Kafka Topic"),(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("a",{parentName:"td",href:"/docs/generated/metamodel/entities/dataset/"},"Dataset")),(0,t.yg)("td",{parentName:"tr",align:null})))),(0,t.yg)("h2",{id:"supported-connectors-and-lineage-extraction"},"Supported Connectors and Lineage Extraction"),(0,t.yg)("p",null,"DataHub supports different connector types with varying levels of lineage extraction capabilities depending on the environment (self-hosted vs Confluent Cloud):"),(0,t.yg)("h3",{id:"source-connectors"},"Source Connectors"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Connector Type"),(0,t.yg)("th",{parentName:"tr",align:null},"Self-hosted Support"),(0,t.yg)("th",{parentName:"tr",align:null},"Confluent Cloud Support"),(0,t.yg)("th",{parentName:"tr",align:null},"Topic Discovery Method"),(0,t.yg)("th",{parentName:"tr",align:null},"Lineage Extraction"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Platform JDBC Source"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"io.confluent.connect.jdbc.JdbcSourceConnector")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Table \u2192 Topic mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Cloud PostgreSQL CDC"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"PostgresCdcSource")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Table \u2192 Topic mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Cloud PostgreSQL CDC V2"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"PostgresCdcSourceV2")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Table \u2192 Topic mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Cloud MySQL Source"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"MySqlSource")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Table \u2192 Topic mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Cloud MySQL CDC"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"MySqlCdcSource")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Table \u2192 Topic mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Debezium MySQL"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"io.debezium.connector.mysql.MySqlConnector")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Partial"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Database \u2192 Topic CDC mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Debezium PostgreSQL"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"io.debezium.connector.postgresql.PostgresConnector")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Partial"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Database \u2192 Topic CDC mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Debezium SQL Server"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"io.debezium.connector.sqlserver.SqlServerConnector")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Partial"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Database \u2192 Topic CDC mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Debezium Oracle"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"io.debezium.connector.oracle.OracleConnector")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Partial"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Database \u2192 Topic CDC mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Debezium DB2"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"io.debezium.connector.db2.Db2Connector")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Partial"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Database \u2192 Topic CDC mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Debezium MongoDB"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"io.debezium.connector.mongodb.MongoDbConnector")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Partial"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Collection \u2192 Topic CDC mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Debezium Vitess"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"io.debezium.connector.vitess.VitessConnector")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Partial"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Table \u2192 Topic CDC mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"MongoDB Source"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"com.mongodb.kafka.connect.MongoSourceConnector")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\ud83d\udd27 Config Required"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Manual config"),(0,t.yg)("td",{parentName:"tr",align:null},"Collection \u2192 Topic mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Generic Connectors")),(0,t.yg)("td",{parentName:"tr",align:null},"\ud83d\udd27 Config Required"),(0,t.yg)("td",{parentName:"tr",align:null},"\ud83d\udd27 Config Required"),(0,t.yg)("td",{parentName:"tr",align:null},"User-defined mapping"),(0,t.yg)("td",{parentName:"tr",align:null},"Custom lineage mapping")))),(0,t.yg)("h3",{id:"sink-connectors"},"Sink Connectors"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Connector Type"),(0,t.yg)("th",{parentName:"tr",align:null},"Self-hosted Support"),(0,t.yg)("th",{parentName:"tr",align:null},"Confluent Cloud Support"),(0,t.yg)("th",{parentName:"tr",align:null},"Topic Discovery Method"),(0,t.yg)("th",{parentName:"tr",align:null},"Lineage Extraction"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"BigQuery Sink"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"com.wepay.kafka.connect.bigquery.BigQuerySinkConnector")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Topic \u2192 Table mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"S3 Sink"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"io.confluent.connect.s3.S3SinkConnector")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Topic \u2192 S3 object mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Snowflake Sink"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"com.snowflake.kafka.connector.SnowflakeSinkConnector")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Topic \u2192 Table mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Cloud PostgreSQL Sink"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"PostgresSink")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Topic \u2192 Table mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Cloud MySQL Sink"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"MySqlSink")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Topic \u2192 Table mapping")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("strong",{parentName:"td"},"Cloud Snowflake Sink"),(0,t.yg)("br",null),(0,t.yg)("inlineCode",{parentName:"td"},"SnowflakeSink")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705 Full"),(0,t.yg)("td",{parentName:"tr",align:null},"Runtime API / Config-based"),(0,t.yg)("td",{parentName:"tr",align:null},"Topic \u2192 Table mapping")))),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Legend:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Full"),": Complete lineage extraction with accurate topic discovery"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Partial"),": Lineage extraction supported but topic discovery may be limited (config-based only)"),(0,t.yg)("li",{parentName:"ul"},"\ud83d\udd27 ",(0,t.yg)("strong",{parentName:"li"},"Config Required"),": Requires ",(0,t.yg)("inlineCode",{parentName:"li"},"generic_connectors")," configuration for lineage mapping")),(0,t.yg)("h3",{id:"supported-transforms"},"Supported Transforms"),(0,t.yg)("p",null,"DataHub uses an ",(0,t.yg)("strong",{parentName:"p"},"advanced transform pipeline strategy")," that automatically handles complex transform chains by applying the complete pipeline to all topics and checking if results exist. This provides robust support for any combination of transforms."),(0,t.yg)("h4",{id:"topic-routing-transforms"},"Topic Routing Transforms"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"RegexRouter"),": ",(0,t.yg)("inlineCode",{parentName:"li"},"org.apache.kafka.connect.transforms.RegexRouter")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Cloud RegexRouter"),": ",(0,t.yg)("inlineCode",{parentName:"li"},"io.confluent.connect.cloud.transforms.TopicRegexRouter")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Debezium EventRouter"),": ",(0,t.yg)("inlineCode",{parentName:"li"},"io.debezium.transforms.outbox.EventRouter")," (Outbox pattern)")),(0,t.yg)("h4",{id:"non-topic-routing-transforms"},"Non-Topic Routing Transforms"),(0,t.yg)("p",null,"DataHub recognizes but passes through these transforms (they don't affect lineage):"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"InsertField, ReplaceField, MaskField, ValueToKey, HoistField, ExtractField"),(0,t.yg)("li",{parentName:"ul"},"SetSchemaMetadata, Flatten, Cast, HeadersFrom, TimestampConverter"),(0,t.yg)("li",{parentName:"ul"},"Filter, InsertHeader, DropHeaders, Drop, TombstoneHandler")),(0,t.yg)("h4",{id:"transform-pipeline-strategy"},"Transform Pipeline Strategy"),(0,t.yg)("p",null,"DataHub uses an improved ",(0,t.yg)("strong",{parentName:"p"},"reverse transform pipeline approach")," that:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Takes all actual topics")," from the connector manifest/Kafka cluster"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Applies the complete transform pipeline")," to each topic"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Checks if transformed results exist")," in the actual topic list"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Creates lineage mappings")," only for successful matches")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Benefits:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Works with any transform combination")," (single or chained transforms)"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Handles complex scenarios")," like EventRouter + RegexRouter chains"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Uses actual topics as source of truth")," (no prediction needed)"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Future-proof")," for new transform types"),(0,t.yg)("li",{parentName:"ul"},"\u2705 ",(0,t.yg)("strong",{parentName:"li"},"Works identically")," for both self-hosted and Confluent Cloud environments")),(0,t.yg)("h2",{id:"capabilities-and-limitations"},"Capabilities and Limitations"),(0,t.yg)("h3",{id:"transform-pipeline-support"},"Transform Pipeline Support"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"\u2705 Fully Supported:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Any combination of transforms"),": RegexRouter, EventRouter, and non-routing transforms"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Complex transform chains"),": Multiple chained transforms automatically handled"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Both environments"),": Self-hosted and Confluent Cloud work identically"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Future-proof"),": New transform types automatically supported")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"\u26a0\ufe0f Considerations:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"For connectors not listed in the supported connector table above, use the ",(0,t.yg)("inlineCode",{parentName:"li"},"generic_connectors")," configuration to provide explicit lineage mappings"),(0,t.yg)("li",{parentName:"ul"},"Some advanced connector-specific features may not be fully supported")),(0,t.yg)("h3",{id:"environment-specific-behavior"},"Environment-Specific Behavior"),(0,t.yg)("h4",{id:"self-hosted-kafka-connect-1"},"Self-hosted Kafka Connect"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Topic Discovery"),": Uses runtime ",(0,t.yg)("inlineCode",{parentName:"li"},"/connectors/{name}/topics")," API endpoint for maximum accuracy"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Requirements"),": Standard Kafka Connect REST API access"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Fallback"),": If runtime API fails, falls back to config-based derivation")),(0,t.yg)("h4",{id:"confluent-cloud-1"},"Confluent Cloud"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Topic Discovery"),": Uses comprehensive Kafka REST API v3 to get all topics, with automatic credential reuse"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Transform Support"),": Full support for all transform combinations via reverse pipeline strategy using actual cluster topics"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Auto-derivation"),": Automatically derives Kafka REST endpoint from connector configurations")),(0,t.yg)("h3",{id:"configuration-control"},"Configuration Control"),(0,t.yg)("p",null,"The ",(0,t.yg)("inlineCode",{parentName:"p"},"use_connect_topics_api")," flag controls topic retrieval behavior:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"When ",(0,t.yg)("inlineCode",{parentName:"strong"},"true")," (default)"),": Uses environment-specific topic discovery with full transform support"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"When ",(0,t.yg)("inlineCode",{parentName:"strong"},"false")),": Disables all topic discovery for air-gapped environments or performance optimization")),(0,t.yg)("h3",{id:"advanced-scenarios"},"Advanced Scenarios"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Complex Transform Chains:"),"\nThe new reverse transform pipeline strategy handles complex scenarios automatically:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},'# Example: EventRouter + RegexRouter chain\ntransforms: EventRouter,RegexRouter\ntransforms.EventRouter.type: io.debezium.transforms.outbox.EventRouter\ntransforms.RegexRouter.type: org.apache.kafka.connect.transforms.RegexRouter\ntransforms.RegexRouter.regex: "outbox\\\\.event\\\\.(.*)"\ntransforms.RegexRouter.replacement: "events.$1"\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Fallback Options:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"If transform pipeline cannot determine mappings, DataHub falls back to simple topic-based lineage"),(0,t.yg)("li",{parentName:"ul"},"For unsupported connector types or complex custom scenarios, use ",(0,t.yg)("inlineCode",{parentName:"li"},"generic_connectors")," configuration")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Performance Optimization:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Set ",(0,t.yg)("inlineCode",{parentName:"li"},"use_connect_topics_api: false")," to disable topic discovery in air-gapped environments"),(0,t.yg)("li",{parentName:"ul"},"Transform pipeline processing adds minimal overhead and improves lineage accuracy\n",(0,t.yg)("img",{parentName:"li",src:"https://img.shields.io/badge/support%20status-certified-brightgreen",alt:"Certified"}))),(0,t.yg)("h3",{id:"important-capabilities"},"Important Capabilities"),(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:null},"Capability"),(0,t.yg)("th",{parentName:"tr",align:null},"Status"),(0,t.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("a",{parentName:"td",href:"/docs/metadata-ingestion/docs/dev_guides/stateful#stale-entity-removal"},"Detect Deleted Entities")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Enabled by default via stateful ingestion.")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},(0,t.yg)("a",{parentName:"td",href:"/docs/platform-instances"},"Platform Instance")),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Enabled by default.")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Schema Metadata"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Enabled by default.")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:null},"Table-Level Lineage"),(0,t.yg)("td",{parentName:"tr",align:null},"\u2705"),(0,t.yg)("td",{parentName:"tr",align:null},"Enabled by default.")))),(0,t.yg)("h3",{id:"cli-based-ingestion"},"CLI based Ingestion"),(0,t.yg)("h3",{id:"starter-recipe"},"Starter Recipe"),(0,t.yg)("p",null,"Check out the following recipe to get started with ingestion! See ",(0,t.yg)("a",{parentName:"p",href:"#config-details"},"below")," for full configuration options."),(0,t.yg)("p",null,"For general pointers on writing and running a recipe, see our ",(0,t.yg)("a",{parentName:"p",href:"/docs/metadata-ingestion#recipes"},"main recipe guide"),"."),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},'source:\n  type: "kafka-connect"\n  config:\n    # Coordinates\n    connect_uri: "http://localhost:8083"\n\n    # Credentials\n    username: admin\n    password: password\n\n    # Optional\n    # Platform instance mapping to use when constructing URNs.\n    # Use if single instance of platform is referred across connectors.\n    platform_instance_map:\n      mysql: mysql_platform_instance\n\nsink:\n  # sink configs\n\n')),(0,t.yg)("h3",{id:"config-details"},"Config Details"),(0,t.yg)(l.A,{mdxType:"Tabs"},(0,t.yg)(r.A,{value:"options",label:"Options",default:!0,mdxType:"TabItem"},(0,t.yg)("p",null,"Note that a ",(0,t.yg)("inlineCode",{parentName:"p"},".")," is used to denote nested fields in the YAML recipe."),(0,t.yg)("div",{className:"config-table"},(0,t.yg)("table",null,(0,t.yg)("thead",{parentName:"table"},(0,t.yg)("tr",{parentName:"thead"},(0,t.yg)("th",{parentName:"tr",align:"left"},"Field"),(0,t.yg)("th",{parentName:"tr",align:"left"},"Description"))),(0,t.yg)("tbody",{parentName:"table"},(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"cluster_name"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of string, null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Cluster to ingest from. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"connect-cluster")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"confluent_cloud_cluster_id"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of string, null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Confluent Cloud Kafka Connect cluster ID (e.g., 'lkc-abc123'). When specified along with confluent_cloud_environment_id, the connect_uri will be automatically constructed. This is the recommended approach for Confluent Cloud instead of manually constructing the full URI. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"None")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"confluent_cloud_environment_id"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of string, null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Confluent Cloud environment ID (e.g., 'env-xyz123'). When specified along with confluent_cloud_cluster_id, the connect_uri will be automatically constructed. This is the recommended approach for Confluent Cloud instead of manually constructing the full URI. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"None")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"connect_to_platform_map"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of string, null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Platform instance mapping when multiple instances for a platform is available. Entry for a platform should be in either ",(0,t.yg)("inlineCode",{parentName:"td"},"platform_instance_map")," or ",(0,t.yg)("inlineCode",{parentName:"td"},"connect_to_platform_map"),". e.g.",(0,t.yg)("inlineCode",{parentName:"td"},'connect_to_platform_map: { "postgres-connector-finance-db": "postgres": "core_finance_instance" }')," ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"None")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"connect_uri"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"string"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"URI to connect to. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"http://localhost:8083/")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"convert_lineage_urns_to_lowercase"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"boolean"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Whether to convert the urns of ingested lineage dataset to lowercase ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"False")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"kafka_api_key"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of string, null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Optional: Confluent Cloud Kafka API key for authenticating with Kafka REST API v3. If not specified, DataHub will reuse the Connect credentials (username/password) for Kafka API authentication. Only needed if you want to use separate credentials for the Kafka API. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"None")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"kafka_api_secret"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of string(password), null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Optional: Confluent Cloud Kafka API secret for authenticating with Kafka REST API v3. If not specified, DataHub will reuse the Connect credentials (username/password) for Kafka API authentication. Only needed if you want to use separate credentials for the Kafka API. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"None")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"kafka_rest_endpoint"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of string, null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Optional: Confluent Cloud Kafka REST API endpoint for comprehensive topic retrieval. Format: ",(0,t.yg)("a",{parentName:"td",href:"https://pkc-xxxxx.region.provider.confluent.cloud"},"https://pkc-xxxxx.region.provider.confluent.cloud")," If not specified, DataHub automatically derives the endpoint from connector configurations (kafka.endpoint). When available, enables getting all topics from Kafka cluster for improved transform pipeline accuracy. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"None")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"password"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of string(password), null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Kafka Connect password. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"None")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"platform_instance"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of string, null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"The instance of the platform that all assets produced by this recipe belong to. This should be unique within the platform. See ",(0,t.yg)("a",{parentName:"td",href:"https://docs.datahub.com/docs/platform-instances/"},"https://docs.datahub.com/docs/platform-instances/")," for more details. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"None")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"platform_instance_map"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of string, null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Platform instance mapping to use when constructing URNs. e.g.",(0,t.yg)("inlineCode",{parentName:"td"},'platform_instance_map: { "hive": "warehouse" }')," ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"None")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"schema_resolver_expand_patterns"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of boolean, null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Enable table pattern expansion using DataHub schema metadata. When use_schema_resolver=True, this controls whether to expand patterns like 'database.*' to actual table names by querying DataHub. Only applies when use_schema_resolver is enabled. Defaults to True when use_schema_resolver is enabled. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"None")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"schema_resolver_finegrained_lineage"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of boolean, null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Enable fine-grained (column-level) lineage extraction using DataHub schema metadata. When use_schema_resolver=True, this controls whether to generate column-level lineage by matching schemas between source tables and Kafka topics. Only applies when use_schema_resolver is enabled. Defaults to True when use_schema_resolver is enabled. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"None")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"use_connect_topics_api"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"boolean"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Whether to use Kafka Connect API for topic retrieval and validation. This flag controls the environment-specific topic retrieval strategy:  ",(0,t.yg)("br",null)," ",(0,t.yg)("strong",{parentName:"td"},"When True (default):")," - ",(0,t.yg)("strong",{parentName:"td"},"Self-hosted environments:")," Uses runtime ",(0,t.yg)("inlineCode",{parentName:"td"},"/connectors/{name}/topics")," API for accurate topic information - ",(0,t.yg)("strong",{parentName:"td"},"Confluent Cloud:")," Uses comprehensive Kafka REST API v3 to get all topics for transform pipeline, with config-based fallback  ",(0,t.yg)("br",null)," ",(0,t.yg)("strong",{parentName:"td"},"When False:")," Disables all API-based topic retrieval for both environments. Returns empty topic lists. Useful for air-gapped environments or when topic validation isn't needed for performance optimization. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"True")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"use_schema_resolver"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"boolean"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Use DataHub's schema metadata to enhance Kafka Connect connector lineage. When enabled (requires DataHub graph connection): 1) Expands table patterns (e.g., 'database.*') to actual tables using DataHub metadata 2) Generates fine-grained column-level lineage for Kafka Connect sources/sinks.  ",(0,t.yg)("br",null),"  ",(0,t.yg)("br",null)," ",(0,t.yg)("strong",{parentName:"td"},"Auto-enabled for Confluent Cloud:")," This feature is automatically enabled for Confluent Cloud environments where DataHub graph connection is required. Set ",(0,t.yg)("inlineCode",{parentName:"td"},"use_schema_resolver: false")," to disable.  ",(0,t.yg)("br",null),"  ",(0,t.yg)("br",null)," ",(0,t.yg)("strong",{parentName:"td"},"Prerequisite:")," Source database tables must be ingested into DataHub before Kafka Connect ingestion for this feature to work. Without prior database ingestion, schema resolver will not find table metadata. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"False")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"username"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of string, null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Kafka Connect username. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"None")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"env"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"string"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"The environment that all assets produced by this connector belong to ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"PROD")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"connector_patterns"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"AllowDenyPattern"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"A class to store allow deny regexes")),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"connector_patterns."),(0,t.yg)("span",{className:"path-main"},"ignoreCase"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of boolean, null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Whether to ignore case sensitivity during pattern matching. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"True")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"connector_patterns."),(0,t.yg)("span",{className:"path-main"},"allow"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"array"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"List of regex patterns to include in ingestion ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"[","'",".","*","'","]")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"connector_patterns.allow."),(0,t.yg)("span",{className:"path-main"},"string"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"string"))),(0,t.yg)("td",{parentName:"tr",align:"left"})),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"connector_patterns."),(0,t.yg)("span",{className:"path-main"},"deny"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"array"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"List of regex patterns to exclude from ingestion. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"[","]")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"connector_patterns.deny."),(0,t.yg)("span",{className:"path-main"},"string"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"string"))),(0,t.yg)("td",{parentName:"tr",align:"left"})),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"generic_connectors"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"array"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Provide lineage graph for sources connectors other than Confluent JDBC Source Connector, Debezium Source Connector, and Mongo Source Connector ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"[","]")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"generic_connectors."),(0,t.yg)("span",{className:"path-main"},"GenericConnectorConfig"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"GenericConnectorConfig"))),(0,t.yg)("td",{parentName:"tr",align:"left"})),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"generic_connectors.GenericConnectorConfig."),(0,t.yg)("span",{className:"path-main"},"connector_name"),"\xa0",(0,t.yg)("abbr",{title:"Required if GenericConnectorConfig is set"},"\u2753"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"string"))),(0,t.yg)("td",{parentName:"tr",align:"left"})),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"generic_connectors.GenericConnectorConfig."),(0,t.yg)("span",{className:"path-main"},"source_dataset"),"\xa0",(0,t.yg)("abbr",{title:"Required if GenericConnectorConfig is set"},"\u2753"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"string"))),(0,t.yg)("td",{parentName:"tr",align:"left"})),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"generic_connectors.GenericConnectorConfig."),(0,t.yg)("span",{className:"path-main"},"source_platform"),"\xa0",(0,t.yg)("abbr",{title:"Required if GenericConnectorConfig is set"},"\u2753"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"string"))),(0,t.yg)("td",{parentName:"tr",align:"left"})),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"provided_configs"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of array, null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Provided Configurations ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"None")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"provided_configs."),(0,t.yg)("span",{className:"path-main"},"ProvidedConfig"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"ProvidedConfig"))),(0,t.yg)("td",{parentName:"tr",align:"left"})),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"provided_configs.ProvidedConfig."),(0,t.yg)("span",{className:"path-main"},"path_key"),"\xa0",(0,t.yg)("abbr",{title:"Required if ProvidedConfig is set"},"\u2753"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"string"))),(0,t.yg)("td",{parentName:"tr",align:"left"})),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"provided_configs.ProvidedConfig."),(0,t.yg)("span",{className:"path-main"},"provider"),"\xa0",(0,t.yg)("abbr",{title:"Required if ProvidedConfig is set"},"\u2753"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"string"))),(0,t.yg)("td",{parentName:"tr",align:"left"})),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"provided_configs.ProvidedConfig."),(0,t.yg)("span",{className:"path-main"},"value"),"\xa0",(0,t.yg)("abbr",{title:"Required if ProvidedConfig is set"},"\u2753"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"string"))),(0,t.yg)("td",{parentName:"tr",align:"left"})),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-main"},"stateful_ingestion"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"One of StatefulStaleMetadataRemovalConfig, null"))),(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"default-line "},"Default: ",(0,t.yg)("span",{className:"default-value"},"None")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"stateful_ingestion."),(0,t.yg)("span",{className:"path-main"},"enabled"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"boolean"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Whether or not to enable stateful ingest. Default: True if a pipeline_name is set and either a datahub-rest sink or ",(0,t.yg)("inlineCode",{parentName:"td"},"datahub_api")," is specified, otherwise False ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"False")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"stateful_ingestion."),(0,t.yg)("span",{className:"path-main"},"fail_safe_threshold"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"number"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Prevents large amount of soft deletes & the state from committing from accidental changes to the source configuration if the relative change percent in entities compared to the previous state is above the 'fail_safe_threshold'. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"75.0")))),(0,t.yg)("tr",{parentName:"tbody"},(0,t.yg)("td",{parentName:"tr",align:"left"},(0,t.yg)("div",{className:"path-line"},(0,t.yg)("span",{className:"path-prefix"},"stateful_ingestion."),(0,t.yg)("span",{className:"path-main"},"remove_stale_metadata"))," ",(0,t.yg)("div",{className:"type-name-line"},(0,t.yg)("span",{className:"type-name"},"boolean"))),(0,t.yg)("td",{parentName:"tr",align:"left"},"Soft-deletes the entities present in the last successful run but missing in the current run with stateful_ingestion enabled. ",(0,t.yg)("div",{className:"default-line default-line-with-docs"},"Default: ",(0,t.yg)("span",{className:"default-value"},"True")))))))),(0,t.yg)(r.A,{value:"schema",label:"Schema",mdxType:"TabItem"},(0,t.yg)("p",null,"The ",(0,t.yg)("a",{parentName:"p",href:"https://json-schema.org/"},"JSONSchema")," for this configuration is inlined below."),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-javascript"},'{\n  "$defs": {\n    "AllowDenyPattern": {\n      "additionalProperties": false,\n      "description": "A class to store allow deny regexes",\n      "properties": {\n        "allow": {\n          "default": [\n            ".*"\n          ],\n          "description": "List of regex patterns to include in ingestion",\n          "items": {\n            "type": "string"\n          },\n          "title": "Allow",\n          "type": "array"\n        },\n        "deny": {\n          "default": [],\n          "description": "List of regex patterns to exclude from ingestion.",\n          "items": {\n            "type": "string"\n          },\n          "title": "Deny",\n          "type": "array"\n        },\n        "ignoreCase": {\n          "anyOf": [\n            {\n              "type": "boolean"\n            },\n            {\n              "type": "null"\n            }\n          ],\n          "default": true,\n          "description": "Whether to ignore case sensitivity during pattern matching.",\n          "title": "Ignorecase"\n        }\n      },\n      "title": "AllowDenyPattern",\n      "type": "object"\n    },\n    "GenericConnectorConfig": {\n      "additionalProperties": false,\n      "properties": {\n        "connector_name": {\n          "title": "Connector Name",\n          "type": "string"\n        },\n        "source_dataset": {\n          "title": "Source Dataset",\n          "type": "string"\n        },\n        "source_platform": {\n          "title": "Source Platform",\n          "type": "string"\n        }\n      },\n      "required": [\n        "connector_name",\n        "source_dataset",\n        "source_platform"\n      ],\n      "title": "GenericConnectorConfig",\n      "type": "object"\n    },\n    "ProvidedConfig": {\n      "additionalProperties": false,\n      "properties": {\n        "provider": {\n          "title": "Provider",\n          "type": "string"\n        },\n        "path_key": {\n          "title": "Path Key",\n          "type": "string"\n        },\n        "value": {\n          "title": "Value",\n          "type": "string"\n        }\n      },\n      "required": [\n        "provider",\n        "path_key",\n        "value"\n      ],\n      "title": "ProvidedConfig",\n      "type": "object"\n    },\n    "StatefulStaleMetadataRemovalConfig": {\n      "additionalProperties": false,\n      "description": "Base specialized config for Stateful Ingestion with stale metadata removal capability.",\n      "properties": {\n        "enabled": {\n          "default": false,\n          "description": "Whether or not to enable stateful ingest. Default: True if a pipeline_name is set and either a datahub-rest sink or `datahub_api` is specified, otherwise False",\n          "title": "Enabled",\n          "type": "boolean"\n        },\n        "remove_stale_metadata": {\n          "default": true,\n          "description": "Soft-deletes the entities present in the last successful run but missing in the current run with stateful_ingestion enabled.",\n          "title": "Remove Stale Metadata",\n          "type": "boolean"\n        },\n        "fail_safe_threshold": {\n          "default": 75.0,\n          "description": "Prevents large amount of soft deletes & the state from committing from accidental changes to the source configuration if the relative change percent in entities compared to the previous state is above the \'fail_safe_threshold\'.",\n          "maximum": 100.0,\n          "minimum": 0.0,\n          "title": "Fail Safe Threshold",\n          "type": "number"\n        }\n      },\n      "title": "StatefulStaleMetadataRemovalConfig",\n      "type": "object"\n    }\n  },\n  "additionalProperties": false,\n  "properties": {\n    "stateful_ingestion": {\n      "anyOf": [\n        {\n          "$ref": "#/$defs/StatefulStaleMetadataRemovalConfig"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null\n    },\n    "env": {\n      "default": "PROD",\n      "description": "The environment that all assets produced by this connector belong to",\n      "title": "Env",\n      "type": "string"\n    },\n    "platform_instance_map": {\n      "anyOf": [\n        {\n          "additionalProperties": {\n            "type": "string"\n          },\n          "type": "object"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "Platform instance mapping to use when constructing URNs. e.g.`platform_instance_map: { \\"hive\\": \\"warehouse\\" }`",\n      "title": "Platform Instance Map"\n    },\n    "platform_instance": {\n      "anyOf": [\n        {\n          "type": "string"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "The instance of the platform that all assets produced by this recipe belong to. This should be unique within the platform. See https://docs.datahub.com/docs/platform-instances/ for more details.",\n      "title": "Platform Instance"\n    },\n    "connect_uri": {\n      "default": "http://localhost:8083/",\n      "description": "URI to connect to.",\n      "title": "Connect Uri",\n      "type": "string"\n    },\n    "username": {\n      "anyOf": [\n        {\n          "type": "string"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "Kafka Connect username.",\n      "title": "Username"\n    },\n    "password": {\n      "anyOf": [\n        {\n          "format": "password",\n          "type": "string",\n          "writeOnly": true\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "Kafka Connect password.",\n      "title": "Password"\n    },\n    "cluster_name": {\n      "anyOf": [\n        {\n          "type": "string"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": "connect-cluster",\n      "description": "Cluster to ingest from.",\n      "title": "Cluster Name"\n    },\n    "convert_lineage_urns_to_lowercase": {\n      "default": false,\n      "description": "Whether to convert the urns of ingested lineage dataset to lowercase",\n      "title": "Convert Lineage Urns To Lowercase",\n      "type": "boolean"\n    },\n    "connector_patterns": {\n      "$ref": "#/$defs/AllowDenyPattern",\n      "default": {\n        "allow": [\n          ".*"\n        ],\n        "deny": [],\n        "ignoreCase": true\n      },\n      "description": "regex patterns for connectors to filter for ingestion."\n    },\n    "provided_configs": {\n      "anyOf": [\n        {\n          "items": {\n            "$ref": "#/$defs/ProvidedConfig"\n          },\n          "type": "array"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "Provided Configurations",\n      "title": "Provided Configs"\n    },\n    "connect_to_platform_map": {\n      "anyOf": [\n        {\n          "additionalProperties": {\n            "additionalProperties": {\n              "type": "string"\n            },\n            "type": "object"\n          },\n          "type": "object"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "Platform instance mapping when multiple instances for a platform is available. Entry for a platform should be in either `platform_instance_map` or `connect_to_platform_map`. e.g.`connect_to_platform_map: { \\"postgres-connector-finance-db\\": \\"postgres\\": \\"core_finance_instance\\" }`",\n      "title": "Connect To Platform Map"\n    },\n    "generic_connectors": {\n      "default": [],\n      "description": "Provide lineage graph for sources connectors other than Confluent JDBC Source Connector, Debezium Source Connector, and Mongo Source Connector",\n      "items": {\n        "$ref": "#/$defs/GenericConnectorConfig"\n      },\n      "title": "Generic Connectors",\n      "type": "array"\n    },\n    "use_connect_topics_api": {\n      "default": true,\n      "description": "Whether to use Kafka Connect API for topic retrieval and validation. This flag controls the environment-specific topic retrieval strategy: \\n**When True (default):** - **Self-hosted environments:** Uses runtime `/connectors/{name}/topics` API for accurate topic information - **Confluent Cloud:** Uses comprehensive Kafka REST API v3 to get all topics for transform pipeline, with config-based fallback \\n**When False:** Disables all API-based topic retrieval for both environments. Returns empty topic lists. Useful for air-gapped environments or when topic validation isn\'t needed for performance optimization.",\n      "title": "Use Connect Topics Api",\n      "type": "boolean"\n    },\n    "kafka_rest_endpoint": {\n      "anyOf": [\n        {\n          "type": "string"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "Optional: Confluent Cloud Kafka REST API endpoint for comprehensive topic retrieval. Format: https://pkc-xxxxx.region.provider.confluent.cloud If not specified, DataHub automatically derives the endpoint from connector configurations (kafka.endpoint). When available, enables getting all topics from Kafka cluster for improved transform pipeline accuracy.",\n      "title": "Kafka Rest Endpoint"\n    },\n    "kafka_api_key": {\n      "anyOf": [\n        {\n          "type": "string"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "Optional: Confluent Cloud Kafka API key for authenticating with Kafka REST API v3. If not specified, DataHub will reuse the Connect credentials (username/password) for Kafka API authentication. Only needed if you want to use separate credentials for the Kafka API.",\n      "title": "Kafka Api Key"\n    },\n    "kafka_api_secret": {\n      "anyOf": [\n        {\n          "format": "password",\n          "type": "string",\n          "writeOnly": true\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "Optional: Confluent Cloud Kafka API secret for authenticating with Kafka REST API v3. If not specified, DataHub will reuse the Connect credentials (username/password) for Kafka API authentication. Only needed if you want to use separate credentials for the Kafka API.",\n      "title": "Kafka Api Secret"\n    },\n    "confluent_cloud_environment_id": {\n      "anyOf": [\n        {\n          "type": "string"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "Confluent Cloud environment ID (e.g., \'env-xyz123\'). When specified along with confluent_cloud_cluster_id, the connect_uri will be automatically constructed. This is the recommended approach for Confluent Cloud instead of manually constructing the full URI.",\n      "title": "Confluent Cloud Environment Id"\n    },\n    "confluent_cloud_cluster_id": {\n      "anyOf": [\n        {\n          "type": "string"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "Confluent Cloud Kafka Connect cluster ID (e.g., \'lkc-abc123\'). When specified along with confluent_cloud_environment_id, the connect_uri will be automatically constructed. This is the recommended approach for Confluent Cloud instead of manually constructing the full URI.",\n      "title": "Confluent Cloud Cluster Id"\n    },\n    "use_schema_resolver": {\n      "default": false,\n      "description": "Use DataHub\'s schema metadata to enhance Kafka Connect connector lineage. When enabled (requires DataHub graph connection): 1) Expands table patterns (e.g., \'database.*\') to actual tables using DataHub metadata 2) Generates fine-grained column-level lineage for Kafka Connect sources/sinks. \\n\\n**Auto-enabled for Confluent Cloud:** This feature is automatically enabled for Confluent Cloud environments where DataHub graph connection is required. Set `use_schema_resolver: false` to disable. \\n\\n**Prerequisite:** Source database tables must be ingested into DataHub before Kafka Connect ingestion for this feature to work. Without prior database ingestion, schema resolver will not find table metadata.",\n      "title": "Use Schema Resolver",\n      "type": "boolean"\n    },\n    "schema_resolver_expand_patterns": {\n      "anyOf": [\n        {\n          "type": "boolean"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "Enable table pattern expansion using DataHub schema metadata. When use_schema_resolver=True, this controls whether to expand patterns like \'database.*\' to actual table names by querying DataHub. Only applies when use_schema_resolver is enabled. Defaults to True when use_schema_resolver is enabled.",\n      "title": "Schema Resolver Expand Patterns"\n    },\n    "schema_resolver_finegrained_lineage": {\n      "anyOf": [\n        {\n          "type": "boolean"\n        },\n        {\n          "type": "null"\n        }\n      ],\n      "default": null,\n      "description": "Enable fine-grained (column-level) lineage extraction using DataHub schema metadata. When use_schema_resolver=True, this controls whether to generate column-level lineage by matching schemas between source tables and Kafka topics. Only applies when use_schema_resolver is enabled. Defaults to True when use_schema_resolver is enabled.",\n      "title": "Schema Resolver Finegrained Lineage"\n    }\n  },\n  "title": "KafkaConnectSourceConfig",\n  "type": "object"\n}\n')))),(0,t.yg)("h2",{id:"advanced-configurations"},"Advanced Configurations"),(0,t.yg)("h3",{id:"environment-specific-topic-discovery"},"Environment-Specific Topic Discovery"),(0,t.yg)("p",null,"DataHub's Kafka Connect source automatically detects your environment (self-hosted vs Confluent Cloud) and uses the appropriate topic discovery strategy:"),(0,t.yg)("h4",{id:"self-hosted-kafka-connect-2"},"Self-hosted Kafka Connect"),(0,t.yg)("p",null,"Uses the runtime ",(0,t.yg)("inlineCode",{parentName:"p"},"/connectors/{name}/topics")," API endpoint for accurate, real-time topic information:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'source:\n  type: kafka-connect\n  config:\n    # Self-hosted Kafka Connect cluster\n    connect_uri: "http://localhost:8083"\n    # use_connect_topics_api: true  # Default - enables runtime topic discovery\n')),(0,t.yg)("h4",{id:"confluent-cloud-2"},"Confluent Cloud"),(0,t.yg)("p",null,"Uses comprehensive transform pipeline support with Kafka REST API v3 topic validation and config-based fallback:"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Recommended approach using environment and cluster IDs:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'source:\n  type: kafka-connect\n  config:\n    # Auto-construct URI from environment and cluster IDs (recommended)\n    confluent_cloud_environment_id: "env-xyz123" # Your Confluent Cloud environment ID\n    confluent_cloud_cluster_id: "lkc-abc456" # Your Kafka Connect cluster ID\n\n    # Standard credentials for Kafka Connect API\n    username: "your-connect-api-key" # API key for Kafka Connect access\n    password: "your-connect-api-secret" # API secret for Kafka Connect access\n\n    # Optional: Separate credentials for Kafka REST API (if different from Connect API)\n    kafka_api_key: "your-kafka-api-key" # API key for Kafka REST API access\n    kafka_api_secret: "your-kafka-api-secret" # API secret for Kafka REST API access\n\n    # Optional: Dedicated Kafka REST endpoint for comprehensive topic retrieval\n    kafka_rest_endpoint: "https://pkc-xxxxx.region.provider.confluent.cloud"\n\n    # use_connect_topics_api: true  # Default - enables comprehensive topic retrieval\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Alternative approach using full URI (legacy):")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'source:\n  type: kafka-connect\n  config:\n    # Confluent Cloud Connect URI - automatically detected\n    connect_uri: "https://api.confluent.cloud/connect/v1/environments/env-123/clusters/lkc-abc456"\n    username: "your-connect-api-key" # API key for Kafka Connect\n    password: "your-connect-api-secret" # API secret for Kafka Connect\n    kafka_api_key: "your-kafka-api-key" # API key for Kafka REST API (if different)\n    kafka_api_secret: "your-kafka-api-secret" # API secret for Kafka REST API (if different)\n\n    # Optional: Dedicated Kafka REST endpoint for comprehensive topic retrieval\n    kafka_rest_endpoint: "https://pkc-xxxxx.region.provider.confluent.cloud"\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"How Lineage Inference Works with Transform Pipelines:")),(0,t.yg)("p",null,"Kafka Connect connectors can apply transforms (like RegexRouter) that modify topic names before data reaches Kafka. DataHub's lineage inference analyzes these transform configurations to determine how topics are produced:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Configuration Analysis")," - Extracts source tables from connector configuration (",(0,t.yg)("inlineCode",{parentName:"li"},"table.include.list"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"database.include.list"),")"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Transform Application")," - Applies configured transforms (RegexRouter, EventRouter, etc.) to predict final topic names"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Topic Validation")," - Validates predicted topics against actual cluster topics using Kafka REST API v3"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Lineage Construction")," - Maps source tables to validated topics, preserving schema information")),(0,t.yg)("p",null,"This approach works for both self-hosted and Confluent Cloud environments:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Self-hosted"),": Uses runtime ",(0,t.yg)("inlineCode",{parentName:"li"},"/connectors/{name}/topics")," API for actual topics produced by each connector"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Confluent Cloud"),": Uses Kafka REST API v3 to get all cluster topics, then applies transform pipeline to match with connector config")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Key Benefits"),":"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"90-95% accuracy")," for Cloud connectors with transforms (significant improvement over previous config-only approach)"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Full RegexRouter support")," with Java regex compatibility"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Complex transform chains")," handled correctly"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Schema preservation")," maintains full table names with schema information")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Configuration Options:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Environment/Cluster IDs (recommended)"),": Use ",(0,t.yg)("inlineCode",{parentName:"li"},"confluent_cloud_environment_id")," and ",(0,t.yg)("inlineCode",{parentName:"li"},"confluent_cloud_cluster_id")," for automatic URI construction"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Auto-derivation"),": DataHub finds Kafka REST endpoint automatically from connector configs"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Manual endpoint"),": Specify ",(0,t.yg)("inlineCode",{parentName:"li"},"kafka_rest_endpoint")," if auto-derivation doesn't work"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Separate credentials (typical)"),": Use ",(0,t.yg)("inlineCode",{parentName:"li"},"connect_api_key"),"/",(0,t.yg)("inlineCode",{parentName:"li"},"connect_api_secret")," for Connect API and ",(0,t.yg)("inlineCode",{parentName:"li"},"kafka_api_key"),"/",(0,t.yg)("inlineCode",{parentName:"li"},"kafka_api_secret")," for Kafka REST API"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Legacy credentials"),": Use ",(0,t.yg)("inlineCode",{parentName:"li"},"username"),"/",(0,t.yg)("inlineCode",{parentName:"li"},"password")," for Connect API (falls back for Kafka API if separate credentials not provided)")),(0,t.yg)("h4",{id:"air-gapped-or-performance-optimized-environments"},"Air-gapped or Performance-Optimized Environments"),(0,t.yg)("p",null,"Disable topic discovery entirely for environments where API access is not available or not needed:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'source:\n  type: kafka-connect\n  config:\n    connect_uri: "http://localhost:8083"\n    use_connect_topics_api: false # Disables all topic discovery API calls\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Note"),": When ",(0,t.yg)("inlineCode",{parentName:"p"},"use_connect_topics_api")," is ",(0,t.yg)("inlineCode",{parentName:"p"},"false"),", topic information will not be extracted, which may impact lineage accuracy but improves performance and works in air-gapped environments."),(0,t.yg)("h3",{id:"enhanced-topic-resolution-for-source-and-sink-connectors"},"Enhanced Topic Resolution for Source and Sink Connectors"),(0,t.yg)("p",null,"DataHub now provides intelligent topic resolution that works reliably across all environments, including Confluent Cloud where the Kafka Connect topics API is unavailable."),(0,t.yg)("h4",{id:"how-it-works"},"How It Works"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Source Connectors")," (Debezium, Snowflake CDC, JDBC):"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Always derive expected topics from connector configuration (",(0,t.yg)("inlineCode",{parentName:"li"},"table.include.list"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"database.include.list"),")"),(0,t.yg)("li",{parentName:"ul"},"Apply configured transforms (RegexRouter, EventRouter, etc.) to predict final topic names"),(0,t.yg)("li",{parentName:"ul"},"When Kafka API is available: Filter to only topics that exist in Kafka"),(0,t.yg)("li",{parentName:"ul"},"When Kafka API is unavailable (Confluent Cloud): Create lineages for all configured tables without filtering")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Sink Connectors")," (S3, Snowflake, BigQuery, JDBC):"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Support both explicit topic lists (",(0,t.yg)("inlineCode",{parentName:"li"},"topics")," field) and regex patterns (",(0,t.yg)("inlineCode",{parentName:"li"},"topics.regex")," field)"),(0,t.yg)("li",{parentName:"ul"},"When ",(0,t.yg)("inlineCode",{parentName:"li"},"topics.regex")," is used:",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},"Priority 1: Match against ",(0,t.yg)("inlineCode",{parentName:"li"},"manifest.topic_names")," from Kafka API (if available)"),(0,t.yg)("li",{parentName:"ul"},"Priority 2: Query DataHub for Kafka topics and match pattern (if ",(0,t.yg)("inlineCode",{parentName:"li"},"use_schema_resolver")," enabled)"),(0,t.yg)("li",{parentName:"ul"},"Priority 3: Warn user that pattern cannot be expanded")))),(0,t.yg)("h4",{id:"configuration-examples"},"Configuration Examples"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Source Connector with Pattern Expansion:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'# Debezium PostgreSQL source with wildcard tables\nconnector.config:\n  table.include.list: "public.analytics_.*"\n  # When Kafka API unavailable, DataHub will:\n  # 1. Query DataHub for all PostgreSQL tables matching pattern\n  # 2. Derive expected topic names (server.schema.table format)\n  # 3. Apply transforms if configured\n  # 4. Create lineages without Kafka validation\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Sink Connector with topics.regex (Confluent Cloud):")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'# S3 sink connector consuming from pattern-matched topics\nconnector.config:\n  topics.regex: "analytics\\\\..*" # Match topics like analytics.users, analytics.orders\n  # When Kafka API unavailable, DataHub will:\n  # 1. Query DataHub for all Kafka topics (requires use_schema_resolver: true)\n  # 2. Match topics against the regex pattern\n  # 3. Create lineages for matched topics\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Enable DataHub Topic Querying for Sink Connectors:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'source:\n  type: kafka-connect\n  config:\n    connect_uri: "https://api.confluent.cloud/connect/v1/environments/env-123/clusters/lkc-abc456"\n    username: "your-connect-api-key"\n    password: "your-connect-api-secret"\n\n    # Enable DataHub schema resolver for topic pattern expansion\n    use_schema_resolver: true # Required for topics.regex fallback\n\n    # Configure graph connection for DataHub queries\n    datahub_gms_url: "http://localhost:8080" # Your DataHub GMS endpoint\n')),(0,t.yg)("h4",{id:"key-benefits"},"Key Benefits"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Confluent Cloud Support"),": Both source and sink connectors work correctly with pattern-based configurations"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Config as Source of Truth"),": Source connectors always derive topics from configuration, not from querying all tables in DataHub"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Smart Fallback"),": Sink connectors can query DataHub for Kafka topics when Kafka API is unavailable"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Pattern Expansion"),": Wildcards in ",(0,t.yg)("inlineCode",{parentName:"li"},"table.include.list")," and ",(0,t.yg)("inlineCode",{parentName:"li"},"topics.regex")," are properly expanded"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Transform Support"),": All transforms (RegexRouter, EventRouter, etc.) are applied correctly")),(0,t.yg)("h4",{id:"when-datahub-topic-querying-is-used"},"When DataHub Topic Querying is Used"),(0,t.yg)("p",null,"DataHub will query for topics in these scenarios:"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Source Connectors:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"When expanding wildcard patterns in ",(0,t.yg)("inlineCode",{parentName:"li"},"table.include.list")," (e.g., ",(0,t.yg)("inlineCode",{parentName:"li"},"ANALYTICS.PUBLIC.*"),")"),(0,t.yg)("li",{parentName:"ul"},"Queries source platform (PostgreSQL, MySQL, etc.) for tables matching the pattern")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Sink Connectors:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"When ",(0,t.yg)("inlineCode",{parentName:"li"},"topics.regex")," is used AND Kafka API is unavailable (Confluent Cloud)"),(0,t.yg)("li",{parentName:"ul"},"Queries DataHub's Kafka platform for topics matching the regex pattern"),(0,t.yg)("li",{parentName:"ul"},"Requires ",(0,t.yg)("inlineCode",{parentName:"li"},"use_schema_resolver: true")," in configuration")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Important Notes:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},'DataHub never queries "all tables" to create lineages - config is always the source of truth'),(0,t.yg)("li",{parentName:"ul"},"Source connectors query source platforms (databases) to expand table patterns"),(0,t.yg)("li",{parentName:"ul"},"Sink connectors query Kafka platform to expand topic regex patterns"),(0,t.yg)("li",{parentName:"ul"},"Both require appropriate DataHub credentials and connectivity")),(0,t.yg)("h3",{id:"using-datahub-schema-resolver-for-pattern-expansion-and-column-level-lineage"},"Using DataHub Schema Resolver for Pattern Expansion and Column-Level Lineage"),(0,t.yg)("p",null,"The Kafka Connect source can query DataHub for schema information to provide two capabilities:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Pattern Expansion")," - Converts wildcard patterns like ",(0,t.yg)("inlineCode",{parentName:"li"},"database.*")," into actual table names by querying DataHub"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Column-Level Lineage")," - Generates field-level lineage by matching schemas between source tables and Kafka topics")),(0,t.yg)("p",null,"Both features require existing metadata in DataHub from your database and Kafka schema registry ingestion."),(0,t.yg)("h4",{id:"auto-enabled-for-confluent-cloud"},"Auto-Enabled for Confluent Cloud"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Starting with the latest version"),", ",(0,t.yg)("inlineCode",{parentName:"p"},"use_schema_resolver")," is ",(0,t.yg)("strong",{parentName:"p"},"automatically enabled")," for Confluent Cloud environments to provide better defaults for enhanced lineage extraction. This gives you column-level lineage and pattern expansion out of the box!"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Confluent Cloud (Auto-Enabled):")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'source:\n  type: kafka-connect\n  config:\n    # Confluent Cloud environment\n    confluent_cloud_environment_id: "env-xyz123"\n    confluent_cloud_cluster_id: "lkc-abc456"\n    username: "your-connect-api-key"\n    password: "your-connect-api-secret"\n\n    # Schema resolver automatically enabled! \u2713\n    # use_schema_resolver: true (auto-enabled)\n    # schema_resolver_expand_patterns: true (auto-enabled)\n    # schema_resolver_finegrained_lineage: true (auto-enabled)\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"To disable")," (if you don't need these features):"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'source:\n  type: kafka-connect\n  config:\n    confluent_cloud_environment_id: "env-xyz123"\n    confluent_cloud_cluster_id: "lkc-abc456"\n    use_schema_resolver: false # Explicitly disable auto-enable\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Self-hosted (Manual Enable Required):")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'source:\n  type: kafka-connect\n  config:\n    connect_uri: "http://localhost:8083"\n\n    # Must explicitly enable for self-hosted\n    use_schema_resolver: true\n\n    # DataHub connection\n    datahub_api:\n      server: "http://localhost:8080"\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Important Prerequisites:")),(0,t.yg)("blockquote",null,(0,t.yg)("p",{parentName:"blockquote"},(0,t.yg)("strong",{parentName:"p"},"\u26a0\ufe0f Source database tables must be ingested into DataHub BEFORE running Kafka Connect ingestion")),(0,t.yg)("p",{parentName:"blockquote"},"The schema resolver queries DataHub for existing table metadata. If your source databases haven't been ingested yet, the feature will have no effect. Run database ingestion first!")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Recommended Ingestion Order:")),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Ingest source databases (PostgreSQL, MySQL, Snowflake, etc.) \u2192 DataHub"),(0,t.yg)("li",{parentName:"ol"},"Ingest Kafka schema registry (optional, for topic schemas) \u2192 DataHub"),(0,t.yg)("li",{parentName:"ol"},"Run Kafka Connect ingestion \u2192 Enjoy enhanced lineage!")),(0,t.yg)("h4",{id:"configuration-overview"},"Configuration Overview"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'source:\n  type: kafka-connect\n  config:\n    connect_uri: "http://localhost:8083"\n\n    # Enable DataHub schema querying (auto-enabled for Confluent Cloud)\n    use_schema_resolver: true\n\n    # Control which features to use (both default to true when schema resolver enabled)\n    schema_resolver_expand_patterns: true # Expand wildcard patterns\n    schema_resolver_finegrained_lineage: true # Generate column-level lineage\n\n    # DataHub connection (required when use_schema_resolver=true)\n    datahub_api:\n      server: "http://localhost:8080"\n      token: "your-datahub-token" # Optional\n')),(0,t.yg)("h4",{id:"pattern-expansion"},"Pattern Expansion"),(0,t.yg)("p",null,"Converts wildcard patterns in connector configurations into actual table names by querying DataHub."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Example: MySQL Source with Wildcards")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'# Connector config contains pattern\nconnector.config:\n  table.include.list: "analytics.user_*" # Pattern: matches user_events, user_profiles, etc.\n\n# DataHub config\nsource:\n  type: kafka-connect\n  config:\n    use_schema_resolver: true\n    schema_resolver_expand_patterns: true\n# Result: DataHub queries for MySQL tables matching "analytics.user_*"\n# Finds: user_events, user_profiles, user_sessions\n# Creates lineage:\n#   mysql.analytics.user_events -> kafka.server.analytics.user_events\n#   mysql.analytics.user_profiles -> kafka.server.analytics.user_profiles\n#   mysql.analytics.user_sessions -> kafka.server.analytics.user_sessions\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"When to use:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Connector configs have wildcard patterns (",(0,t.yg)("inlineCode",{parentName:"li"},"database.*"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"schema.table_*"),")"),(0,t.yg)("li",{parentName:"ul"},"You want accurate lineage without manually listing every table"),(0,t.yg)("li",{parentName:"ul"},"Source metadata exists in DataHub from database ingestion")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"When to skip:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Connector configs use explicit table lists (no patterns)"),(0,t.yg)("li",{parentName:"ul"},"Source metadata not yet in DataHub"),(0,t.yg)("li",{parentName:"ul"},"Want faster ingestion without DataHub API calls")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Configuration:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},"source:\n  type: kafka-connect\n  config:\n    use_schema_resolver: true\n    schema_resolver_expand_patterns: true # Enable pattern expansion\n\n\n    # If you only want column-level lineage but NOT pattern expansion:\n    # schema_resolver_expand_patterns: false\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Behavior without schema resolver:"),"\nPatterns are treated as literal table names, resulting in potentially incorrect lineage."),(0,t.yg)("h4",{id:"column-level-lineage"},"Column-Level Lineage"),(0,t.yg)("p",null,"Generates field-level lineage by matching column names between source tables and Kafka topics."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Example: PostgreSQL to Kafka CDC")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},"source:\n  type: kafka-connect\n  config:\n    use_schema_resolver: true\n    schema_resolver_finegrained_lineage: true\n# Source table schema in DataHub:\n# postgres.public.users: [user_id, email, created_at, updated_at]\n\n# Kafka topic schema in DataHub:\n# kafka.server.public.users: [user_id, email, created_at, updated_at]\n\n# Result: Column-level lineage created:\n#   postgres.public.users.user_id -> kafka.server.public.users.user_id\n#   postgres.public.users.email -> kafka.server.public.users.email\n#   postgres.public.users.created_at -> kafka.server.public.users.created_at\n#   postgres.public.users.updated_at -> kafka.server.public.users.updated_at\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Requirements:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Source table schema exists in DataHub (from database ingestion)"),(0,t.yg)("li",{parentName:"ul"},"Kafka topic schema exists in DataHub (from schema registry or Kafka ingestion)"),(0,t.yg)("li",{parentName:"ul"},"Column names match between source and target (case-insensitive matching)")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Benefits:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Impact Analysis"),": See which fields are affected by schema changes"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Data Tracing"),": Track specific data elements through pipelines"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Schema Understanding"),": Visualize how data flows at the field level")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"ReplaceField Transform Support:")),(0,t.yg)("p",null,"Column-level lineage respects ReplaceField transforms that filter or rename columns:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'# Connector excludes specific fields\nconnector.config:\n  transforms: "removeFields"\n  transforms.removeFields.type: "org.apache.kafka.connect.transforms.ReplaceField$Value"\n  transforms.removeFields.exclude: "internal_id,temp_column"\n# DataHub behavior:\n# Source schema: [user_id, email, internal_id, temp_column]\n# After transform: [user_id, email]\n# Column lineage created only for: user_id, email\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Configuration:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},"source:\n  type: kafka-connect\n  config:\n    use_schema_resolver: true\n    schema_resolver_finegrained_lineage: true # Enable column-level lineage\n\n\n    # If you only want pattern expansion but NOT column-level lineage:\n    # schema_resolver_finegrained_lineage: false\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Behavior without schema resolver:"),"\nOnly dataset-level lineage is created (e.g., ",(0,t.yg)("inlineCode",{parentName:"p"},"postgres.users -> kafka.users"),"), without field-level detail."),(0,t.yg)("h4",{id:"complete-configuration-example"},"Complete Configuration Example"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'source:\n  type: kafka-connect\n  config:\n    # Kafka Connect cluster\n    connect_uri: "http://localhost:8083"\n    cluster_name: "production-connect"\n\n    # Enable schema resolver features\n    use_schema_resolver: true\n    schema_resolver_expand_patterns: true # Expand wildcard patterns\n    schema_resolver_finegrained_lineage: true # Generate column-level lineage\n\n    # DataHub connection\n    datahub_api:\n      server: "http://datahub.company.com"\n      token: "${DATAHUB_TOKEN}"\n\n    # Platform instances (if using multiple)\n    platform_instance_map:\n      postgres: "prod-postgres"\n      kafka: "prod-kafka"\n')),(0,t.yg)("h4",{id:"performance-impact"},"Performance Impact"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"API Calls per Connector:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Pattern expansion: 1 GraphQL query per unique wildcard pattern"),(0,t.yg)("li",{parentName:"ul"},"Column-level lineage: 2 GraphQL queries (source schema + target schema)"),(0,t.yg)("li",{parentName:"ul"},"Results cached for ingestion run duration")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Optimization:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'# Minimal configuration - no schema resolver\nsource:\n  type: kafka-connect\n  config:\n    connect_uri: "http://localhost:8083"\n    # use_schema_resolver: false  # Default - no DataHub queries\n\n# Pattern expansion only\nsource:\n  type: kafka-connect\n  config:\n    use_schema_resolver: true\n    schema_resolver_expand_patterns: true\n    schema_resolver_finegrained_lineage: false  # Skip column lineage for faster ingestion\n\n# Column lineage only\nsource:\n  type: kafka-connect\n  config:\n    use_schema_resolver: true\n    schema_resolver_expand_patterns: false      # Skip pattern expansion\n    schema_resolver_finegrained_lineage: true\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Best Practice:"),"\nRun database and Kafka schema ingestion before Kafka Connect ingestion to pre-populate DataHub with schema metadata."),(0,t.yg)("h4",{id:"troubleshooting"},"Troubleshooting"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},'"Pattern expansion found no matches for: analytics.',"*",'"')),(0,t.yg)("p",null,"Causes:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Source database metadata not in DataHub"),(0,t.yg)("li",{parentName:"ul"},"Pattern syntax doesn't match DataHub dataset names"),(0,t.yg)("li",{parentName:"ul"},"Platform instance mismatch")),(0,t.yg)("p",null,"Solutions:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Run database ingestion first to populate DataHub"),(0,t.yg)("li",{parentName:"ol"},"Verify pattern matches table naming in source system"),(0,t.yg)("li",{parentName:"ol"},"Check ",(0,t.yg)("inlineCode",{parentName:"li"},"platform_instance_map")," matches database ingestion config"),(0,t.yg)("li",{parentName:"ol"},"Use explicit table list to bypass pattern expansion temporarily")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},'"SchemaResolver not available: DataHub graph connection is not available"')),(0,t.yg)("p",null,"Causes:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Missing ",(0,t.yg)("inlineCode",{parentName:"li"},"datahub_api")," configuration"),(0,t.yg)("li",{parentName:"ul"},"DataHub GMS not accessible")),(0,t.yg)("p",null,"Solutions:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'source:\n  type: kafka-connect\n  config:\n    use_schema_resolver: true\n    datahub_api:\n      server: "http://localhost:8080" # Add DataHub GMS URL\n      token: "your-token" # Add if authentication enabled\n')),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Column-level lineage not appearing")),(0,t.yg)("p",null,"Check:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Source table schema exists: Search for table in DataHub UI"),(0,t.yg)("li",{parentName:"ol"},"Kafka topic schema exists: Search for topic in DataHub UI"),(0,t.yg)("li",{parentName:"ol"},"Column names match (case differences are handled automatically)"),(0,t.yg)("li",{parentName:"ol"},"Check ingestion logs for warnings about missing schemas")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Slow ingestion with schema resolver enabled")),(0,t.yg)("p",null,"Profile:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},'Check logs for "Schema resolver cache hits: X, misses: Y"'),(0,t.yg)("li",{parentName:"ul"},"High misses indicate missing metadata in DataHub")),(0,t.yg)("p",null,"Temporarily disable to compare:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},"use_schema_resolver: false\n")),(0,t.yg)("h3",{id:"working-with-platform-instances"},"Working with Platform Instances"),(0,t.yg)("p",null,"If you've multiple instances of kafka OR source/sink systems that are referred in your ",(0,t.yg)("inlineCode",{parentName:"p"},"kafka-connect")," setup, you'd need to configure platform instance for these systems in ",(0,t.yg)("inlineCode",{parentName:"p"},"kafka-connect")," recipe to generate correct lineage edges. You must have already set ",(0,t.yg)("inlineCode",{parentName:"p"},"platform_instance")," in recipes of original source/sink systems. Refer the document ",(0,t.yg)("a",{parentName:"p",href:"/docs/platform-instances"},"Working with Platform Instances")," to understand more about this."),(0,t.yg)("p",null,"There are two options available to declare source/sink system's ",(0,t.yg)("inlineCode",{parentName:"p"},"platform_instance")," in ",(0,t.yg)("inlineCode",{parentName:"p"},"kafka-connect")," recipe. If single instance of platform is used across all ",(0,t.yg)("inlineCode",{parentName:"p"},"kafka-connect")," connectors, you can use ",(0,t.yg)("inlineCode",{parentName:"p"},"platform_instance_map")," to specify platform_instance to use for a platform when constructing URNs for lineage."),(0,t.yg)("p",null,"Example:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},"# Map of platform name to platform instance\nplatform_instance_map:\n  snowflake: snowflake_platform_instance\n  mysql: mysql_platform_instance\n")),(0,t.yg)("p",null,"If multiple instances of platform are used across ",(0,t.yg)("inlineCode",{parentName:"p"},"kafka-connect")," connectors, you'd need to specify platform_instance to use for platform for every connector."),(0,t.yg)("h4",{id:"example---multiple-mysql-source-connectors-each-reading-from-different-mysql-instance"},"Example - Multiple MySQL Source Connectors each reading from different mysql instance"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},"# Map of platform name to platform instance per connector\nconnect_to_platform_map:\n  mysql_connector1:\n    mysql: mysql_instance1\n\n  mysql_connector2:\n    mysql: mysql_instance2\n")),(0,t.yg)("p",null,"Here mysql_connector1 and mysql_connector2 are names of MySQL source connectors as defined in ",(0,t.yg)("inlineCode",{parentName:"p"},"kafka-connect")," connector config."),(0,t.yg)("h4",{id:"example---multiple-mysql-source-connectors-each-reading-from-difference-mysql-instance-and-writing-to-different-kafka-cluster"},"Example - Multiple MySQL Source Connectors each reading from difference mysql instance and writing to different kafka cluster"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},"connect_to_platform_map:\n  mysql_connector1:\n    mysql: mysql_instance1\n    kafka: kafka_instance1\n\n  mysql_connector2:\n    mysql: mysql_instance2\n    kafka: kafka_instance2\n")),(0,t.yg)("p",null,"You can also use combination of ",(0,t.yg)("inlineCode",{parentName:"p"},"platform_instance_map")," and ",(0,t.yg)("inlineCode",{parentName:"p"},"connect_to_platform_map")," in your recipe. Note that, the platform_instance specified for the connector in ",(0,t.yg)("inlineCode",{parentName:"p"},"connect_to_platform_map")," will always take higher precedance even if platform_instance for same platform is set in ",(0,t.yg)("inlineCode",{parentName:"p"},"platform_instance_map"),"."),(0,t.yg)("p",null,"If you do not use ",(0,t.yg)("inlineCode",{parentName:"p"},"platform_instance")," in original source/sink recipes, you do not need to specify them in above configurations."),(0,t.yg)("p",null,"Note that, you do not need to specify platform_instance for BigQuery."),(0,t.yg)("h4",{id:"example---multiple-bigquery-sink-connectors-each-writing-to-different-kafka-cluster"},"Example - Multiple BigQuery Sink Connectors each writing to different kafka cluster"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},"connect_to_platform_map:\n  bigquery_connector1:\n    kafka: kafka_instance1\n\n  bigquery_connector2:\n    kafka: kafka_instance2\n")),(0,t.yg)("h3",{id:"provided-configurations-from-external-sources"},"Provided Configurations from External Sources"),(0,t.yg)("p",null,"Kafka Connect supports pluggable configuration providers which can load configuration data from external sources at runtime. These values are not available to DataHub ingestion source through Kafka Connect APIs. If you are using such provided configurations to specify connection url (database, etc) in Kafka Connect connector configuration then you will need also add these in ",(0,t.yg)("inlineCode",{parentName:"p"},"provided_configs")," section in recipe for DataHub to generate correct lineage."),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},"# Optional mapping of provider configurations if using\nprovided_configs:\n  - provider: env\n    path_key: MYSQL_CONNECTION_URL\n    value: jdbc:mysql://test_mysql:3306/librarydb\n")),(0,t.yg)("h2",{id:"troubleshooting-1"},"Troubleshooting"),(0,t.yg)("h3",{id:"topic-discovery-issues"},"Topic Discovery Issues"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Problem"),": Missing or incomplete topic information in lineage"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Solutions"),":"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Verify Environment Detection"),":"),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-bash"},'# Check logs for environment detection messages\n# Self-hosted: "Detected self-hosted Kafka Connect - using runtime topics API"\n# Confluent Cloud: "Detected Confluent Cloud - using comprehensive Kafka REST API topic retrieval"\n'))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Test API Connectivity"),":"),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-bash"},'# For self-hosted - test topics API\ncurl -X GET "http://localhost:8083/connectors/{connector-name}/topics"\n\n# For Confluent Cloud - test Kafka REST API v3\ncurl -X GET "https://pkc-xxxxx.region.provider.confluent.cloud/kafka/v3/clusters/{cluster-id}/topics"\n'))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Configuration Troubleshooting"),":"),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-yml"},"# Enable debug logging\nsource:\n  type: kafka-connect\n  config:\n    # ... other config ...\n    use_connect_topics_api: true # Ensure this is enabled (default)\n")))),(0,t.yg)("h3",{id:"environment-specific-issues"},"Environment-Specific Issues"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Self-hosted Issues"),":"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"403/401 errors"),": Check authentication credentials (",(0,t.yg)("inlineCode",{parentName:"li"},"username"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"password"),")"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"404 errors"),": Verify Kafka Connect cluster is running and REST API is accessible"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Empty topic lists"),": Check if connectors are actually running and processing data")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Confluent Cloud Issues"),":"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Missing topics"),": Verify connector configuration has proper source table fields (",(0,t.yg)("inlineCode",{parentName:"li"},"table.include.list"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"query"),")"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Transform accuracy"),": Check that RegexRouter patterns in connector config are valid Java regex"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Complex transforms"),": Now fully supported via forward transform pipeline with topic validation"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Schema preservation"),": Full schema information (e.g., ",(0,t.yg)("inlineCode",{parentName:"li"},"public.users"),") is maintained through transform pipeline")),(0,t.yg)("h3",{id:"performance-optimization"},"Performance Optimization"),(0,t.yg)("p",null,"If topic discovery is impacting performance:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yml"},'source:\n  type: kafka-connect\n  config:\n    connect_uri: "http://localhost:8083"\n    use_connect_topics_api: false # Disable for better performance (no topic info)\n')),(0,t.yg)("h3",{id:"code-coordinates"},"Code Coordinates"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Class Name: ",(0,t.yg)("inlineCode",{parentName:"li"},"datahub.ingestion.source.kafka_connect.kafka_connect.KafkaConnectSource")),(0,t.yg)("li",{parentName:"ul"},"Browse on ",(0,t.yg)("a",{parentName:"li",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/ingestion/source/kafka_connect/kafka_connect.py"},"GitHub"))),(0,t.yg)("h2",null,"Questions"),(0,t.yg)("p",null,"If you've got any questions on configuring ingestion for Kafka Connect, feel free to ping us on ",(0,t.yg)("a",{parentName:"p",href:"https://datahub.com/slack"},"our Slack"),"."))}f.isMDXComponent=!0}}]);