"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[95913],{66626:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/business-glossary","metadata":{"permalink":"/learn/business-glossary","source":"@site/src/learn/business-glossary.md","title":"What is a Business Glossary and How to Standardize It","description":"Understand how a standardized business glossary aids in achieving consistency, compliance, and efficient data use.","date":"2024-06-03T05:00:00.000Z","formattedDate":"June 3, 2024","tags":[{"label":"Business Glossary","permalink":"/learn/tags/business-glossary"},{"label":"Use Case","permalink":"/learn/tags/use-case"},{"label":"For Data Governance Leads","permalink":"/learn/tags/for-data-governance-leads"}],"readingTime":6.615,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"What is a Business Glossary and How to Standardize It","description":"Understand how a standardized business glossary aids in achieving consistency, compliance, and efficient data use.","tags":["Business Glossary","Use Case","For Data Governance Leads"],"image":"/img/learn/use-case-business-glossary.png","hide_table_of_contents":false,"audience":["Data Governance Leads"],"date":"2024-06-03T05:00"},"nextItem":{"title":"What is a Business Metric and How to Define and Standardize Them","permalink":"/learn/business-metric"}},"content":"Understand how a standardized business glossary aids in achieving consistency, compliance, and efficient data use.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nHave you ever faced confusion due to inconsistent business terminology within your organization? This lack of standardization can lead to misunderstandings, compliance issues, and inefficient data use. In this post, we\u2019ll explore the importance of having a standardized business glossary, its benefits, and how you can implement one effectively in your organization.\\n\\n## What is a Business Glossary?\\n\\nA Business Glossary is like a dictionary for your company. It contains definitions of key business terms that everyone in the organization uses, ensuring everyone speaks the same language, especially when it comes to important concepts related to the data your company collects, processes, and uses.\\n\\nFor example, below are some sales-related glossary terms that can be used in an IT company.\\n\\n| Term | Definition | Usage |\\n| --- | --- | --- |\\n| CRM (Customer Relationship Management) | Software that manages a company\'s interactions with current and potential customers. | CRMs help streamline processes and improve customer relationships. |\\n| Lead | A potential customer who has shown interest in a company\'s product or service. | Leads are nurtured by the sales team to convert into customers. |\\n| Pipeline | The stages through which a sales prospect moves from initial contact to final sale. | Sales pipelines track progress and forecast future sales. |\\n| Quota | A sales target set for a salesperson or team for a specific period. | Quotas motivate sales teams and measure performance. |\\n| Conversion Rate | The percentage of leads that turn into actual sales. | High conversion rates indicate effective sales strategies. |\\n| Upselling | Encouraging customers to purchase a more expensive or upgraded version of a product. | Upselling increases revenue by enhancing the customer purchase. |\\n| Churn Rate | The percentage of customers who stop using a product or service over a given period. | Reducing churn rate is crucial for maintaining steady growth. |\\n| MQL (Marketing Qualified Lead) | A lead that has been deemed more likely to become a customer based on marketing efforts. | MQLs are passed from the marketing team to the sales team for further nurturing. |\\n| ARR (Annual Recurring Revenue) | The amount of revenue that a company expects to receive from its customers on an annual basis for subscriptions. | ARR helps in financial forecasting and performance measurement. |\\n\\n## What is Business Glossary Standardization?\\n\\nBusiness glossary standardization means creating and maintaining a consistent set of business terms and definitions used across the organization. This practice is essential for maintaining clarity and consistency in how data is interpreted and used across different departments.\\n\\n## Why Should You Care?\\n\\n### The Challenge\\n\\nWithout a consistent understanding and use of business terminology, your company lacks a unified understanding of its data. This can lead to inconsistencies, increased compliance risk, and less effective use of data. Different teams may describe the same concepts in various ways, causing confusion about customers, key metrics, products, marketing, and more.\\n\\n### The Benefits\\n\\nFor a governance lead, standardizing the business glossary is crucial for several reasons:\\n\\n- **Reduces Confusion, Facilitates Discovery:** Ensures data quality, consistency, and reliability, which are critical for effective decision-making.\\n- **Regulatory Compliance:** Aligns data use with regulatory definitions and requirements, essential for compliance with financial regulations.\\n- **Supports Risk Management:** Provides consistent terminology for analyzing market trends, credit risk, and operational risks.\\n- **Training and Onboarding:** Helps new employees quickly understand the company\u2019s specific language and metrics, speeding up the training process.\\n\\n### Real-World Impact\\n\\nImagine a financial services company where different teams use varied terminologies for the same concepts, such as \\"customer lifetime value.\\" (CLV) This inconsistency can lead to misinterpretations, faulty risk assessments, and regulatory non-compliance, ultimately affecting the company\'s reputation and financial stability.\\n\\nHere\'s how different teams might interpret CLV and the potential implications:\\n\\n| Team | Interpretation of CLV | Focus | Implications |\\n| --- | --- | --- | --- |\\n| Marketing | Total revenue generated from a customer over their entire relationship with the company | Campaign effectiveness, customer acquisition costs, return on marketing investment | Revenue maximization through frequent promotions, potentially ignoring the cost of service and risk associated with certain customer segments |\\n| Sales | Projected future sales from a customer based on past purchasing behavior | Sales targets, customer retention, cross-selling/up-selling opportunities | Aggressive sales tactics to boost short-term sales, potentially leading to customer churn if the value delivered does not meet |\\n| Finance | Net present value (NPV), factoring in the time value of money and associated costs over the customer relationship period | Profitability, cost management, financial forecasting | Conservative growth strategies, focusing on high-value, low-risk customers, potentially overlooking opportunities for broader market expansion |\\n\\n Different interpretations can lead to conflicting strategies and objectives across teams. For instance, Marketing\u2019s aggressive acquisition strategy may lead to a significant increase in new customers and short-term revenue. However, if Finance\u2019s NPV analysis reveals that these customers are not profitable long-term, the company may face financial strain due to high acquisition costs and low profitability.\\n\\n The Sales team\u2019s push for upselling may generate short-term sales increases, aligning with their CLV projections. However, if customers feel pressured and perceive the upsells as unnecessary, this could lead to dissatisfaction and higher churn rates, ultimately reducing the actual lifetime value of these customers.\\n\\n The conflicting strategies can result in misaligned priorities, where Marketing focuses on volume, Sales on immediate revenue, and Finance on long-term profitability. This misalignment can lead to inefficient resource allocation, where Marketing spends heavily on acquisition, Sales focuses on short-term gains, and Finance restricts budgets due to profitability concerns.\\n\\n### Example Discovery Questions\\n\\n- Have you ever experienced confusion or errors due to inconsistent terminology in your organization\'s data reports? How do you currently manage and standardize business terms across departments?\\n- If your organization lacks a standardized business glossary, what challenges do you face in ensuring regulatory compliance and reliable data analysis?\\n- When onboarding new employees, do you find that inconsistent terminology slows down their training and understanding of company data? How could a standardized glossary improve this process?\\n\\n## How to Standardize a Business Glossary\\n\\n### General Approach\\n\\nTo standardize a business glossary, start by identifying key business terms and their definitions. Engage stakeholders from various departments to ensure comprehensive coverage and agreement. Regularly update the glossary to reflect changes in business processes and regulatory requirements.\\n\\n### Alternatives and Best Practices\\n\\nSome companies use manual methods to track data terminology and manage access requests. While these methods can work, they are often inefficient and error-prone. Best practices include using automated tools that provide consistent updates and easy access to the glossary for all employees.\\n\\n### Our Solution\\n\\nDataHub Cloud offers comprehensive features designed to support the authoring of a unified business glossary for your organization:\\n\\n<p align=\\"center\\">\\n  <img width=\\"80%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/glossary-terms/business-glossary-center.png\\"/>\\n  <br />\\n  <i style={{color:\\"grey\\"}}>Business Glossary Center</i>\\n</p>\\n\\n- **[Centralized Business Glossary](https://docs.datahub.com/docs/glossary/business-glossary):** A repository for all business terms and definitions, ensuring consistency across the organization.\\n\\n\\n<p align=\\"center\\">\\n  <img width=\\"80%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/glossary-terms/approval-workflow.png\\"/>\\n <br />\\n  <i style={{color:\\"grey\\"}}>Approval Flows</i>\\n</p>\\n\\n\\n- **[Approval Flows](https://docs.datahub.com/docs/managed-datahub/approval-workflows):** Structured workflows for approving changes to the glossary, maintaining quality and consistency through time\\n\\n- **Automated Data Classification:** Tools to tag critical data assets - tables, columns, dashboards, and pipelines - with terms from the business glossary using automations and custom rules.\\n\\nBy implementing these solutions, you can ensure that your business terminology is consistently defined and accurately used across all teams, supporting reliable decision-making and regulatory compliance.\\n\\n## Conclusion\\n\\nStandardizing your business glossary is essential for maintaining consistency, ensuring compliance, and optimizing data use. By implementing best practices and leveraging advanced tools, you can achieve a more efficient and reliable data management process. This investment will lead to better decision-making, reduced compliance risks, and a more cohesive organizational understanding of data."},{"id":"/business-metric","metadata":{"permalink":"/learn/business-metric","source":"@site/src/learn/business-metric.md","title":"What is a Business Metric and How to Define and Standardize Them","description":"Learn the importance of consistent metric definitions and calculation methods to ensure organizational alignment.","date":"2024-06-03T04:00:00.000Z","formattedDate":"June 3, 2024","tags":[{"label":"Business Metric","permalink":"/learn/tags/business-metric"},{"label":"Use Case","permalink":"/learn/tags/use-case"},{"label":"For Data Analysts","permalink":"/learn/tags/for-data-analysts"}],"readingTime":4.59,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"What is a Business Metric and How to Define and Standardize Them","description":"Learn the importance of consistent metric definitions and calculation methods to ensure organizational alignment.","tags":["Business Metric","Use Case","For Data Analysts"],"image":"/img/learn/use-case-business-metric.png","hide_table_of_contents":false,"audience":["Data Analysts"],"date":"2024-06-03T04:00"},"prevItem":{"title":"What is a Business Glossary and How to Standardize It","permalink":"/learn/business-glossary"},"nextItem":{"title":"What is a Data Pipeline and Why Should We Optimize It","permalink":"/learn/data-pipeline"}},"content":"Learn the importance of consistent metric definitions and calculation methods to ensure organizational alignment.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nHave you ever been part of a project where different teams had conflicting definitions for key business metrics like revenue, churn, or weekly active users? This misalignment can cause significant issues, leading to incorrect analysis and poor decision-making. In this post, we will explore the importance of defining and standardizing business metrics, why it matters, and how you can do it effectively within your organization.\\n\\n## What is Business Metrics Definition and Standardization?\\n\\nStandardizing business metrics definition involves creating consistent and universally understood definitions for key performance indicators (KPIs) across your organization. Think of it as creating a common language that everyone in your company can use when discussing critical metrics like revenue, churn, or engagement. This ensures that all teams are on the same page, which is essential for accurate analysis and strategic decision-making.\\n\\n## Why Should You Care About Business Metrics Definition and Standardization?\\n\\n### The Challenge\\n\\nIn many organizations, KPIs are used to drive critical day-to-day operating decisions. They often emerge organically in response to the data needs of management. Over time, organizations can naturally develop inconsistent sources, representations, and vocabulary around such metrics. When there is a lack of consistent understanding of these metrics, it can lead to meaningful discrepancies in data interpretation and decision-making.\\n\\n### Importance\\n\\nStandardizing business metrics is crucial because these metrics are direct indicators of the performance and health of various functions within an organization. More often than not, these metrics are used for not only making day-to-day operating decisions, but also for reporting out business performance. Standardized metrics provide immediate insight into whether the business is on track to meet its objectives and serve as solid foundations upon which other second-order metrics may be derived.\\n\\n### Real-World Impact\\n\\nConsider a scenario where the finance team defines revenue differently from the product team. If these discrepancies are not reconciled, it could lead to conflicting reports and misguided strategies. For instance, a marketing campaign analyzed with inconsistent metrics might appear successful in one report and unsuccessful in another, causing confusion and potentially leading to incorrect strategic decisions. Disagreements about the source-of-truth or accuracy of a given metric are commonplace; perhaps you can recall some examples from your own experience. \\n\\n### Example Discovery Questions and Explanations\\n\\n- **Current Management and Challenges:** \\"How do you currently manage and standardize definitions for core business metrics across different teams, and what challenges have you encountered in this process?\\" This question helps to uncover the existing processes and pain points in managing metrics, providing insights into potential areas where our product can offer significant improvements.\\n- **Educating your Workforce:** \u201cHow do you educate new employees about the most important metrics at the organization?\u201d This question helps to recognize and eliminate inefficient sharing of tribal knowledge within an organization when an employee joins or leaves.\\n- **Impact of Misalignment:** \\"Can you describe a recent instance where misalignment on metric definitions impacted a business decision or analysis, and how was the issue resolved?\\" This question aims to highlight the real-world consequences of not having standardized metrics, emphasizing the importance of our solution in preventing such issues.\\n\\n## How to Define and Standardize Business Metrics\\n\\n### General Approach\\n\\nStart by identifying key business metrics that are actively used to power decision making at the organization. Involve stakeholders from different departments to agree on a standard set of definitions, and propose a lightweight process for introducing new ones. Document these definitions and ensure they are easily accessible to everyone in the organization. Regular reviews and updates are necessary to keep the metrics relevant and aligned with business goals.\\n\\n### Alternatives and Best Practices\\n\\nSome companies try to align metric definitions through emails and meetings. While this is a good place to start, it is often impractical at scale. Instead, best practices involve using a centralized system for defining and discovering key business metrics. Implementing approval flows and lineage tracking can ensure that all changes are reviewed and that the physical origins of a metric - e.g. the actual tables and rows that power it - are immediately clear. By making metrics centrally visible, you can begin to establish accountability and audibility around your key metrics, increasing their reliability through time and improving the quality of your decisions. \\n\\n### Our Solution\\n\\nDataHub Cloud offers comprehensive features designed to tackle the challenges of defining and standardizing business metrics:\\n\\n<p align=\\"center\\">\\n  <img width=\\"80%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/glossary-terms/business-glossary-center.png\\"/>\\n  <br />\\n  <i style={{color:\\"grey\\"}}>Business Glossary Center</i>\\n</p>\\n\\n\\n- **[Business Glossary](https://docs.datahub.com/docs/glossary/business-glossary):** A centralized repository for all metrics definitions, ensuring consistency across the organization.\\n\\n<p align=\\"center\\">\\n  <img width=\\"80%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/glossary-terms/approval-workflow.png\\"/>\\n <br />\\n  <i style={{color:\\"grey\\"}}>Approval Flows</i>\\n</p>\\n\\n- **[Approval Flows](https://docs.datahub.com/docs/managed-datahub/change-proposals#proposing-tags-and-glossary-terms):** Structured workflows for approving changes to metric definitions, maintaining accuracy and reliability.\\n\\n<p align=\\"center\\">\\n  <img width=\\"80%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/data-freshness/lineage.png\\"/>\\n <br />\\n  <i style={{color:\\"grey\\"}}>Lineage Tracking</i>\\n</p>\\n\\n- **[Lineage Tracking](https://docs.datahub.com/docs/features/feature-guides/lineage):** Tools to track the origin and transformations of metrics, ensuring they align with standardized definitions.\\n\\nBy implementing these solutions, you can ensure that your business metrics are consistently defined and accurately used across all teams, supporting reliable analysis and decision-making.\\n\\n### Conclusion\\n\\nDefining and standardizing business metrics is essential for ensuring consistent, accurate, and reliable data analysis and decision-making within an organization. By implementing best practices and leveraging advanced tools like our product\u2019s business glossary, approval flows, and lineage tracking, you can achieve a more cohesive and efficient approach to managing business metrics. This investment will lead to better insights, more informed decisions, and ultimately, a more successful data-driven organization."},{"id":"/data-pipeline","metadata":{"permalink":"/learn/data-pipeline","source":"@site/src/learn/data-pipeline.md","title":"What is a Data Pipeline and Why Should We Optimize It","description":"Discover the importance of optimizing data pipelines to maintain data freshness and control costs.","date":"2024-06-03T03:00:00.000Z","formattedDate":"June 3, 2024","tags":[{"label":"Data Pipeline","permalink":"/learn/tags/data-pipeline"},{"label":"Use Case","permalink":"/learn/tags/use-case"},{"label":"For Data Engineers","permalink":"/learn/tags/for-data-engineers"}],"readingTime":4.14,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"What is a Data Pipeline and Why Should We Optimize It","description":"Discover the importance of optimizing data pipelines to maintain data freshness and control costs.","tags":["Data Pipeline","Use Case","For Data Engineers"],"image":"/img/learn/use-case-data-pipeline.png","hide_table_of_contents":false,"audience":["Data Engineers"],"date":"2024-06-03T03:00"},"prevItem":{"title":"What is a Business Metric and How to Define and Standardize Them","permalink":"/learn/business-metric"},"nextItem":{"title":"What is a Data Mesh and How to Implement It in Your Organization","permalink":"/learn/data-mesh"}},"content":"Discover the importance of optimizing data pipelines to maintain data freshness and control costs.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nHave you ever been frustrated by slow and unreliable data pipelines or unexpectedly high cloud bills? In the modern data world, maintaining efficient, reliable, and cost-effective data pipelines is crucial for delivering timely, high-quality data. This post will explore the importance of optimizing data pipelines, why it matters, and how to achieve it effectively.\\n\\n## What is a Data Pipeline?\\n\\nA data pipeline is a series of processes that move data from one system to another - a key component in the supply chain for data. Think of it like a conveyor belt in a factory, transporting raw materials to different stations where they are processed into the final product. In the context of data, pipelines extract, transform, and load data (ETL) from various sources to destinations like data warehouses, ensuring the data is ready for analysis and use in applications such as machine learning models and business intelligence dashboards.\\n\\n\\n<p align=\\"center\\">\\n  <img width=\\"80%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/data-pipeline/pipeline-lineage.png\\"/>\\n <br />\\n  <i style={{color:\\"grey\\"}}>Data Pipeline Example</i>\\n</p>\\n\\n## Why Should You Care About Data Pipeline Optimization?\\n\\n### The Problem\\n\\nOver time, data pipelines can slow down or become unreliable due to new dependencies, application code bugs, and poorly optimized queries, leading to missed data freshness SLAs and increased cloud costs. For data engineers, this means more time spent on manual debugging and  justifying costs to your executives. \\n\\n### Importance\\n\\nEfficient data pipelines are essential for maintaining the performance of mission-critical tables, dashboards, and ML models powering key use cases for your organization. For example, a price prediction model relies on timely data to provide accurate results, directly impacting revenue. Similarly, outdated customer data can harm a company\u2019s reputation and customer satisfaction.\\n\\n### Real-World Impact\\n\\nImagine you\u2019re managing a recommendation engine for an e-commerce site. If your data pipeline is delayed, the recommendations could become outdated, leading to missed sales opportunities - financial costs - and a poor user experience - reputational costs. Alternatively, consider a fraud detection system that relies on real-time data; any delay or downtime could mean the difference between catching fraudulent activity and suffering significant financial loss.\\n\\n### Questions To Ask\\n\\n- Have you ever noticed a decline in the freshness of crucial data or an uptick in cloud costs for specific pipelines? How do you currently approach diagnosing and optimizing these pipelines?\\n- If your organization is facing increasing cloud bills due to data pipeline inefficiencies, what strategies or tools do you employ to monitor and optimize costs? How do you balance the trade-off between performance, cost, and meeting business stakeholders\' expectations for data delivery?\\n- Are you taking proactive measures to prevent data pipelines from becoming slower, more fragile, or more expensive over time? Do you have a system in place for regularly reviewing and optimizing key data pipelines to prevent performance or cost degradation?\\n\\n## How to Optimize Data Pipelines\\n\\n### General Approach\\n\\nTo optimize your data pipelines, start by identifying bottlenecks and inefficiencies in the pipelines that generate your most mission-critical tables, dashboards, and models. Regularly review and update queries, and monitor pipeline performance by measuring aggregate pipeline run times as well as more granular tracking at the step or query level to catch issues early. Implement automation wherever possible to reduce manual intervention and ensure consistency.\\n\\n### Alternatives and Best Practices\\n\\nSome companies resort to manual debugging or use communication tools like Slack to triage issues. While these methods can work, they are often time-consuming and prone to errors. Instead, consider leveraging tools that provide lineage tracking, last updated time, and automated monitoring to streamline the optimization process.\\n\\n### Our Solution\\n\\nDataHub Cloud offers comprehensive features designed to optimize data pipelines:\\n\\n<p align=\\"center\\">\\n  <img width=\\"80%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/data-pipeline/lineage-tracking.png\\"/>\\n <br />\\n  <i style={{color:\\"grey\\"}}>Pipeline Catalog</i>\\n</p>\\n\\n- **Pipeline Cataloging:** Quickly browse all of the data pipelines running inside your organization, and track critical human context like pipeline ownership / accountability, purpose / documentation, and compliance labels in one place.\\n\\n<p align=\\"center\\">\\n  <img width=\\"80%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/data-pipeline/pipeline-cataloging.png\\"/>\\n <br />\\n  <i style={{color:\\"grey\\"}}>Lineage Tracking</i>\\n</p>\\n\\n- **[Lineage Tracking](https://docs.datahub.com/docs/features/feature-guides/lineage) and [Impact Analysis](https://docs.datahub.com/docs/act-on-metadata/impact-analysis):** Understand the flow of data through your pipelines to identify and resolve inefficiencies quickly. Easily see which assets are consumed and produced by which pipelines.\\n- **Freshness Monitoring:** Track the freshness using Freshness Assertions of your data to ensure SLAs are met consistently.\\n- **Cost Management Tooling:** Monitor and optimize cloud costs associated with your data pipelines to improve cost-efficiency.\\n\\nBy implementing these solutions, you can ensure that your data pipelines are running efficiently, meeting delivery SLAs, and staying within budget.\\n\\n\\n\\n## Conclusion\\n\\nOptimizing data pipelines is essential for maintaining data reliability, controlling costs, and ultimately ensuring your business continues to run smoothly. By implementing best practices and leveraging advanced tools like our product\u2019s lineage tracking and automated monitoring features, you can achieve efficient and cost-effective data pipelines. Investing time and resources into optimization will ultimately lead to better performance, lower costs, and more satisfied stakeholders."},{"id":"/data-mesh","metadata":{"permalink":"/learn/data-mesh","source":"@site/src/learn/data-mesh.md","title":"What is a Data Mesh and How to Implement It in Your Organization","description":"Learn how a data mesh aligns data management with domain expertise, enhancing overall organizational agility.","date":"2024-06-03T02:00:00.000Z","formattedDate":"June 3, 2024","tags":[{"label":"Data Mesh","permalink":"/learn/tags/data-mesh"},{"label":"Use Case","permalink":"/learn/tags/use-case"},{"label":"For Data Architects","permalink":"/learn/tags/for-data-architects"},{"label":"For Data Platform Leads","permalink":"/learn/tags/for-data-platform-leads"}],"readingTime":5.755,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"What is a Data Mesh and How to Implement It in Your Organization","description":"Learn how a data mesh aligns data management with domain expertise, enhancing overall organizational agility.","tags":["Data Mesh","Use Case","For Data Architects","For Data Platform Leads"],"image":"/img/learn/use-case-data-mesh.png","hide_table_of_contents":false,"audience":["Data Architects","Data Platform Leads"],"date":"2024-06-03T02:00"},"prevItem":{"title":"What is a Data Pipeline and Why Should We Optimize It","permalink":"/learn/data-pipeline"},"nextItem":{"title":"Ensuring Data Freshness: Why It Matters and How to Achieve It","permalink":"/learn/data-freshness"}},"content":"Learn how a data mesh aligns data management with domain expertise, enhancing overall organizational agility.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nHave you faced challenges in managing decentralized data across various business units or domains? Implementing a [Data Mesh](https://martinfowler.com/articles/data-mesh-principles.html) can address these issues, aligning data management with domain expertise and enhancing your organization\u2019s overall agility. In this post, we\'ll explore what a Data Mesh is, why it\'s beneficial, and how to implement it effectively within your organization.\\n\\n## What is Data Mesh?\\n\\nData Mesh is a decentralized data architecture that shifts the responsibility of data management from a central team to individual business units, or \\"domains.\\" Each domain in turn produces \u201cdata products\u201d, or consumable data artifacts, ensuring that data management is closely aligned with domain-specific expertise. This approach promotes agility, scalability, and the ability to generate insights more effectively. \\n\\nIf you\u2019re familiar with [Service-Oriented Architectures](https://en.wikipedia.org/wiki/Service-oriented_architecture), i.e. micro-services, this might sound familiar. Data Mesh is a somewhat analogous concept, but applied to data!\\n\\n<p align=\\"center\\">\\n  <img width=\\"70%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/data-mesh/data-mesh-principles.png\\"/>\\n <br />\\n  <i style={{color:\\"grey\\"}}>4 Principles of Data Mesh</i>\\n</p>\\n\\n\\n| Principle | Explanation | \\n| --- | --- |\\n| Domain Data Ownership | Organizing data into explicit domains based on the structure of your organization, and then assigning clear accountability to each. This enables you to more easily increase the number of sources of data, variety of use cases, and diversity of access models to the data increases.  |\\n| Data as a product | Domain data should be highly accessible and highly reliable by default. It should be easy to discover, easy to understand, easy to access securely, and high quality.  |\\n| Self-Service | Domain teams should be able to independently create, consume, and manage data products on top of a general-purpose platform that can hide the complexity of building, executing and maintaining secure and interoperable data products. |\\n| Federated Governance | Consistent standards that are enforced by process and technology around interoperability, compliance, and quality. This makes it easy for data consumers to interact with data products across domains in familiar way and ensures quality is maintained uniformly.  |\\n\\n<p align=\\"center\\">\\n  <img width=\\"70%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/data-mesh/data-mesh-arc.png\\"/>\\n <br />\\n  <i style={{color:\\"grey\\"}}>Logical architecture of data mesh approach, Image Credit: <a href=\\"https://dddeurope.academy/data-mesh-zhamak-dheghani/\\">Zhamak Dehghani</a></i>\\n</p>\\n\\n\\n\\n## Why Implement Data Mesh?\\n\\nFor data architects and data platform leads, implementing a Data Mesh can resolve various challenges associated with managing decentralized data, particularly as you try to scale up. \\n\\nTraditional data lakes or warehouses can become central bottlenecks, impairing access, understanding, accountability, and quality of data - ultimately, its usability. These architectures can struggle to meet the diverse needs of different business units, leading to inefficiencies. \\n\\nData Mesh addresses these issues by formally dividing data into decentralized domains, which are owned by the individual teams who are experts in those domains. This approach allows each business unit or domain to manage its own data, enabling independent creation and consumption of data and increasing the agility, reliability, scalability of an organization\u2019s data practice.  \\n\\n### Key Considerations for Your Organization\\n\\n**Decentralized Data Management:** Have you experienced difficulties or bottlenecks in managing data across various business units? Implementing a Data Mesh can alleviate these issues by allowing each domain to build and share its own data products, enhancing agility and scalability.\\n\\n**Overcoming Centralized Bottlenecks:** If your organization relies on a centralized data lake or warehouse, or data platform team, have you encountered limitations in scalability or delays in data access and analysis? Data Mesh can help overcome these bottlenecks by \u201cpushing down\u201d data ownership and management to domain experts.\\n\\n**Enhancing Agility and Reliability:** How important is it for your organization to respond quickly to market changes and generate insights reliably? By formally defining the responsibilities around data \u201cproducts\u201d, a data mesh architecture can provide the flexibility and speed needed to stay competitive.\\n\\n## How to Implement Data Mesh\\n\\nImplementing Data Mesh doesn\u2019t need to be a headache. Here\u2019s how your organization can move towards a better future:\\n\\n### Best Practices and Strategies\\n\\n**Define Domains and Data Products**\\n\\nFormally define the different business units or domains within your organization and define the data products each domain will own and manage, and then begin to organize the data on your existing warehouse or lake around these domains. This ensures clarity and responsibility for data management.\\n\\n**Establish Clear Contracts**\\n\\nCreate a clear set of expectations around what it means to be a domain or data product owner within your organization. Then, build processes and systems to both reinforce and monitor these expectations. This helps maintain consistency and reliability across the organization.\\n\\n**Monitor Data Quality**\\n\\nUse metadata validation and data quality assertions to ensure that your expectations are being met. This includes setting standards for both data quality - freshness, volume, column validity - as well compliance with your less technical requirements - ownership, data documentation, and data classification.\\n\\n**Move Towards Federated Governance**\\n\\nAdopt a federated governance model to balance autonomy and control. While domains manage their data products, a central team can oversee governance standards and ensure compliance with organizational policies via a well-defined review process.\\n\\n### Alternatives\\n\\nWhile a centralized data lake or warehouse can simplify data governance by virtue of keeping everything in one place, it can become a bottleneck as your data organization grows. Decentralized Data Mesh can provide a more scalable and agile approach, by distributing day-to-day responsibility for accessing, producing, and validating data while enforcing a centralized set of standards and processes. \\n\\n### Our Solution\\n\\nDataHub Cloud offers a comprehensive set of features designed to support the implementation of a Data Mesh at your organization:\\n\\n- **[Data Domains](https://docs.datahub.com/docs/domains)**: Clearly define and manage data products within each business unit.\\n- **[Data Products](https://docs.datahub.com/docs/dataproducts):** Ensure each domain owns and manages its data products, promoting autonomy and agility.\\n- **[Data Contracts](https://docs.datahub.com/docs/managed-datahub/observe/data-contract)**: Establish clear agreements between domains to ensure consistency and reliability.\\n    \\n\\n<p align=\\"center\\">\\n  <img width=\\"80%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/data-mesh/data-contract.png\\"/> \\n  <br />\\n  <i style={{color:\\"grey\\"}}>Data Contracts in DataHub Cloud UI</i>\\n</p>\\n\\n\\n\\n- **[Assertions](https://docs.datahub.com/docs/managed-datahub/observe/assertions)** Monitor data quality using freshness, volume, column validity, schema, and custom SQL checks to get notified first when things go wrong\\n\\n\\n<p align=\\"center\\">\\n  <img width=\\"80%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/data-mesh/assertion-results.png\\"/>\\n <br />\\n  <i style={{color:\\"grey\\"}}>Assertion Results</i>\\n</p>\\n\\n\\n\\n- **[Metadata Tests](https://docs.datahub.com/docs/tests/metadata-tests)**: Monitor and enforce a central set of standards or policies across all of your data assets, e.g. to ensure data documentation, data ownership, and data classification.\\n\\n<p align=\\"center\\">\\n  <img width=\\"80%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/data-mesh/test-results.png\\"/>\\n <br />\\n  <i style={{color:\\"grey\\"}}>Metadata Test Results</i>\\n</p>\\n\\nBy implementing these solutions, you can effectively manage decentralized data, enhance agility, and generate insights more efficiently.\\n\\n## Conclusion\\n\\nImplementing a Data Mesh can significantly improve your organization\'s ability to manage and leverage decentralized data. By understanding the benefits of data mesh and following best practices for implementation, you can overcome the limitations of centralized data systems and enhance your agility, scalability, and ability to generate insights. DataHub Cloud was built from the ground up to help you achieve this, providing the tools and features necessary to implement a large-scale Data Mesh successfully."},{"id":"/data-freshness","metadata":{"permalink":"/learn/data-freshness","source":"@site/src/learn/data-freshness.md","title":"Ensuring Data Freshness: Why It Matters and How to Achieve It","description":"Explore the significance of maintaining up-to-date data, the challenges involved, and how our solutions can ensure your data remains fresh to meet SLAs.","date":"2024-06-03T01:00:00.000Z","formattedDate":"June 3, 2024","tags":[{"label":"Data Freshness","permalink":"/learn/tags/data-freshness"},{"label":"Use Case","permalink":"/learn/tags/use-case"},{"label":"For Data Engineers","permalink":"/learn/tags/for-data-engineers"}],"readingTime":5.505,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Ensuring Data Freshness: Why It Matters and How to Achieve It","description":"Explore the significance of maintaining up-to-date data, the challenges involved, and how our solutions can ensure your data remains fresh to meet SLAs.","tags":["Data Freshness","Use Case","For Data Engineers"],"image":"/img/learn/use-case-data-freshness.png","hide_table_of_contents":false,"audience":["Data Engineers"],"date":"2024-06-03T01:00"},"prevItem":{"title":"What is a Data Mesh and How to Implement It in Your Organization","permalink":"/learn/data-mesh"}},"content":"Explore the significance of maintaining up-to-date data, the challenges involved, and how our solutions can ensure your data remains fresh to meet SLAs.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nHave you ever experienced delays in delivering tables that or machine learning (ML) models that directly power customer experiences due to stale data? Ensuring timely data is crucial for maintaining the effectiveness and reliability of these mission-critical products. In this post, we\'ll explore the importance of data freshness, the challenges associated with it, and how DataHub can help you meet your data freshness SLAs consistently.\\n\\n## What is Data Freshness?\\n\\nData freshness refers to the timeliness and completeness of data used to build tables and ML models. Specifically, freshness can be measured by the difference in time between when some event *actually occurs* vs when that record of that event is reflected in a dataset or used to train an AI model. \\n\\nTo make things concrete, let\u2019s imagine you run an e-commerce business selling t-shirts. When a user clicks the final \u201cpurchase\u201d button to finalize a purchase, this interaction is recorded, eventually winding up in a consolidated \u201cclick_events\u201d table on your data warehouse. Data freshness in this case could be measured by comparing when the actual click was performed against when the record of the click landed in the data warehouse. In reality, freshness can be measured against any reference point - e.g. event time, ingestion time, or something else - in relation to when a target table, model, or other data product is updated with new data. \\n\\n<p align=\\"center\\">\\n  <img width=\\"70%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/data-freshness/freshness-concept.png\\"/>\\n <br />\\n  <i style={{color:\\"grey\\"}}>Data Freshness</i>\\n</p>\\n\\nOftentimes, data pipelines are designed in order meet some well-defined availability latency, or data freshness SLA, with the specifics of this type of agreement dictating how and when the data pipeline is triggered to run.  \\n\\nIn the modern data landscape, ensuring that data is up-to-date is vital for building high-quality data products, from reporting dashboards used to drive day-to-day company decisions to personalized and dynamic data- or AI-powered product experiences. \\n\\n## Why Data Freshness Matters\\n\\nFor many organizations, fresh data is more than a \u2018nice to have\u2019. \\n\\nMission-critical ML models, like those used for price prediction or fraud detection, depend heavily on fresh data to make accurate predictions. Delays in updating these models can lead to  lost revenue and damage to your company\'s reputation.\\n\\nCustomer-facing data products, for example recommendation features, also need timely updates to ensure that customers receive the most recent and relevant information personalized to them. Delays in data freshness can result in customer frustration, user churn, and loss of trust.\\n\\n### Key Considerations for Your Organization\\n\\n**Critical Data and ML Models:**\\n\\nCan you recall examples when your organization faced challenges in maintaining the timeliness of mission-critical datasets and ML models? If your organization relies on data to deliver concrete product experiences, compliance auditing, or for making high-quality day-to-day decision, then stale data can significantly impact revenue and customer satisfaction. Consider identifying which datasets and models are most critical to your operations and quantifying the business impact of delays.\\n\\n**Impact Identification and Response:**\\n\\nBecause data is highly interconnected, delays in data freshness can lead to cascading problems, particularly of your organization lacks a robust system for identifying and resolving such problems. How does your organization prioritize and manage such incidents? Processes for quickly identifying and resolving root causes are essential for minimizing negative impacts on revenue and reputation.\\n\\n**Automated Freshness Monitoring:**\\n    \\nIf data freshness problems often go undetected for long periods of time, there may be opportunities to automate the detection of such problems for core tables and AI models so that your team is first to know when something goes wrong.\\n\\n## How to Ensure Data Freshness\\n\\nEnsuring data freshness involves several best practices and strategies. Here\u2019s how you can achieve it:\\n\\n### Best Practices and Strategies\\n\\n**Data Lineage Tracking:**\\n\\nUtilize data lineage tracking to establish a bird\u2019s eye view of data flowing through your systems - a picture of the supply chain of data within your organization. This helps in pinpointing hotspots where delays occur and understanding the full impact of such delays to coordinate an effective response.\\n\\n**Automation and Monitoring:**\\n\\nImplement automated freshness monitoring to detect and address issues promptly. This reduces the need for manual debugging and allows for quicker response times. It can also help you to establish peace-of-mind by targeting your most impactful assets.\\n\\n**Incident Management:**\\n\\nEstablish clear protocols for incident management to prioritize and resolve data freshness issues effectively. This includes setting up notifications and alerts for timely intervention, and a broader communication strategy to involve all stakeholders (even those downstream) in the case of an issue.\\n\\n### Alternatives\\n\\nWhile manual investigation and communication using tools like Slack can help triage issues, they  often result in time-consuming, inefficient, and informal processes for addressing data quality issues related to freshness, ultimately leading to lower quality outcomes. Automated freshness incident detection and structured incident management via dedicated data monitoring tools can help improve the situation by providing a single place for detecting, communicating, and coordinating to resolve data freshness issues. \\n\\n### How DataHub Can Help\\n\\nDataHub offers comprehensive features designed to tackle data freshness challenges:\\n\\n\\n**[End-To-End Data Lineage](https://docs.datahub.com/docs/features/feature-guides/lineage) and [Impact Analysis](https://docs.datahub.com/docs/act-on-metadata/impact-analysis):** Easily track the flow of data through your organization to identify, debug, and resolve delays quickly.\\n<p align=\\"center\\">\\n  <img width=\\"80%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/data-freshness/lineage.png\\"/>\\n <br />\\n  <i style={{color:\\"grey\\"}}>Data Lineage</i>\\n</p>\\n\\n\\n**Freshness Monitoring & Alerting:** Automatically detect and alert when data freshness issues occur, to ensure timely updates by proactively monitoring key datasets for updates. Check out [Assertions](https://docs.datahub.com/docs/managed-datahub/observe/assertions) and [Freshness Assertions](https://docs.datahub.com/docs/managed-datahub/observe/freshness-assertions), Available in **DataHub Cloud Only.**\\n\\n<p align=\\"center\\">\\n  <img width=\\"80%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/data-freshness/freshness-assertions.png\\"/>\\n <br />\\n  <i style={{color:\\"grey\\"}}>Freshness Assertions Results</i>\\n</p>\\n\\n\\n<p align=\\"center\\">\\n  <img width=\\"80%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/data-freshness/smart-assertions.png\\"/>\\n <br />\\n  <i style={{color:\\"grey\\"}}>Smart assertions checks for changes on a cadence based on the Table history, by default using the Audit Log.</i>\\n</p>\\n\\n\\n**[Incident Management](https://docs.datahub.com/docs/incidents/incidents)** : Centralize data incident management and begin to effectively triage, prioritize, communicate and resolve data freshness issues to all relevant stakeholders. Check out [subscription & notification](https://docs.datahub.com/docs/managed-datahub/subscription-and-notification) features as well.\\n\\n<p align=\\"center\\">\\n  <img width=\\"80%\\"  src=\\"https://raw.githubusercontent.com/datahub-project/static-assets/main/imgs/blogs/data-freshness/incidents.png\\"/>\\n</p>\\n\\n\\nBy implementing these solutions, you can ensure that your key datasets and models are always up-to-date, maintaining their relevancy, accuracy, and reliability for critical use cases within your organization. \\n\\n## Conclusion\\n\\nEnsuring data freshness is essential for the performance and reliability of critical datasets and AI/ML models. By understanding the importance of data freshness and implementing best practices and automated solutions, you can effectively manage and mitigate delays, thereby protecting your revenue and reputation. DataHub is designed to help you achieve this, providing the tools and features necessary to keep your data fresh and your operations running smoothly."}]}')}}]);