"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[24304],{15680:(e,a,t)=>{t.d(a,{xA:()=>p,yg:()=>m});var n=t(96540);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function l(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=n.createContext({}),u=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},p=function(e){var a=u(e.components);return n.createElement(s.Provider,{value:a},e.children)},d="mdxType",g={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},c=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),d=u(t),c=r,m=d["".concat(s,".").concat(c)]||d[c]||g[c]||o;return t?n.createElement(m,i(i({ref:a},p),{},{components:t})):n.createElement(m,i({ref:a},p))}));function m(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=c;var l={};for(var s in a)hasOwnProperty.call(a,s)&&(l[s]=a[s]);l.originalType=e,l[d]="string"==typeof e?e:r,i[1]=l;for(var u=2;u<o;u++)i[u]=t[u];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}c.displayName="MDXCreateElement"},6196:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>p,contentTitle:()=>s,default:()=>m,frontMatter:()=>l,metadata:()=>u,toc:()=>d});t(96540);var n=t(15680);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){return a=null!=a?a:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):function(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))})),e}function i(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}const l={title:"DataHub Upgrade Docker Image",sidebar_label:"Upgrade Docker Image",slug:"/docker/datahub-upgrade",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docker/datahub-upgrade/README.md"},s="DataHub Upgrade Docker Image",u={unversionedId:"docker/datahub-upgrade/README",id:"docker/datahub-upgrade/README",title:"DataHub Upgrade Docker Image",description:"This container is used to automatically apply upgrades from one version of DataHub to another. It contains",source:"@site/genDocs/docker/datahub-upgrade/README.md",sourceDirName:"docker/datahub-upgrade",slug:"/docker/datahub-upgrade",permalink:"/docs/docker/datahub-upgrade",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docker/datahub-upgrade/README.md",tags:[],version:"current",frontMatter:{title:"DataHub Upgrade Docker Image",sidebar_label:"Upgrade Docker Image",slug:"/docker/datahub-upgrade",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docker/datahub-upgrade/README.md"},sidebar:"overviewSidebar",previous:{title:"React Analytics",permalink:"/docs/datahub-web-react/src/app/analytics"},next:{title:"Adding a Metadata Ingestion Source",permalink:"/docs/metadata-ingestion/adding-source"}},p={},d=[{value:"Supported Upgrades",id:"supported-upgrades",level:2},{value:"Environment Variables",id:"environment-variables",level:2},{value:"Command-Line Arguments",id:"command-line-arguments",level:2},{value:"Selecting the Upgrade to Run",id:"selecting-the-upgrade-to-run",level:3},{value:"Provided Arguments for a Given Upgrade Job",id:"provided-arguments-for-a-given-upgrade-job",level:3}],g={toc:d},c="wrapper";function m(e){var{components:a}=e,t=i(e,["components"]);return(0,n.yg)(c,o(function(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{},n=Object.keys(t);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(t).filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable})))),n.forEach((function(a){r(e,a,t[a])}))}return e}({},g,t),{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"datahub-upgrade-docker-image"},"DataHub Upgrade Docker Image"),(0,n.yg)("p",null,"This container is used to automatically apply upgrades from one version of DataHub to another. It contains\na set of executable jobs, which can be used to perform various types of system maintenance on-demand."),(0,n.yg)("p",null,"It also supports regularly upgrade tasks that need to occur between versions of DataHub, and should be run\neach time DataHub is deployed. More on this below. (TODO)"),(0,n.yg)("h2",{id:"supported-upgrades"},"Supported Upgrades"),(0,n.yg)("p",null,"The following jobs are supported:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"SystemUpdate"),": Performs any tasks required to update to a new version of DataHub. For example, applying new configurations to the search & graph indexes, ingesting default settings, and more. Once completed, emits a message to the DataHub Upgrade History Kafka (",(0,n.yg)("inlineCode",{parentName:"p"},"DataHubUpgradeHistory_v1"),") topic, which signals to other pods that DataHub is ready to start.\nNote that this ",(0,n.yg)("em",{parentName:"p"},"must"),' be executed any time the DataHub version is incremented before starting or restarting other system containers. Dependent services will wait until the Kafka message is emitted corresponding to the code they are running.\nA unique "version id" is generated based on a combination of the a) embedded git tag corresponding to the version of DataHub running and b) an optional revision number, provided via the ',(0,n.yg)("inlineCode",{parentName:"p"},"DATAHUB_REVISION")," environment variable. Helm uses\nthe latter to ensure that the system upgrade job is executed every single time a deployment of DataHub is performed, even if the container version has not changed.\nImportant: This job runs as a pre-install hook via the DataHub Helm Charts, i.e. before deploying new version tags for each container.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"SystemUpdateBlocking"),": Performs any ",(0,n.yg)("em",{parentName:"p"},"blocking")," tasks required to update to a new version of DataHub, as a subset of ",(0,n.yg)("strong",{parentName:"p"},"SystemUpdate"),".")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"SystemUpdateNonBlocking"),": Performs any ",(0,n.yg)("em",{parentName:"p"},"nonblocking")," tasks required to update to a new version of DataHub, as a subset of ",(0,n.yg)("strong",{parentName:"p"},"SystemUpdate"),".")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"RestoreIndices"),": Restores indices by fetching the latest version of each aspect and restating MetadataChangeLog events for each latest aspect. Arguments include:"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},"batchSize")," (Optional): The number of rows to migrate at a time. Defaults to 1000."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},"batchDelayMs")," (Optional): The number of milliseconds of delay between migrated batches. Used for rate limiting. Defaults to 250."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},"numThreads")," (Optional): The number of threads to use, defaults to 1. Note that this is not used if ",(0,n.yg)("inlineCode",{parentName:"li"},"urnBasedPagination")," is true."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},"aspectName")," (Optional): The aspect name for producing events."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},"urn")," (Optional): The urn for producing events."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},"urnLike")," (Optional): The urn pattern for producing events, using ",(0,n.yg)("inlineCode",{parentName:"li"},"%")," as a wild card"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},"urnBasedPagination")," (Optional): Paginate the SQL results using the urn + aspect string instead of ",(0,n.yg)("inlineCode",{parentName:"li"},"OFFSET"),". Defaults to false,\nthough should improve performance for large amounts of data."))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"RestoreBackup"),": Restores the primary storage - the SQL document DB - from an available backup of the local database. Requires that the backup reader and backup are provided. Note that this does not also restore the secondary indexes, the graph or search storage. To do so, you should run the ",(0,n.yg)("strong",{parentName:"p"},"RestoreIndices")," upgrade job.\nArguments include:"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},"BACKUP_READER")," (Required): The backup reader to use to read and restore the db. The only backup reader currently supported is ",(0,n.yg)("inlineCode",{parentName:"li"},"LOCAL_PARQUET"),", which requires a parquet-formatted backup file path to be specified via the ",(0,n.yg)("inlineCode",{parentName:"li"},"BACKUP_FILE_PATH")," argument."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},"BACKUP_FILE_PATH")," (Required): The path of the backup file. If you are running in a container, this needs to the location where the backup file has been mounted into the container."))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"EvaluateTests"),": Executes all Metadata Tests in batches. Running this job can slow down DataHub, and it in some cases requires full scans of the document db. Generally, it's recommended to configure this to run one time per day (which is the helm CronJob default).\nArguments include:"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},"batchSize")," (Optional): The number of assets to test at a time. Defaults to 1000."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},"batchDelayMs")," (Optional): The number of milliseconds of delay between evaluated asset batches. Used for rate limiting. Defaults to 250."))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"(Legacy) ",(0,n.yg)("strong",{parentName:"p"},"NoCodeDataMigration"),": Performs a series of pre-flight qualification checks and then migrates metadata","*","aspect table data\nto metadata_aspect_v2 table. Arguments include:"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},"batchSize")," (Optional): The number of rows to migrate at a time. Defaults to 1000."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},"batchDelayMs")," (Optional): The number of milliseconds of delay between migrated batches. Used for rate limiting. Defaults to 250."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},"dbType")," (Optional): The target DB type. Valid values are ",(0,n.yg)("inlineCode",{parentName:"li"},"MYSQL"),", ",(0,n.yg)("inlineCode",{parentName:"li"},"MARIA"),", ",(0,n.yg)("inlineCode",{parentName:"li"},"POSTGRES"),". Defaults to ",(0,n.yg)("inlineCode",{parentName:"li"},"MYSQL"),".")),(0,n.yg)("p",{parentName:"li"},"If you are using newer versions of DataHub (v1.0.0 or above), this upgrade job will not be relevant.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"(Legacy) ",(0,n.yg)("strong",{parentName:"p"},"NoCodeDataMigrationCleanup"),": Cleanses graph index, search index, and key-value store of legacy DataHub data (metadata_aspect table) once\nthe No Code Data Migration has completed successfully. No arguments."),(0,n.yg)("p",{parentName:"li"},"If you are using newer versions of DataHub (v1.0.0 or above), this upgrade job will not be relevant."))),(0,n.yg)("h2",{id:"environment-variables"},"Environment Variables"),(0,n.yg)("p",null,"To run the ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub-upgrade")," container, some environment variables must be provided in order to tell the upgrade CLI\nwhere the running DataHub containers reside."),(0,n.yg)("p",null,"Below details the required configurations. By default, these configs are provided for local docker-compose deployments of\nDataHub within ",(0,n.yg)("inlineCode",{parentName:"p"},"docker/datahub-upgrade/env/docker.env"),". They assume that there is a Docker network called datahub_network\nwhere the DataHub containers can be found."),(0,n.yg)("p",null,"These are also the variables used when the provided ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub-upgrade.sh")," script is executed. To run the upgrade CLI for non-local deployments,\nfollow these steps:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},'Define new ".env" variable to hold your environment variables.')),(0,n.yg)("p",null,"The following variables may be provided:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-aidl"},"# Required Environment Variables\nEBEAN_DATASOURCE_USERNAME=datahub\nEBEAN_DATASOURCE_PASSWORD=datahub\nEBEAN_DATASOURCE_HOST=<your-ebean-host>:3306\nEBEAN_DATASOURCE_URL=jdbc:mysql://<your-ebean-host>:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8\nEBEAN_DATASOURCE_DRIVER=com.mysql.jdbc.Driver\n\nKAFKA_BOOTSTRAP_SERVER=<your-kafka-host>:29092\nKAFKA_SCHEMAREGISTRY_URL=http://<your-kafka-host>:8081\n\nELASTICSEARCH_HOST=<your-elastic-host>\nELASTICSEARCH_PORT=9200\n\nNEO4J_HOST=http://<your-neo-host>:7474\nNEO4J_URI=bolt://<your-neo-host>\nNEO4J_USERNAME=neo4j\nNEO4J_PASSWORD=datahub\n\nDATAHUB_GMS_HOST=<your-gms-host>>\nDATAHUB_GMS_PORT=8080\n\n# Datahub protocol (default http)\n# DATAHUB_GMS_PROTOCOL=http\n\nDATAHUB_MAE_CONSUMER_HOST=<your-mae-consumer-host>\nDATAHUB_MAE_CONSUMER_PORT=9091\n\n# Optional Arguments\n\n# Uncomment and set these to support SSL connection to Elasticsearch\n# ELASTICSEARCH_USE_SSL=\n# ELASTICSEARCH_SSL_PROTOCOL=\n# ELASTICSEARCH_SSL_SECURE_RANDOM_IMPL=\n# ELASTICSEARCH_SSL_TRUSTSTORE_FILE=\n# ELASTICSEARCH_SSL_TRUSTSTORE_TYPE=\n# ELASTICSEARCH_SSL_TRUSTSTORE_PASSWORD=\n# ELASTICSEARCH_SSL_KEYSTORE_FILE=\n# ELASTICSEARCH_SSL_KEYSTORE_TYPE=\n# ELASTICSEARCH_SSL_KEYSTORE_PASSWORD=\n")),(0,n.yg)("p",null,"These variables tell the upgrade job how to connect to critical storage systems like Kafka, MySQL / Postgres, and Elasticsearch or OpenSearch."),(0,n.yg)("ol",{start:2},(0,n.yg)("li",{parentName:"ol"},"Pull (or build) & execute the ",(0,n.yg)("inlineCode",{parentName:"li"},"datahub-upgrade")," container:")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-aidl"},"docker pull acryldata/datahub-upgrade:head && docker run --env-file *path-to-custom-env-file.env* acryldata/datahub-upgrade:head -u <Upgrade Job Name> -a <Upgrade Job Arguments>\n")),(0,n.yg)("h2",{id:"command-line-arguments"},"Command-Line Arguments"),(0,n.yg)("h3",{id:"selecting-the-upgrade-to-run"},"Selecting the Upgrade to Run"),(0,n.yg)("p",null,"The primary argument required by the datahub-upgrade container is the name of the upgrade to perform. This argument\ncan be specified using the ",(0,n.yg)("inlineCode",{parentName:"p"},"-u")," flag when running the ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub-upgrade")," container."),(0,n.yg)("p",null,'For example, to run the migration named "NoCodeDataMigration", you would do execute the following:'),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-aidl"},"./datahub-upgrade.sh -u NoCodeDataMigration\n")),(0,n.yg)("p",null,"OR"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-aidl"},"docker pull acryldata/datahub-upgrade:head && docker run --env-file env/docker.env acryldata/datahub-upgrade:head -u NoCodeDataMigration\n")),(0,n.yg)("h3",{id:"provided-arguments-for-a-given-upgrade-job"},"Provided Arguments for a Given Upgrade Job"),(0,n.yg)("p",null,"In addition to the required ",(0,n.yg)("inlineCode",{parentName:"p"},"-u")," argument, each upgrade may require specific arguments. You can provide arguments to individual\nupgrades using multiple ",(0,n.yg)("inlineCode",{parentName:"p"},"-a")," arguments."),(0,n.yg)("p",null,"For example, the NoCodeDataMigration upgrade provides 2 optional arguments detailed above: ",(0,n.yg)("em",{parentName:"p"},"batchSize")," and ",(0,n.yg)("em",{parentName:"p"},"batchDelayMs"),".\nTo specify these, you can use a combination of ",(0,n.yg)("inlineCode",{parentName:"p"},"-a")," arguments and of the form ",(0,n.yg)("em",{parentName:"p"},"argumentName=argumentValue")," as follows:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-aidl"},"./datahub-upgrade.sh -u NoCodeDataMigration -a batchSize=500 -a batchDelayMs=1000 // Small batches with 1 second delay.\n")),(0,n.yg)("p",null,"OR"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-aidl"},"docker pull acryldata/datahub-upgrade:head && docker run --env-file env/docker.env acryldata/datahub-upgrade:head -u NoCodeDataMigration -a batchSize=500 -a batchDelayMs=1000\n")))}m.isMDXComponent=!0}}]);