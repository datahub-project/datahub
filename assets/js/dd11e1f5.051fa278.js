"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[75372],{25207:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>m,contentTitle:()=>p,default:()=>_,frontMatter:()=>l,metadata:()=>u,toc:()=>c});t(96540);var n=t(15680),r=t(53720),i=t(5400);function o(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function s(e,a){return a=null!=a?a:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):function(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))})),e}function d(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}const l={title:"Emitting Patch Updates to DataHub",slug:"/advanced/patch",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/advanced/patch.md"},p="Emitting Patch Updates to DataHub",u={unversionedId:"docs/advanced/patch",id:"docs/advanced/patch",title:"Emitting Patch Updates to DataHub",description:"Why Would You Use Patch",source:"@site/genDocs/docs/advanced/patch.md",sourceDirName:"docs/advanced",slug:"/advanced/patch",permalink:"/docs/advanced/patch",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/advanced/patch.md",tags:[],version:"current",frontMatter:{title:"Emitting Patch Updates to DataHub",slug:"/advanced/patch",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/advanced/patch.md"},sidebar:"overviewSidebar",previous:{title:"Terms",permalink:"/docs/api/tutorials/terms"},next:{title:"Search",permalink:"/docs/api/tutorials/sdk/search_client"}},m={},c=[{value:"Why Would You Use Patch",id:"why-would-you-use-patch",level:2},{value:"How To Use Patches",id:"how-to-use-patches",level:2},{value:"Add &amp; Remove Owners for Dataset",id:"add--remove-owners-for-dataset",level:3},{value:"Add &amp; Remove Tags for Dataset",id:"add--remove-tags-for-dataset",level:3},{value:"Add &amp; Remove Glossary Terms for Dataset",id:"add--remove-glossary-terms-for-dataset",level:3},{value:"Add &amp; Remove Structured Properties for Dataset",id:"add--remove-structured-properties-for-dataset",level:3},{value:"Add &amp; Remove Upstream Lineage for Dataset",id:"add--remove-upstream-lineage-for-dataset",level:3},{value:"Add &amp; Remove Read-Only Custom Properties for Dataset",id:"add--remove-read-only-custom-properties-for-dataset",level:3},{value:"Add &amp; Remove Data Job Lineage",id:"add--remove-data-job-lineage",level:3},{value:"Advanced: How Patch works",id:"advanced-how-patch-works",level:2},{value:"Path",id:"path",level:3},{value:"Examples",id:"examples",level:4},{value:"Operation",id:"operation",level:3},{value:"Add",id:"add",level:4},{value:"Remove",id:"remove",level:4},{value:"Value",id:"value",level:3},{value:"Examples",id:"examples-1",level:4},{value:"Implementation details",id:"implementation-details",level:3},{value:"Template Classes",id:"template-classes",level:4},{value:"ArrayMergingTemplate &amp; CompoundKeyTemplate",id:"arraymergingtemplate--compoundkeytemplate",level:5},{value:"PatchBuilders",id:"patchbuilders",level:4}],h={toc:c},g="wrapper";function _(e){var{components:a}=e,t=d(e,["components"]);return(0,n.yg)(g,s(function(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{},n=Object.keys(t);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(t).filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable})))),n.forEach((function(a){o(e,a,t[a])}))}return e}({},h,t),{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"emitting-patch-updates-to-datahub"},"Emitting Patch Updates to DataHub"),(0,n.yg)("h2",{id:"why-would-you-use-patch"},"Why Would You Use Patch"),(0,n.yg)("p",null,"By default, most of the SDK tutorials and APIs involve applying full upserts at the aspect level, e.g. replacing the aspect entirely.\nThis means that when you want to change even a single field within an aspect without modifying others, you need to do a read-modify-write to avoid overwriting existing fields.\nTo support these scenarios, DataHub supports ",(0,n.yg)("inlineCode",{parentName:"p"},"PATCH")," operations to perform targeted changes for individual fields or values within arrays of fields are possible without impacting other existing metadata."),(0,n.yg)("admonition",{type:"note"},(0,n.yg)("p",{parentName:"admonition"},"PATCH support is now supported generically via ",(0,n.yg)("a",{parentName:"p",href:"/docs/api/openapi/openapi-usage-guide#generic-patching"},"OpenAPI"),". Traditional PATCH support is only available for a selected set of aspects. The complete list of Aspects that are supported are maintained by the ",(0,n.yg)("inlineCode",{parentName:"p"},"SUPPORTED_TEMPLATES")," constant ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/entity-registry/src/main/java/com/linkedin/metadata/aspect/patch/template/AspectTemplateEngine.java#L23"},"here"),".")),(0,n.yg)("h2",{id:"how-to-use-patches"},"How To Use Patches"),(0,n.yg)("p",null,"The Patch builders are available in both Python and Java SDKs:"),(0,n.yg)(r.A,{groupId:"sdk-language",mdxType:"Tabs"},(0,n.yg)(i.A,{value:"python",label:"Python SDK",default:!0,mdxType:"TabItem"},(0,n.yg)("p",null,"The Python Patch builders are entity-oriented and located in the ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/tree/9588440549f3d99965085e97b214a7dabc181ed2/metadata-ingestion/src/datahub/specific"},"metadata-ingestion")," module and located in the ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub.specific")," module.\nPatch builder helper classes exist for"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/specific/dataset.py"},"Datasets")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/specific/chart.py"},"Charts")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/specific/dashboard.py"},"Dashboards")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/specific/datajob.py"},"Data Jobs (Tasks)")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/specific/dataproduct.py"},"Data Products"))),(0,n.yg)("p",null,"And we are gladly accepting contributions for Containers, Data Flows (Pipelines), Tags, Glossary Terms, Domains, and ML Models.")),(0,n.yg)(i.A,{value:"java",label:"Java SDK",mdxType:"TabItem"},(0,n.yg)("p",null,"The Java Patch builders are aspect-oriented and located in the ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/tree/master/metadata-integration/java/datahub-client/src/main/java/datahub/client/patch"},"datahub-client")," module under the ",(0,n.yg)("inlineCode",{parentName:"p"},"datahub.client.patch")," namespace."))),(0,n.yg)("h3",{id:"add--remove-owners-for-dataset"},"Add & Remove Owners for Dataset"),(0,n.yg)("p",null,"To add & remove specific owners for a dataset:"),(0,n.yg)(r.A,{groupId:"sdk-language",mdxType:"Tabs"},(0,n.yg)(i.A,{value:"python",label:"Python SDK",default:!0,mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'# Inlined from /metadata-ingestion/examples/library/dataset_add_owner_patch.py\nfrom datahub.emitter.mce_builder import make_dataset_urn, make_group_urn, make_user_urn\nfrom datahub.ingestion.graph.client import DataHubGraph, DataHubGraphConfig\nfrom datahub.metadata.schema_classes import OwnerClass, OwnershipTypeClass\nfrom datahub.specific.dataset import DatasetPatchBuilder\n\n# Create DataHub Client\ndatahub_client = DataHubGraph(DataHubGraphConfig(server="http://localhost:8080"))\n\n# Create Dataset URN\ndataset_urn = make_dataset_urn(\n    platform="snowflake", name="fct_users_created", env="PROD"\n)\n\n# Create Dataset Patch to Add + Remove Owners\npatch_builder = DatasetPatchBuilder(dataset_urn)\npatch_builder.add_owner(\n    OwnerClass(make_user_urn("user-to-add-id"), OwnershipTypeClass.TECHNICAL_OWNER)\n)\npatch_builder.remove_owner(make_group_urn("group-to-remove-id"))\npatch_mcps = patch_builder.build()\n\n# Emit Dataset Patch\nfor patch_mcp in patch_mcps:\n    datahub_client.emit(patch_mcp)\n\n')))),(0,n.yg)("h3",{id:"add--remove-tags-for-dataset"},"Add & Remove Tags for Dataset"),(0,n.yg)("p",null,"To add & remove specific tags for a dataset:"),(0,n.yg)(r.A,{groupId:"sdk-language",mdxType:"Tabs"},(0,n.yg)(i.A,{value:"python",label:"Python SDK",default:!0,mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'# Inlined from /metadata-ingestion/examples/library/dataset_add_tag_patch.py\nfrom datahub.emitter.mce_builder import make_tag_urn\nfrom datahub.metadata.schema_classes import TagAssociationClass\nfrom datahub.sdk import DataHubClient, DatasetUrn\nfrom datahub.specific.dataset import DatasetPatchBuilder\n\nclient = DataHubClient.from_env()\n\n# Create the Dataset updater.\npatch_builder = DatasetPatchBuilder(\n    DatasetUrn(platform="snowflake", name="fct_users_created", env="PROD")\n)\npatch_builder.add_tag(TagAssociationClass(make_tag_urn("tag-to-add-id")))\npatch_builder.remove_tag("urn:li:tag:tag-to-remove-id")\n\n# Do the update.\nclient.entities.update(patch_builder)\n\n')),(0,n.yg)("p",null,"And for a specific schema field within the Dataset:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'# Inlined from /metadata-ingestion/examples/library/dataset_field_add_tag_patch.py\nfrom datahub.emitter.mce_builder import make_dataset_urn, make_tag_urn\nfrom datahub.ingestion.graph.client import DataHubGraph, DataHubGraphConfig\nfrom datahub.metadata.schema_classes import TagAssociationClass\nfrom datahub.specific.dataset import DatasetPatchBuilder\n\n# Create DataHub Client\ndatahub_client = DataHubGraph(DataHubGraphConfig(server="http://localhost:8080"))\n\n# Create Dataset URN\ndataset_urn = make_dataset_urn(\n    platform="snowflake", name="fct_users_created", env="PROD"\n)\n\n# Create Dataset Patch to Add + Remove Tag for \'profile_id\' column\npatch_builder = DatasetPatchBuilder(dataset_urn)\npatch_builder.for_field("profile_id").add_tag(\n    TagAssociationClass(make_tag_urn("tag-to-add-id"))\n)\npatch_builder.for_field("profile_id").remove_tag("urn:li:tag:tag-to-remove-id")\npatch_mcps = patch_builder.build()\n\n# Emit Dataset Patch\nfor patch_mcp in patch_mcps:\n    datahub_client.emit(patch_mcp)\n\n')))),(0,n.yg)("h3",{id:"add--remove-glossary-terms-for-dataset"},"Add & Remove Glossary Terms for Dataset"),(0,n.yg)("p",null,"To add & remove specific glossary terms for a dataset:"),(0,n.yg)(r.A,{groupId:"sdk-language",mdxType:"Tabs"},(0,n.yg)(i.A,{value:"python",label:"Python SDK",default:!0,mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'# Inlined from /metadata-ingestion/examples/library/dataset_add_glossary_term_patch.py\nfrom datahub.emitter.mce_builder import make_dataset_urn, make_term_urn\nfrom datahub.ingestion.graph.client import DataHubGraph, DataHubGraphConfig\nfrom datahub.metadata.schema_classes import GlossaryTermAssociationClass\nfrom datahub.specific.dataset import DatasetPatchBuilder\n\n# Create DataHub Client\ndatahub_client = DataHubGraph(DataHubGraphConfig(server="http://localhost:8080"))\n\n# Create Dataset URN\ndataset_urn = make_dataset_urn(\n    platform="snowflake", name="fct_users_created", env="PROD"\n)\n\n# Create Dataset Patch to Add + Remove Term for \'profile_id\' column\npatch_builder = DatasetPatchBuilder(dataset_urn)\npatch_builder.add_term(GlossaryTermAssociationClass(make_term_urn("term-to-add-id")))\npatch_builder.remove_term(make_term_urn("term-to-remove-id"))\npatch_mcps = patch_builder.build()\n\n# Emit Dataset Patch\nfor patch_mcp in patch_mcps:\n    datahub_client.emit(patch_mcp)\n\n')),(0,n.yg)("p",null,"And for a specific schema field within the Dataset:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'# Inlined from /metadata-ingestion/examples/library/dataset_field_add_glossary_term_patch.py\nfrom datahub.emitter.mce_builder import make_dataset_urn, make_term_urn\nfrom datahub.ingestion.graph.client import DataHubGraph, DataHubGraphConfig\nfrom datahub.metadata.schema_classes import GlossaryTermAssociationClass\nfrom datahub.specific.dataset import DatasetPatchBuilder\n\n# Create DataHub Client\ndatahub_client = DataHubGraph(DataHubGraphConfig(server="http://localhost:8080"))\n\n# Create Dataset URN\ndataset_urn = make_dataset_urn(\n    platform="snowflake", name="fct_users_created", env="PROD"\n)\n\n# Create Dataset Patch to Add + Remove Term for \'profile_id\' column\npatch_builder = DatasetPatchBuilder(dataset_urn)\npatch_builder.for_field("profile_id").add_term(\n    GlossaryTermAssociationClass(make_term_urn("term-to-add-id"))\n)\npatch_builder.for_field("profile_id").remove_term(\n    "urn:li:glossaryTerm:term-to-remove-id"\n)\npatch_mcps = patch_builder.build()\n\n# Emit Dataset Patch\nfor patch_mcp in patch_mcps:\n    datahub_client.emit(patch_mcp)\n\n')))),(0,n.yg)("h3",{id:"add--remove-structured-properties-for-dataset"},"Add & Remove Structured Properties for Dataset"),(0,n.yg)("p",null,"To add & remove structured properties for a dataset:"),(0,n.yg)(r.A,{groupId:"sdk-language",mdxType:"Tabs"},(0,n.yg)(i.A,{value:"python",label:"Python SDK",default:!0,mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'# Inlined from /metadata-ingestion/examples/library/dataset_add_structured_properties_patch.py\nfrom datahub.emitter.mce_builder import make_dataset_urn\nfrom datahub.ingestion.graph.client import DataHubGraph, DataHubGraphConfig\nfrom datahub.specific.dataset import DatasetPatchBuilder\n\n# Create DataHub Client\ndatahub_client = DataHubGraph(DataHubGraphConfig(server="http://localhost:8080"))\n\n# Create Dataset URN\ndataset_urn = make_dataset_urn(platform="hive", name="fct_users_created", env="PROD")\n\n# Create Dataset Patch to Add and Remove Structured Properties\npatch_builder = DatasetPatchBuilder(dataset_urn)\npatch_builder.add_structured_property(\n    "urn:li:structuredProperty:retentionTimeInDays", 12\n)\npatch_builder.remove_structured_property(\n    "urn:li:structuredProperty:customClassification"\n)\npatch_mcps = patch_builder.build()\n\n# Emit Dataset Patch\nfor patch_mcp in patch_mcps:\n    datahub_client.emit(patch_mcp)\n\n')))),(0,n.yg)("h3",{id:"add--remove-upstream-lineage-for-dataset"},"Add & Remove Upstream Lineage for Dataset"),(0,n.yg)("p",null,"To add & remove a lineage edge connecting a dataset to it's upstream or input at both the dataset and schema field level:"),(0,n.yg)(r.A,{groupId:"sdk-language",mdxType:"Tabs"},(0,n.yg)(i.A,{value:"python",label:"Python SDK",default:!0,mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'# Inlined from /metadata-ingestion/examples/library/dataset_add_upstream_lineage_patch.py\nfrom datahub.emitter.mce_builder import make_dataset_urn, make_schema_field_urn\nfrom datahub.ingestion.graph.client import DataHubGraph, DataHubGraphConfig\nfrom datahub.metadata.schema_classes import (\n    DatasetLineageTypeClass,\n    FineGrainedLineageClass,\n    FineGrainedLineageUpstreamTypeClass,\n    UpstreamClass,\n)\nfrom datahub.specific.dataset import DatasetPatchBuilder\n\n# Create DataHub Client\ndatahub_client = DataHubGraph(DataHubGraphConfig(server="http://localhost:8080"))\n\n# Create Dataset URN\ndataset_urn = make_dataset_urn(\n    platform="snowflake", name="fct_users_created", env="PROD"\n)\nupstream_to_remove_urn = make_dataset_urn(\n    platform="s3", name="fct_users_old", env="PROD"\n)\nupstream_to_add_urn = make_dataset_urn(platform="s3", name="fct_users_new", env="PROD")\n\n# Create Dataset Patch to Add & Remove Upstream Lineage Edges\npatch_builder = DatasetPatchBuilder(dataset_urn)\npatch_builder.remove_upstream_lineage(upstream_to_remove_urn)\npatch_builder.add_upstream_lineage(\n    UpstreamClass(upstream_to_add_urn, DatasetLineageTypeClass.TRANSFORMED)\n)\n\n# ...And also include schema field lineage\nupstream_field_to_add_urn = make_schema_field_urn(upstream_to_add_urn, "profile_id")\ndownstream_field_to_add_urn = make_schema_field_urn(dataset_urn, "profile_id")\n\npatch_builder.add_fine_grained_lineage(\n    FineGrainedLineageClass(\n        FineGrainedLineageUpstreamTypeClass.FIELD_SET,\n        FineGrainedLineageUpstreamTypeClass.FIELD_SET,\n        [upstream_field_to_add_urn],\n        [downstream_field_to_add_urn],\n    )\n)\n\nupstream_field_to_remove_urn = make_schema_field_urn(\n    upstream_to_remove_urn, "profile_id"\n)\ndownstream_field_to_remove_urn = make_schema_field_urn(dataset_urn, "profile_id")\n\npatch_builder.remove_fine_grained_lineage(\n    FineGrainedLineageClass(\n        FineGrainedLineageUpstreamTypeClass.FIELD_SET,\n        FineGrainedLineageUpstreamTypeClass.FIELD_SET,\n        [upstream_field_to_remove_urn],\n        [downstream_field_to_remove_urn],\n    )\n)\n\npatch_mcps = patch_builder.build()\n\n\n# Emit Dataset Patch\nfor patch_mcp in patch_mcps:\n    datahub_client.emit(patch_mcp)\n\n')))),(0,n.yg)("h3",{id:"add--remove-read-only-custom-properties-for-dataset"},"Add & Remove Read-Only Custom Properties for Dataset"),(0,n.yg)("p",null,"To add & remove specific custom properties for a dataset:"),(0,n.yg)(r.A,{groupId:"sdk-language",mdxType:"Tabs"},(0,n.yg)(i.A,{value:"python",label:"Python SDK",default:!0,mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'# Inlined from /metadata-ingestion/examples/library/dataset_add_remove_custom_properties_patch.py\nfrom datahub.emitter.mce_builder import make_dataset_urn\nfrom datahub.ingestion.graph.client import DataHubGraph, DataHubGraphConfig\nfrom datahub.specific.dataset import DatasetPatchBuilder\n\n# Create DataHub Client\ndatahub_client = DataHubGraph(DataHubGraphConfig(server="http://localhost:8080"))\n\n# Create Dataset URN\ndataset_urn = make_dataset_urn(platform="hive", name="fct_users_created", env="PROD")\n\n# Create Dataset Patch to Add + Remove Custom Properties\npatch_builder = DatasetPatchBuilder(dataset_urn)\npatch_builder.add_custom_property("cluster_name", "datahubproject.acryl.io")\npatch_builder.remove_custom_property("retention_time")\npatch_mcps = patch_builder.build()\n\n# Emit Dataset Patch\nfor patch_mcp in patch_mcps:\n    datahub_client.emit(patch_mcp)\n\n'))),(0,n.yg)(i.A,{value:"java",label:"Java SDK",mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'# Inlined from /metadata-integration/java/examples/src/main/java/io/datahubproject/examples/DatasetCustomPropertiesAddRemove.java\npackage io.datahubproject.examples;\n\nimport com.linkedin.common.urn.UrnUtils;\nimport com.linkedin.metadata.aspect.patch.builder.DatasetPropertiesPatchBuilder;\nimport com.linkedin.mxe.MetadataChangeProposal;\nimport datahub.client.MetadataWriteResponse;\nimport datahub.client.rest.RestEmitter;\nimport java.io.IOException;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport lombok.extern.slf4j.Slf4j;\n\n@Slf4j\nclass DatasetCustomPropertiesAddRemove {\n\n  private DatasetCustomPropertiesAddRemove() {}\n\n  /**\n   * Applies Add and Remove property operations on an existing custom properties aspect without\n   * affecting any other properties\n   *\n   * @param args\n   * @throws IOException\n   * @throws ExecutionException\n   * @throws InterruptedException\n   */\n  public static void main(String[] args)\n      throws IOException, ExecutionException, InterruptedException {\n    MetadataChangeProposal datasetPropertiesProposal =\n        new DatasetPropertiesPatchBuilder()\n            .urn(UrnUtils.toDatasetUrn("hive", "fct_users_deleted", "PROD"))\n            .addCustomProperty("cluster_name", "datahubproject.acryl.io")\n            .removeCustomProperty("retention_time")\n            .build();\n\n    String token = "";\n    RestEmitter emitter = RestEmitter.create(b -> b.server("http://localhost:8080").token(token));\n    try {\n      Future<MetadataWriteResponse> response = emitter.emit(datasetPropertiesProposal);\n\n      System.out.println(response.get().getResponseContent());\n    } catch (Exception e) {\n      log.error("Failed to emit metadata to DataHub", e);\n      throw e;\n    } finally {\n      emitter.close();\n    }\n  }\n}\n\n')))),(0,n.yg)("h3",{id:"add--remove-data-job-lineage"},"Add & Remove Data Job Lineage"),(0,n.yg)(r.A,{groupId:"sdk-language",mdxType:"Tabs"},(0,n.yg)(i.A,{value:"python",label:"Python SDK",default:!0,mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'# Inlined from /metadata-ingestion/examples/library/datajob_add_lineage_patch.py\nfrom datahub.emitter.mce_builder import (\n    make_data_job_urn,\n    make_dataset_urn,\n    make_schema_field_urn,\n)\nfrom datahub.ingestion.graph.client import DataHubGraph, DataHubGraphConfig\nfrom datahub.metadata.schema_classes import (\n    FineGrainedLineageClass as FineGrainedLineage,\n    FineGrainedLineageDownstreamTypeClass as FineGrainedLineageDownstreamType,\n    FineGrainedLineageUpstreamTypeClass as FineGrainedLineageUpstreamType,\n)\nfrom datahub.specific.datajob import DataJobPatchBuilder\n\n# Create DataHub Client\ndatahub_client = DataHubGraph(DataHubGraphConfig(server="http://localhost:8080"))\n\n# Create DataJob URN\ndatajob_urn = make_data_job_urn(\n    orchestrator="airflow", flow_id="dag_abc", job_id="task_456"\n)\n\n# Create DataJob Patch to Add Lineage\npatch_builder = DataJobPatchBuilder(datajob_urn)\npatch_builder.add_input_dataset(\n    make_dataset_urn(platform="kafka", name="SampleKafkaDataset", env="PROD")\n)\npatch_builder.add_output_dataset(\n    make_dataset_urn(platform="hive", name="SampleHiveDataset", env="PROD")\n)\npatch_builder.add_input_datajob(\n    make_data_job_urn(orchestrator="airflow", flow_id="dag_abc", job_id="task_123")\n)\npatch_builder.add_input_dataset_field(\n    make_schema_field_urn(\n        parent_urn=make_dataset_urn(\n            platform="hive", name="fct_users_deleted", env="PROD"\n        ),\n        field_path="user_id",\n    )\n)\npatch_builder.add_output_dataset_field(\n    make_schema_field_urn(\n        parent_urn=make_dataset_urn(\n            platform="hive", name="fct_users_created", env="PROD"\n        ),\n        field_path="user_id",\n    )\n)\n\n# Update column-level lineage through the Data Job\nlineage1 = FineGrainedLineage(\n    upstreamType=FineGrainedLineageUpstreamType.FIELD_SET,\n    upstreams=[\n        make_schema_field_urn(make_dataset_urn("postgres", "raw_data.users"), "user_id")\n    ],\n    downstreamType=FineGrainedLineageDownstreamType.FIELD,\n    downstreams=[\n        make_schema_field_urn(\n            make_dataset_urn("postgres", "analytics.user_metrics"),\n            "user_id",\n        )\n    ],\n    transformOperation="IDENTITY",\n    confidenceScore=1.0,\n)\npatch_builder.add_fine_grained_lineage(lineage1)\npatch_builder.remove_fine_grained_lineage(lineage1)\n# Replaces all existing fine-grained lineages\npatch_builder.set_fine_grained_lineages([lineage1])\n\npatch_mcps = patch_builder.build()\n\n# Emit DataJob Patch\nfor patch_mcp in patch_mcps:\n    datahub_client.emit(patch_mcp)\n\n'))),(0,n.yg)(i.A,{value:"java",label:"Java SDK",mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'# Inlined from /metadata-integration/java/examples/src/main/java/io/datahubproject/examples/DataJobLineageAdd.java\npackage io.datahubproject.examples;\n\nimport com.linkedin.common.urn.DataJobUrn;\nimport com.linkedin.common.urn.DatasetUrn;\nimport com.linkedin.common.urn.UrnUtils;\nimport com.linkedin.metadata.aspect.patch.builder.DataJobInputOutputPatchBuilder;\nimport com.linkedin.mxe.MetadataChangeProposal;\nimport datahub.client.MetadataWriteResponse;\nimport datahub.client.rest.RestEmitter;\nimport java.io.IOException;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport lombok.extern.slf4j.Slf4j;\n\n@Slf4j\nclass DataJobLineageAdd {\n\n  private DataJobLineageAdd() {}\n\n  /**\n   * Adds lineage to an existing DataJob without affecting any lineage\n   *\n   * @param args\n   * @throws IOException\n   * @throws ExecutionException\n   * @throws InterruptedException\n   */\n  public static void main(String[] args)\n      throws IOException, ExecutionException, InterruptedException {\n    String token = "";\n    try (RestEmitter emitter =\n        RestEmitter.create(b -> b.server("http://localhost:8080").token(token))) {\n      MetadataChangeProposal dataJobIOPatch =\n          new DataJobInputOutputPatchBuilder()\n              .urn(\n                  UrnUtils.getUrn(\n                      "urn:li:dataJob:(urn:li:dataFlow:(airflow,dag_abc,PROD),task_456)"))\n              .addInputDatasetEdge(\n                  DatasetUrn.createFromString(\n                      "urn:li:dataset:(urn:li:dataPlatform:kafka,SampleKafkaDataset,PROD)"))\n              .addOutputDatasetEdge(\n                  DatasetUrn.createFromString(\n                      "urn:li:dataset:(urn:li:dataPlatform:kafka,SampleHiveDataset,PROD)"))\n              .addInputDatajobEdge(\n                  DataJobUrn.createFromString(\n                      "urn:li:dataJob:(urn:li:dataFlow:(airflow,dag_abc,PROD),task_123)"))\n              .addInputDatasetField(\n                  UrnUtils.getUrn(\n                      "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_deleted,PROD),user_id)"))\n              .addOutputDatasetField(\n                  UrnUtils.getUrn(\n                      "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_created,PROD),user_id)"))\n              .build();\n\n      Future<MetadataWriteResponse> response = emitter.emit(dataJobIOPatch);\n\n      System.out.println(response.get().getResponseContent());\n    } catch (Exception e) {\n      log.error("Failed to emit metadata to DataHub", e);\n      throw new RuntimeException(e);\n    }\n  }\n}\n\n')))),(0,n.yg)("h2",{id:"advanced-how-patch-works"},"Advanced: How Patch works"),(0,n.yg)("p",null,"To understand how patching works, it's important to understand a bit about our ",(0,n.yg)("a",{parentName:"p",href:"/docs/what/aspect"},"models"),". Entities are comprised of Aspects\nwhich can be reasoned about as JSON representations of the object models. To be able to patch these we utilize ",(0,n.yg)("a",{parentName:"p",href:"https://jsonpatch.com/"},"JsonPatch"),". The components of a JSON Patch are the path, operation, and value."),(0,n.yg)("h3",{id:"path"},"Path"),(0,n.yg)("p",null,"The JSON path refers to a value within the schema. This can be a single field or can be an entire object reference depending on what the path is.\nFor our patches we are primarily targeting single fields or even single array elements within a field. To be able to target array elements by id, we go through a translation process\nof the schema to transform arrays into maps. This allows a path to reference a particular array element by key rather than by index, for example a specific tag urn being added to a dataset."),(0,n.yg)("h4",{id:"examples"},"Examples"),(0,n.yg)("p",null,"A patch path for targeting an upstream dataset:"),(0,n.yg)("p",null,(0,n.yg)("inlineCode",{parentName:"p"},"/upstreams/urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_created_upstream,PROD)")),(0,n.yg)("p",null,"Breakdown:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"/upstreams")," -> References the upstreams field of the UpstreamLineage aspect, this is an array of Upstream objects where the key is the Urn"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"/urn:...")," -> The dataset to be targeted by the operation")),(0,n.yg)("p",null,"A patch path for targeting a fine-grained lineage upstream:"),(0,n.yg)("p",null,(0,n.yg)("inlineCode",{parentName:"p"},"/fineGrainedLineages/TRANSFORM/urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_created,PROD),foo)/urn:li:query:queryId/urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_created_upstream,PROD),bar)")),(0,n.yg)("p",null,"Breakdown:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"/fineGrainedLineages")," -> References the fineGrainedLineages field on an UpstreamLineage, this is an array of FineGrainedLineage objects keyed on transformOperation, downstream urn, and query urn"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"/TRANSFORM")," -> transformOperation, one of the fields determining the key for a fineGrainedLineage"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"/urn:li:schemaField:...")," -> The downstream schemaField referenced in this schema, part of the key for a fineGrainedLineage"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"/urn:li:query:...")," -> The query urn this relationship was derived from, part of the key for a fineGrainedLineage"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"/urn:li:schemaField:")," -> The upstream urn that is being targeted by this patch operation")),(0,n.yg)("p",null,"This showcases that in some cases the key for objects is simple, in others in can be complex to determine, but for our fully supported use cases we have\nSDK support on both the Java and Python side that will generate these patches for you as long as you supply the required method parameters.\nPath is generally the most complicated portion of a patch to reason about as it requires intimate knowledge of the schema and models."),(0,n.yg)("h3",{id:"operation"},"Operation"),(0,n.yg)("p",null,"Operation is a limited enum of a few supported types pulled directly from the JSON Patch spec. DataHub only supports ",(0,n.yg)("inlineCode",{parentName:"p"},"ADD")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"REMOVE")," of these options\nas the other patch operations do not currently have a use case within our system."),(0,n.yg)("h4",{id:"add"},"Add"),(0,n.yg)("p",null,"Add is a bit of a misnomer for the JSON Patch spec, it is not an explicit add but an upsert/replace. If the path specified does not exist, it will be created,\nbut if the path already exists the value will be replaced. Patch operations apply at a path level so it is possible to do full replaces of arrays or objects in the schema\nusing adds, but generally the most useful use case for patch is to add elements to arrays without affecting the other elements as full upserts are supported by standard ingestion."),(0,n.yg)("h4",{id:"remove"},"Remove"),(0,n.yg)("p",null,"Remove operations require the path specified to be present, or an error will be thrown, otherwise they operate as one would expect. The specified path will be removed from the aspect."),(0,n.yg)("h3",{id:"value"},"Value"),(0,n.yg)("p",null,"Value is the actual information that will be stored at a path. If the path references an object then this will include the JSON key value pairs for that object."),(0,n.yg)("h4",{id:"examples-1"},"Examples"),(0,n.yg)("p",null,"An example UpstreamLineage object value:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "auditStamp": {\n    "time": 0,\n    "actor": "urn:li:corpuser:unknown"\n  },\n  "dataset": "urn:li:dataset:(urn:li:dataPlatform:s3,my-bucket/my-folder/my-file.txt,PROD)",\n  "type": "TRANSFORMED"\n}\n')),(0,n.yg)("p",null,"For the previous path example (",(0,n.yg)("inlineCode",{parentName:"p"},"/upstreams/urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_created_upstream,PROD)"),"), this object would represent the UpstreamLineage object for that path.\nThis specifies the required fields to properly represent that object. Note: by modifying this path, you could reference a single field within the UpstreamLineage object itself, like so:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "path": "/upstreams/urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_created_upstream,PROD)/type",\n  "op": "ADD",\n  "value": "VIEW"\n}\n')),(0,n.yg)("h3",{id:"implementation-details"},"Implementation details"),(0,n.yg)("h4",{id:"template-classes"},"Template Classes"),(0,n.yg)("p",null,"Template classes are the mechanism that maps fields to their corresponding JSON paths. Since DataMaps are not true JSON, first we convert a RecordTemplate to a JSON String,\nperform any additional process to map array fields to their keys, apply the patch, and then convert the JSON object back to a RecordTemplate to work with the rest of the application."),(0,n.yg)("p",null,"The template classes we currently support can be found in the ",(0,n.yg)("inlineCode",{parentName:"p"},"entity-registry")," module. They are split up by aspect, with the GenericTemplate applying to any non-directly supported aspects.\nThe GenericTemplate allows for use cases that we have not gotten around to directly support yet, but puts more burden on the user to generate patches correctly."),(0,n.yg)("p",null,"The template classes are utilized in ",(0,n.yg)("inlineCode",{parentName:"p"},"EntityServiceImpl")," where a MCP is determined to be either a patch or standard upsert which then routes through to the stored templates registered on the EntityRegistry.\nThe core logical flow each Template runs through is set up in the ",(0,n.yg)("inlineCode",{parentName:"p"},"Template")," interface, with some more specific logic in the lower level interfaces for constructing/deconstructing array field keys.\nMost of the complexity around these classes is knowledge of schema and JSON path traversals."),(0,n.yg)("h5",{id:"arraymergingtemplate--compoundkeytemplate"},"ArrayMergingTemplate & CompoundKeyTemplate"),(0,n.yg)("p",null,(0,n.yg)("inlineCode",{parentName:"p"},"ArrayMergingTemplate")," is utilized for any aspect which has array fields and may either be used directly or use ",(0,n.yg)("inlineCode",{parentName:"p"},"CompoundKeyTemplate"),". ",(0,n.yg)("inlineCode",{parentName:"p"},"ArrayMergingTemplate")," is the simpler one that can only be used directly for\nsingle value keys. ",(0,n.yg)("inlineCode",{parentName:"p"},"CompoundKeyTemplate")," allows for support of multi-field keys. For more complex examples like FineGrainedLineage, further logic is needed to construct a key as it is not generalizable to other aspects, see ",(0,n.yg)("inlineCode",{parentName:"p"},"UpstreamLineageTemplate")," for full special case implementation."),(0,n.yg)("h4",{id:"patchbuilders"},"PatchBuilders"),(0,n.yg)("p",null,"There are patch builder SDK classes for constructing patches in both Java and Python. The Java patch builders all extend ",(0,n.yg)("inlineCode",{parentName:"p"},"AbstractMultiFieldPatchBuilder")," which sets up the\nbase functionality for patch builder subtypes. Each implementation of this abstract class is targeted at a particular aspect and contains specific field based update methods\nfor the most common use cases. On the Python side patch builders live in the ",(0,n.yg)("inlineCode",{parentName:"p"},"src/specific/")," directory and are organized by entity type."))}_.isMDXComponent=!0}}]);