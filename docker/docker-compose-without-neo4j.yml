# Docker compose file covering DataHub's default configuration, which is to run all containers on a single host.

# Please see the README.md for instructions as to how to use and customize.

# NOTE: This file will cannot build! No dockerfiles are set. See the README.md in this directory.
---
version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:5.4.0
    env_file: zookeeper/env/docker.env
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - ${DATAHUB_MAPPED_ZK_PORT:-2181}:2181
    volumes:
      - zkdata:/var/lib/zookeeper

  broker:
    image: confluentinc/cp-kafka:5.4.0
    env_file: broker/env/docker.env
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    ports:
      - ${DATAHUB_MAPPED_KAFKA_BROKER_PORT:-9092}:9092

  # This "container" is a workaround to pre-create topics
  kafka-setup:
    build:
      context: kafka-setup
    image: ${DATAHUB_KAFKA_SETUP_IMAGE:-linkedin/datahub-kafka-setup}:${DATAHUB_VERSION:-head}
    env_file: kafka-setup/env/docker.env
    hostname: kafka-setup
    container_name: kafka-setup
    depends_on:
      - broker
      - schema-registry

  schema-registry:
    image: confluentinc/cp-schema-registry:5.4.0
    env_file: schema-registry/env/docker.env
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - zookeeper
      - broker
    ports:
      - ${DATAHUB_MAPPED_SCHEMA_REGISTRY_PORT:-8081}:8081

  elasticsearch:
    image: elasticsearch:7.9.3
    env_file: elasticsearch/env/docker.env
    container_name: elasticsearch
    hostname: elasticsearch
    ports:
      - ${DATAHUB_MAPPED_ELASTIC_PORT:-9200}:9200
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    volumes:
      - esdata:/usr/share/elasticsearch/data
    healthcheck:
        test: ["CMD-SHELL", "curl -sS --fail 'http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=0s' || exit 1"]
        start_period: 2m
        retries: 4

  # This "container" is a workaround to pre-create search indices
  elasticsearch-setup:
    build:
      context: ../
      dockerfile: docker/elasticsearch-setup/Dockerfile
    image: ${DATAHUB_ELASTIC_SETUP_IMAGE:-linkedin/datahub-elasticsearch-setup}:${DATAHUB_VERSION:-head}
    env_file: elasticsearch-setup/env/docker.env
    hostname: elasticsearch-setup
    container_name: elasticsearch-setup
    depends_on:
      - elasticsearch

  datahub-gms:
    build:
        context: ../
        dockerfile: docker/datahub-gms/Dockerfile
    image: ${DATAHUB_GMS_IMAGE:-linkedin/datahub-gms}:${DATAHUB_VERSION:-head}
    env_file: datahub-gms/env/docker-without-neo4j.env
    hostname: datahub-gms
    container_name: datahub-gms
    ports:
      - ${DATAHUB_MAPPED_GMS_PORT:-8080}:8080
    depends_on:
      - elasticsearch-setup
      - kafka-setup
      - mysql

  datahub-frontend-react:
    build:
      context: ../
      dockerfile: docker/datahub-frontend/Dockerfile
    image: ${DATAHUB_FRONTEND_IMAGE:-linkedin/datahub-frontend-react}:${DATAHUB_VERSION:-head}
    env_file: datahub-frontend/env/docker.env
    hostname: datahub-frontend-react
    container_name: datahub-frontend-react
    ports:
      - ${DATAHUB_MAPPED_FRONTEND_PORT:-9002}:9002
    depends_on:
      - datahub-gms
    volumes:
      - ${HOME}/.datahub/plugins:/etc/datahub/plugins

  datahub-actions:
    image: acryldata/datahub-actions:${ACTIONS_VERSION:-head}
    hostname: actions
    env_file: datahub-actions/env/docker.env
    restart: on-failure:5
    depends_on:
      - datahub-gms


networks:
  default:
    name: datahub_network

volumes:
  esdata:
  zkdata:
