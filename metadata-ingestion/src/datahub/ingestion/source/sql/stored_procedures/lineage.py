import logging
from typing import Callable, Optional

from datahub.metadata.schema_classes import DataJobInputOutputClass
from datahub.sql_parsing.datajob import to_datajob_input_output
from datahub.sql_parsing.schema_resolver import SchemaResolver
from datahub.sql_parsing.split_statements import split_statements
from datahub.sql_parsing.sql_parsing_aggregator import (
    ObservedQuery,
    SqlParsingAggregator,
)

logger = logging.getLogger(__name__)


def parse_procedure_code(
    *,
    schema_resolver: SchemaResolver,
    default_db: Optional[str],
    default_schema: Optional[str],
    code: str,
    is_temp_table: Callable[[str], bool],
    raise_: bool = False,
    procedure_name: Optional[str] = None,
) -> Optional[DataJobInputOutputClass]:
    statements = list(split_statements(code))
    proc_label = f" for {procedure_name}" if procedure_name else ""
    logger.debug(
        f"[SPLIT] Procedure{proc_label} into {len(statements)} statements "
        f"(db={default_db}, schema={default_schema})"
    )

    aggregator = SqlParsingAggregator(
        platform=schema_resolver.platform,
        platform_instance=schema_resolver.platform_instance,
        env=schema_resolver.env,
        schema_resolver=schema_resolver,
        generate_lineage=True,
        generate_queries=False,
        generate_usage_statistics=False,
        generate_operations=False,
        generate_query_subject_fields=False,
        generate_query_usage_statistics=False,
        is_temp_table=is_temp_table,
    )

    for query in statements:
        aggregator.add_observed_query(
            observed=ObservedQuery(
                default_db=default_db,
                default_schema=default_schema,
                query=query,
            )
        )

    logger.debug(
        f"[STATS] Parsing{proc_label}: "
        f"total={aggregator.report.num_observed_queries}, "
        f"failed={aggregator.report.num_observed_queries_failed}, "
        f"success={aggregator.report.num_observed_queries - aggregator.report.num_observed_queries_failed}"
    )

    if aggregator.report.num_observed_queries_failed and raise_:
        logger.info(aggregator.report.as_string())
        raise ValueError(
            f"Failed to parse {aggregator.report.num_observed_queries_failed} queries."
        )

    mcps = list(aggregator.gen_metadata())
    logger.debug(f"[GEN]{proc_label} Generated {len(mcps)} MCP(s) from aggregator")

    result = to_datajob_input_output(
        mcps=mcps,
        ignore_extra_mcps=True,
    )

    if result:
        logger.debug(
            f"[OK] to_datajob_input_output{proc_label} SUCCESS: "
            f"inputs={len(result.inputDatasets)}, "
            f"outputs={len(result.outputDatasets)}"
        )
        # Log actual table URNs for debugging
        if result.inputDatasets:
            logger.debug(f"   Input dataset URNs{proc_label}:")
            for urn in result.inputDatasets:
                logger.debug(f"     - {urn}")
        if result.outputDatasets:
            logger.debug(f"   Output dataset URNs{proc_label}:")
            for urn in result.outputDatasets:
                logger.debug(f"     - {urn}")
    else:
        logger.debug(
            f"[FAIL] to_datajob_input_output{proc_label} returned None "
            "(likely all tables filtered as temp)"
        )
        # Try to understand why it returned None
        if len(mcps) == 0:
            logger.debug(f"   Reason{proc_label}: No MCPs generated by aggregator")
        else:
            logger.debug(
                f"   Reason{proc_label}: Generated {len(mcps)} MCP(s) but all tables were filtered out"
            )

    return result
