<configuration scan="true" scanPeriod="120 seconds">
    <property name="LOG_DIR" value="${LOG_DIR:-/tmp/datahub/logs/gms}"/>

    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <!-- SanitizingPatternLayout will clean %msg before output -->
        <!-- Java class: src/main/java/com/linkedin/metadata/utils/log/SanitizingPatternLayout.java -->
        <layout class="com.linkedin.metadata.utils.log.SanitizingPatternLayout">
            <pattern>%date{ISO8601} [%thread] %-5level %logger{36}:%L - %msg%n</pattern>
        </layout>
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <filter class="com.linkedin.metadata.utils.log.LogMessageFilter">
            <excluded>scanned from multiple locations</excluded>
            <excluded>[ignore_throttled] parameter is deprecated because frozen indices have been deprecated</excluded>
        </filter>
    </appender>

    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${LOG_DIR}/gms.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
            <FileNamePattern>${LOG_DIR}/gms.%d{yyyy-dd-MM}-%i.log</FileNamePattern>
            <!-- each archived file, size max 10MB -->
            <maxFileSize>100MB</maxFileSize>
            <!-- total size of all archive files, if total size > 10GB, it will delete old archived file -->
            <totalSizeCap>10GB</totalSizeCap>
            <!-- 30 days to keep -->
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <!-- SanitizingPatternLayoutEncoder will clean %msg before writing to file -->
        <!-- Java class: src/main/java/com/linkedin/metadata/utils/log/SanitizingPatternLayoutEncoder.java -->
        <encoder class="com.linkedin.metadata.utils.log.SanitizingPatternLayoutEncoder">
            <pattern>%date{ISO8601} [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
        <filter class="com.linkedin.metadata.utils.log.LogMessageFilter">
            <excluded>scanned from multiple locations</excluded>
            <excluded>[ignore_throttled] parameter is deprecated because frozen indices have been deprecated</excluded>
        </filter>
    </appender>

    <appender name="DEBUG_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${LOG_DIR}/gms.debug.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
            <FileNamePattern>${LOG_DIR}/gms.debug.%d{yyyy-dd-MM}-%i.log</FileNamePattern>
            <!-- each archived file, size max 10MB -->
            <maxFileSize>100MB</maxFileSize>
            <!-- total size of all archive files, if total size > 2GB, it will delete old archived file -->
            <totalSizeCap>2GB</totalSizeCap>
            <!-- 1 days to keep -->
            <maxHistory>1</maxHistory>
        </rollingPolicy>
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>${logging.appender.debug_file.level:-DEBUG}</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <!-- SanitizingPatternLayoutEncoder for debug logs -->
        <!-- Java class: src/main/java/com/linkedin/metadata/utils/log/SanitizingPatternLayoutEncoder.java -->
        <encoder class="com.linkedin.metadata.utils.log.SanitizingPatternLayoutEncoder">
            <pattern>%date{ISO8601} [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <appender name="GRAPHQL_DEBUG_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${LOG_DIR}/gms.graphql.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
            <FileNamePattern>${LOG_DIR}/gms.graphql.%d{yyyy-dd-MM}-%i.log</FileNamePattern>
            <!-- each archived file, size max 10MB -->
            <maxFileSize>100MB</maxFileSize>
            <!-- total size of all archive files, if total size > 2GB, it will delete old archived file -->
            <totalSizeCap>2GB</totalSizeCap>
            <!-- 1 days to keep -->
            <maxHistory>1</maxHistory>
        </rollingPolicy>
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>${logging.appender.graphql_debug_file.level:-DEBUG}</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <!-- SanitizingPatternLayoutEncoder for GraphQL debug logs -->
        <!-- Java class: src/main/java/com/linkedin/metadata/utils/log/SanitizingPatternLayoutEncoder.java -->
        <encoder class="com.linkedin.metadata.utils.log.SanitizingPatternLayoutEncoder">
            <pattern>%date{ISO8601} [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <appender name="ASYNC_GRAPHQL_DEBUG_FILE" class="ch.qos.logback.classic.AsyncAppender">
        <appender-ref ref="GRAPHQL_DEBUG_FILE" />
    </appender>

    <!-- Remove this to disable logging debug -->
    <logger name="com.linkedin" level="DEBUG">
        <appender-ref ref="DEBUG_FILE"/>
    </logger>

    <logger name="org.apache.kafka.clients" level="INFO">
        <appender-ref ref="STDOUT" />
    </logger>

    <logger name="org.reflections.Reflections" level="ERROR">
        <appender-ref ref="STDOUT" />
    </logger>

    <logger name="com.linkedin.metadata.models.registry.PatchEntityRegistry" level="INFO">
        <appender-ref ref="STDOUT" />
    </logger>

    <logger name="com.datahub.graphql" level="TRACE">
        <appender-ref ref="ASYNC_GRAPHQL_DEBUG_FILE"/>
    </logger>

    <logger name="com.datahub.graphql" level="DEBUG">
        <appender-ref ref="ASYNC_GRAPHQL_DEBUG_FILE"/>
    </logger>

    <root level="INFO">
        <appender-ref ref="STDOUT" />
        <appender-ref ref="FILE"/>
    </root>
</configuration>
